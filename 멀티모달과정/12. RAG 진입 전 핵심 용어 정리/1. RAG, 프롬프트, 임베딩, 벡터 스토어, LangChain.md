![[Pasted image 20250812174336.png]]



![[Pasted image 20250812174353.png]]

### RAG(검색 증강 생성)의 8단계 상세 설명

RAG는 LLM이 학습하지 않은 외부 데이터를 활용하여 답변의 정확성과 최신성을 높이는 기술입니다. 다음은 LangChain 프레임워크를 활용한 RAG의 8단계 구성 요소입니다.

**1단계: 문서 로드 (Document Loader)** 가장 먼저 외부 문서를 불러오는 단계입니다. 이 문서들은 LLM이 학습하지 않은 최신 정보나 특정 분야의 지식, 기업의 내부 자료 등이 될 수 있습니다. 이미지에 제시된 것처럼 DOCX, PDF, CSV 등 다양한 형식의 파일을 불러올 수 있습니다.

**2단계: 텍스트 분할 (Text Splitter)** 불러온 문서는 길이가 매우 길 수 있습니다. 이를 효율적으로 처리하고, 답변 생성에 필요한 핵심 정보를 정확하게 찾기 위해 문서를 일정한 크기의 작은 덩어리(**청크**)로 분할합니다.

**3단계: 임베딩 (Embedding)** 임베딩은 텍스트를 AI가 이해할 수 있는 언어, 즉 **벡터(Vector)**로 변환하는 과정입니다. 벡터는 텍스트의 의미와 문맥을 담고 있는 숫자 배열로, 의미적으로 비슷한 텍스트들은 서로 가까운 위치에 배치됩니다. 이 단계를 통해 분할된 청크들을 모두 벡터로 변환합니다.

**4단계: 벡터 스토어 (Vector Store)** 벡터 스토어는 3단계에서 변환된 벡터들을 저장하고 관리하는 데이터베이스입니다. 효율적인 검색을 위해 벡터를 인덱싱하며, 사용자의 질의에 해당하는 벡터를 빠르게 찾을 수 있도록 돕습니다.

**5단계: 리트리버 (Retriever)** 리트리버는 사용자의 질문을 받아서 4단계의 **벡터 스토어**에서 질문과 가장 관련성이 높은 문서를 검색하는 역할을 합니다. 사용자의 질문 또한 임베딩 과정을 거쳐 벡터로 변환된 후, 벡터 스토어에 저장된 문서 벡터들과의 유사도를 비교하여 가장 유사한 문서를 찾아냅니다.

**6단계: 프롬프트 (Prompt)** 이 단계에서는 5단계에서 검색된 관련 문서 내용과 사용자의 원래 질문을 결합하여, LLM이 답변을 생성할 수 있도록 **최종 프롬프트**를 구성합니다. 이 과정은 LLM에게 답변에 필요한 모든 문맥 정보를 제공합니다.

**7단계: LLM (Large Language Model)** 6단계에서 구성된 프롬프트를 바탕으로 LLM이 답변을 생성하거나 특정 작업을 수행합니다. RAG 기술의 핵심은 LLM이 학습 데이터가 아닌, 제공된 프롬프트의 외부 데이터를 기반으로 답변을 생성하도록 유도하는 것입니다.

**8단계: 체인 (Chain)** 체인은 이전에 설명한 **LangChain**의 핵심 기능 중 하나로, 1단계부터 7단계까지의 모든 과정을 하나의 자동화된 워크플로우로 연결하는 역할을 합니다. 이를 통해 사용자는 복잡한 과정을 거치지 않고 질문 하나만으로도 최종 답변을 얻을 수 있습니다.

### 용어 설명

- **RAG (Retrieval-Augmented Generation, 검색 증강 생성)**: LLM이 학습하지 않은 외부 지식을 활용하여 답변을 생성하는 기술입니다. LLM의 답변에 최신성과 정확성을 더하는 데 사용됩니다.
    
- **임베딩 (Embedding)**: 텍스트, 이미지 등 비정형 데이터를 AI가 이해할 수 있는 숫자 벡터로 변환하는 과정입니다. 의미적으로 유사한 데이터는 벡터 공간에서 서로 가까이 위치하게 됩니다.
    
- **벡터 스토어 (Vector Store)**: 임베딩을 통해 생성된 벡터들을 저장하고, 유사한 벡터를 효율적으로 검색할 수 있도록 특화된 데이터베이스입니다.
    
- **리트리버 (Retriever)**: 벡터 스토어에서 사용자의 질문과 가장 관련성 높은 문서를 찾아 검색하는 모듈입니다.
    
- **LangChain**: LLM을 기반으로 한 복잡한 애플리케이션을 쉽게 개발할 수 있도록 돕는 프레임워크입니다. RAG의 8단계와 같은 복잡한 워크플로우를 **체인** 형태로 구축하는 데 핵심적인 역할을 합니다.