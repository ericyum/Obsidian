
### RAG(Retrieval-Augmented Generation)의 필요성

RAG는 LLM(Large Language Model)의 성능을 향상시키기 위한 혁신적인 기술입니다. 특히 2022년 ChatGPT 등장 이후 인공지능이 인간처럼 대화하고 창의적인 글쓰기까지 가능해졌지만, 다음의 한계점들이 드러났습니다:

- **최신 정보의 부재**: ChatGPT는 학습된 시점 이후의 정보를 알지 못해, 최신 정보를 기반으로 답변할 수 없습니다.
- **할루시네이션(환각) 현상**: 복잡한 질문에 대해 사실과 다른 정보를 생성하는 환각 현상이 발생할 수 있습니다. RAG는 유효한 정보만을 기반으로 답변을 도출하게 강제하여 할루시네이션을 줄이는 데 효과적입니다.
- **개인 및 회사 내부 데이터 접근 불가**: 개인 정보나 회사 내부 데이터와 같이 학습되지 않은 특정 도메인 정보에 대해 답변할 수 없습니다.
- **문서 길이 문제**: 문서의 양이 많아질수록 할루시네이션 현상이 발생하기 쉬우며, 긴 문서 전체를 입력으로 넣는 데 제한이 있습니다.
- **답변 과정의 불투명성**: 기존 LLM은 답변 생성 과정이 불투명하여 왜 그런 답변이 나왔는지 확인하기 어렵습니다. RAG는 답변 과정의 투명성과 해석 가능성을 높여줍니다.
- **높은 API 사용 비용**: 문서의 양이 많아질수록 토큰 비용이 증가합니다. RAG를 사용하면 필요한 부분만 발췌하여 사용하여 비용 및 성능 면에서 효율적입니다.

RAG는 이러한 LLM의 한계를 보완하고 응답의 정확성을 높이는 데 필수적인 방식입니다. 외부 지식을 활용하여 AI의 지식 범위를 확장하고 응답의 정확성을 높이는 혁신적인 방법으로, 마치 AI가 거대한 디지털 도서관의 열쇠를 건네주는 것과 같습니다.

### 정보 검색 및 청크 분할 (Load & Split)

RAG 프로세스는 8단계 파이프라인으로 구성되며, 그 시작은 '문서 로드(Load)'와 '텍스트 분할(Split)'입니다.

1. **문서 로드 (Document Load)**: PDF, DOCX, CSV 등의 다양한 외부 문서를 불러오는 단계입니다. 이는 학습시키기 전에 확장된 필요한 정보를 미리 가져오고 초기 처리하는 과정과 같습니다.
2. **텍스트 분할 (Text Split)**: 불러온 문서는 그대로 사용되지 않고, 모델이 처리하기 용이하도록 작은 단위의 '청크(chunk)'로 나뉩니다. 텍스트 분할은 정보의 손실을 막으면서도 LLM이 한 번에 처리할 수 있는 최대 토큰 양에 맞춰 정보를 분할하는 것이 중요합니다. 예를 들어, 23페이지 분량의 PDF 문서는 1,000토큰 기준으로 69개의 청크로 분할되어 저장될 수 있습니다.

### 임베딩 및 유사도 계산 (Embed & Retrieve)

청크로 분할된 텍스트는 '임베딩(Embed)' 과정을 거쳐 숫자로 변환됩니다.

1. **임베딩 (Embedding)**: 임베딩은 컴퓨터가 문서의 내용과 의미를 이해하고 처리할 수 있도록 텍스트를 정량적인 숫자(벡터)로 변환하는 과정입니다. 같은 의미를 지닌 단어들은 비슷한 벡터 공간에 위치하게 되므로, 이를 통해 단어들 간의 유사도를 계산할 수 있습니다.
2. **유사도 계산 및 검색 (Retrieve)**: 사용자가 질문을 입력하면, 이 질문 역시 임베딩 과정을 거쳐 벡터로 변환됩니다. 변환된 질문 벡터는 데이터베이스에 미리 저장된 문서 청크들의 벡터와 **유사도를 계산**하여 가장 관련성이 높은 청크들을 찾아냅니다. 코사인 유사도(Cosine Similarity)나 MMR(Maximal Marginal Relevance) 같은 알고리즘이 이 비교 과정에 활용되어, 관련성 높고 가장 관련성이 적은 청크들을 선택합니다.

### 벡터 스토어 저장 개념 (Store)

임베딩된 청크들은 '벡터 스토어(Vector Store)'라는 데이터베이스에 저장됩니다.

- **저장 원리**: 임베딩된 청크들은 이 데이터베이스에 저장되어 나중에 빠르고 효율적으로 검색될 수 있도록 합니다. 이 과정을 **전처리 과정(pre-process)**이라고 합니다.
- **활용**: 사용자의 질문이 들어오면 벡터 스토어에서 질문과 유사도가 높은 청크를 검색하여 LLM에 **컨텍스트(context)**로 전달합니다. 이렇게 검색된 정보는 LLM이 답변을 생성하는 데 필요한 '참고 정보' 또는 '근거 자료'로 활용됩니다. 이는 LLM이 학습하지 않은 최신 정보나 특정 도메인에 대한 정보에 접근하여 더 정확하고 풍부한 답변을 제공할 수 있게 합니다.

이러한 과정을 통해 RAG는 LLM이 유용한 정보를 기반으로 응답을 도출하도록 하며, 주어진 문서에서 답변의 출처를 찾도록 하여 오류를 줄일 수 있습니다.