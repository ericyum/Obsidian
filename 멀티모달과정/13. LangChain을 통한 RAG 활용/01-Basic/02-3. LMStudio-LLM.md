# LMStudio Gemma 3 + LangChain 튜토리얼

## 사전 준비사항

1. LMStudio 실행

2. Gemma 3 모델 로드

3. Local Server 시작 (포트: 1234)





# 1. 환경 설정 및 패키지 임포트

```python
# 패키지 임포트

from langchain_openai import OpenAI, ChatOpenAI

from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from langchain.schema import HumanMessage, SystemMessage

from langchain.prompts import PromptTemplate

from langchain.chains import LLMChain

import os

  

# LMStudio 서버 설정

#LMSTUDIO_BASE_URL = "http://localhost:1234/v1"

LMSTUDIO_BASE_URL = "http://192.168.0.178:1234/v1"

# LMSTUDIO_BASE_URL = "http://127.0.0.1:1234/v1"

LMSTUDIO_API_KEY = "not-needed"  # LMStudio는 API 키가 필요없음

# MODEL_NAME = "gemma-3-27b"  # LMStudio에 로드된 모델 이름

MODEL_NAME = "exaone-4.0-1.2b"

  

print("설정 완료!")
```






# 2. 기본 LLM 사용법

```python
# LMStudio용 OpenAI 호환 LLM 생성

# 객체 생성

llm = ChatOpenAI(

    base_url=LMSTUDIO_BASE_URL,

    api_key=LMSTUDIO_API_KEY,

    model=MODEL_NAME,

    temperature=0.1,  # 창의성 수준 (0.0 ~ 1.0)

    max_tokens=4096,

)

  

# 질의내용

question = "대한민국의 수도는 어디인가요?"

  

# 질의

response = llm.invoke(question)
```

---

```python
response.content
```

![[Pasted image 20250814171548.png]]

---

```python
response.response_metadata
```

![[Pasted image 20250814171617.png]]





# 스트리밍 출력

스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용하다.

```python
# 스트림 방식으로 질의

# answer 에 스트리밍 답변의 결과를 받습니다.

answer = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)

# answer가 저장되지는 않음

for token in answer:

    print(token.content, end="", flush=True)
```

---

```python
from langchain_teddynote.messages import stream_response

# 스트림 방식으로 질의

# answer 에 스트리밍 답변의 결과를 받습니다.

answer = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")

stream_response(answer)
```





# 멀티모달 모델(이미지 인식)

멀티모달은 여러 가지 형태의 정보(모달)를 통합하여 처리하는 기술이나 접근 방식을 의미합니다. 이는 다음과 같은 다양한 데이터 유형을 포함할 수 있다.

- 텍스트: 문서, 책, 웹 페이지 등의 글자로 된 정보
- 이미지: 사진, 그래픽, 그림 등 시각적 정보
- 오디오: 음성, 음악, 소리 효과 등의 청각적 정보
- 비디오: 동영상 클립, 실시간 스트리밍 등 시각적 및 청각적 정보의 결합

**gemma-3-27b, exaone-4.0-1.2b 둘 다 텍스트만을 처리하기 때문에 다소 무겁더라도 멀티모달을 처리 할 수 있는 모델(llava-v1.5-7b-llamafile, gemma-3-12b)을 따로 사용해야 한다.**





# System, User 프롬프트 수정

```python
system_prompt = """당신은 표(재무제표) 를 해석하는 금융 AI 어시스턴트 입니다.

당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것입니다."""

  

user_prompt = """당신에게 주어진 표는 회사의 재무제표 입니다. 흥미로운 사실을 정리하여 답변하세요."""

  

# 멀티모달 객체 생성

multimodal_llm_with_prompt = MultiModal(

    llm, system_prompt=system_prompt, user_prompt=user_prompt

)
```

---

```python
# 로컬 PC 에 저장되어 있는 이미지의 경로 입력

IMAGE_PATH_FROM_FILE = "https://storage.googleapis.com/static.fastcampus.co.kr/prod/uploads/202212/080345-661/kwon-01.png"

  

# 이미지 파일로 부터 질의(스트림 방식)

answer = multimodal_llm_with_prompt.stream(IMAGE_PATH_FROM_FILE)

  

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)

stream_response(answer)
```