
```python
# API KEY를 환경변수로 관리하기 위한 설정 파일

from dotenv import load_dotenv

  

# API KEY 정보로드

load_dotenv()
```

```python
# LangSmith 추적을 설정합니다. https://smith.langchain.com

# .env 파일에 LANGCHAIN_API_KEY를 입력합니다.

# !pip install -qU langchain-teddynote

from langchain_teddynote import logging

  

# 프로젝트 이름을 입력합니다.

logging.langsmith("CH01-Basic")
```

`logging` : `logging` 모듈은 LangChain 애플리케이션의 실행 과정을 추적하고 디버깅하는 데 유용한 기능을 제공

`logging.langsmith("CH01-Basic")`: `logging` 모듈의 `langsmith` 함수를 호출하여 LangSmith 로깅을 활성화합니다.

- `"CH01-Basic"`: 이 문자열은 LangSmith에서 추적할 프로젝트의 이름입니다. LangSmith UI에서 이 이름으로 실행 내역을 확인할 수 있습니다.










# ChatOpenAI

OpenAI 사의 채팅 전용 Large Language Model(llm) 입니다.

객체를 생성할 때 다음을 옵션 값을 지정할 수 있습니다. 옵션에 대한 상세 설명은 다음과 같습니다.

`temperature`

- 사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.

`max_tokens`

- 채팅 완성에서 생성할 토큰의 최대 개수입니다.

`model_name`: 적용 가능한 모델 리스트a

- `gpt-4.1`

- `gpt-4.1-mini`

- `gpt-4.1-nano`

- `gpt-4.1`

- `gpt-4.1-mini`

- `o1-mini`, `o3`, `o4-mini`: tier5 계정 이상만 사용 가능. $1,000 이상 충전해야 tier5 계정이 됩니다.

```python
from langchain_openai import ChatOpenAI

  

# 객체 생성

llm = ChatOpenAI(

    temperature=0.1,  # 창의성 (0.0 ~ 2.0)

    model_name="gpt-4.1-nano",  # 모델명

)

  

# 질의내용

question = "대한민국의 수도는 어디인가요?"

  

# 질의

print(f"[답변]: {llm.invoke(question)}")
```

### 코드 분석

---

#### 1. `from langchain_openai import ChatOpenAI`

이 코드는 **LangChain의 `langchain_openai` 패키지에서 `ChatOpenAI` 클래스를 가져오는 역할**을 합니다. `ChatOpenAI`는 OpenAI의 챗(Chat) 모델(예: GPT-3.5-turbo, GPT-4)과 상호작용하기 위한 인터페이스를 제공합니다. LangChain의 다른 도구들과 쉽게 연결하여 사용할 수 있도록 만들어졌죠.

---

#### 2. `llm = ChatOpenAI(...)`

`ChatOpenAI` 클래스의 객체를 생성하고 `llm` 변수에 할당합니다. 이 객체가 바로 우리가 사용할 **LLM 모델**입니다. 객체를 생성할 때 다음과 같은 매개변수들을 설정할 수 있습니다.

- `temperature=0.1`: **모델의 창의성을 조절하는 매개변수**입니다. 0.0에 가까울수록 답변이 일관적이고 사실에 기반하며, 2.0에 가까울수록 더 창의적이고 다양한 답변을 생성합니다. 여기서는 0.1로 설정하여 비교적 안정적이고 사실적인 답변을 유도하고 있습니다.
    
- `model_name="gpt-4.1-nano"`: **사용할 모델의 이름을 지정**합니다. 이 예시에서는 "gpt-4.1-nano"라는 모델을 사용하도록 설정했네요.
    

---

#### 3. `question = "대한민국의 수도는 어디인가요?"`

모델에게 던질 질문을 문자열로 정의하고 `question` 변수에 저장합니다.

---

#### 4. `print(f"[답변]: {llm.invoke(question)}")`

`llm` 객체의 `.invoke()` 메서드를 호출하여 LLM 모델에 질문을 보냅니다.

- `.invoke()`: LangChain에서 모델을 실행하는 가장 기본적인 메서드입니다. 입력으로 받은 질문(`question`)을 모델에 전달하고, 모델의 응답을 반환합니다.
    
- `f"[답변]: ..."`: 모델이 반환한 답변을 보기 좋게 출력하는 코드입니다.










# 답변의 형식(AI Message)

```python
# 질의내용

question = "대한민국의 수도는 어디인가요?"

  

# 질의

response = llm.invoke(question)
```

```python
response
```

![[Pasted image 20250813155349.png]]

```python
response.content
```

![[Pasted image 20250813155407.png]]

```python
response.response_metadata
```

![[Pasted image 20250813155425.png]]









# LogProb 활성화

주어진 텍스트에 대한 모델의 **토큰 확률의 로그 값** 을 의미합니다. 토큰이란 문장을 구성하는 개별 단어나 문자 등의 요소를 의미하고, 확률은 **모델이 그 토큰을 예측할 확률**을 나타냅니다.

```python
# 객체 생성

llm_with_logprob = ChatOpenAI(

    temperature=0.1,  # 창의성 (0.0 ~ 2.0)

    max_tokens=2048,  # 최대 토큰수

    model_name="gpt-4.1-nano",  # 모델명

).bind(logprobs=True)

# 질의내용

question = "대한민국의 수도는 어디인가요?"

  

# 질의

response = llm_with_logprob.invoke(question)

# 결과 출력

response.response_metadata
```

### 코드 분석

---

#### 1. `llm_with_logprob = ChatOpenAI(...).bind(logprobs=True)`

이 부분은 이전 코드와 비슷하게 `ChatOpenAI` 객체를 생성하지만, 몇 가지 중요한 차이가 있습니다.

- `max_tokens=2048`: 모델이 생성할 수 있는 **최대 토큰 수를 2048개로 설정**했습니다. 이는 답변의 길이를 제한하거나 조절할 때 사용합니다.
    
- `.bind(logprobs=True)`: 이 부분이 핵심입니다. `.bind()` 메서드는 **모델에 특정 설정을 고정**시키는 역할을 합니다. `logprobs=True`는 모델이 답변을 생성할 때, 각 토큰에 대한 **로그 확률(LogProb) 값을 포함**하여 반환하도록 지시합니다.
    

**LogProb(로그 확률)이란?**

- **확률(Probability)**: 모델이 특정 토큰을 예측할 확률입니다. 예를 들어, "대한민국" 다음에 "의"가 올 확률이 95%라면, 이 확률이 모델의 예측 신뢰도를 나타냅니다.
    
- **로그 확률(LogProb)**: 이 확률에 로그를 취한 값입니다. 확률 값이 매우 작아질 때, 작은 숫자로 다루기 쉽도록 변환한 것입니다. 일반적으로 확률이 높을수록 LogProb 값은 0에 가까운 큰 값이 됩니다 (예: 95% -> LogProb = -0.022), 확률이 낮을수록 음의 무한대에 가까운 작은 값이 됩니다 (예: 1% -> LogProb = -4.6).
    

---

#### 2. `response = llm_with_logprob.invoke(question)`

`logprobs` 옵션이 바인딩된 `llm_with_logprob` 객체를 사용해 질문을 보냅니다. 모델은 이제 답변 텍스트뿐만 아니라 각 토큰의 LogProb 정보까지 함께 반환합니다.

---

#### 3. `response.response_metadata`

`.invoke()` 메서드의 반환값인 `response` 객체에는 여러 정보가 담겨 있습니다. `.response_metadata` 속성은 모델이 제공하는 **메타데이터(부가 정보)**를 포함하고 있으며, `logprobs=True`로 설정했기 때문에 이 안에 **토큰별 LogProb 정보가 들어있습니다.**









# 스트리밍 출력

스트리밍 옵션은 질의에 대한 답변을 실시간으로 받을 때 유용합니다.

```python
# 스트림 방식으로 질의

# answer 에 스트리밍 답변의 결과를 받습니다.

answer = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)

for token in answer:

    print(token.content, end="", flush=True)
```

```python
from langchain_teddynote.messages import stream_response

# 스트림 방식으로 질의

# answer 에 스트리밍 답변의 결과를 받습니다.

answer = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")

stream_response(answer)   
```

### 첫 번째 코드 블록 설명

이 코드는 **기본적인 LangChain의 스트리밍 방식**을 보여줍니다.

Python

```
# 스트림 방식으로 질의

# answer 에 스트리밍 답변의 결과를 받습니다.

answer = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)

for token in answer:

    print(token.content, end="", flush=True)
```

1. `llm.stream(...)`: `llm` 객체의 `.stream()` 메서드를 호출하여 LLM에 질문을 보냅니다. `.invoke()`와 달리, `.stream()`은 최종 답변이 아닌 **토큰 하나하나를 순서대로 반환하는 이터레이터(iterator)**를 반환합니다.
    
2. `for token in answer`: `for` 루프를 사용해 `answer` 이터레이터에서 토큰을 하나씩 가져옵니다. `token` 객체에는 다양한 정보가 담겨 있지만, `token.content`에는 실제 답변 텍스트의 일부분(예: '제주', '도', '는')이 들어있습니다.
    
3. `print(token.content, end="", flush=True)`: 가져온 토큰의 내용을 바로 출력합니다.
    
    - `end=""`: `print` 함수의 기본 동작인 줄바꿈을 하지 않도록 설정합니다.
        
    - `flush=True`: 버퍼에 쌓이지 않고 즉시 화면에 출력되도록 합니다. 이 설정이 없으면 스트리밍처럼 보이지 않고 한꺼번에 출력될 수도 있습니다.
        

이 코드는 **가장 기본적인 스트리밍 구현 방식**으로, 직접 토큰을 제어하며 출력할 때 사용하기 좋습니다.

---

### 두 번째 코드 블록 설명

이 코드는 **`langchain-teddynote` 라이브러리의 헬퍼 함수를 사용한 간편한 스트리밍 방식**입니다.

Python

```
from langchain_teddynote.messages import stream_response

# 스트림 방식으로 질의

# answer 에 스트리밍 답변의 결과를 받습니다.

answer = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")

stream_response(answer)
```

1. `from langchain_teddynote.messages import stream_response`: `langchain-teddynote` 라이브러리에서 `stream_response` 함수를 가져옵니다.
    
2. `answer = llm.stream(...)`: 위 코드와 동일하게 스트리밍 이터레이터를 가져옵니다.
    
3. `stream_response(answer)`: `stream_response` 함수에 이터레이터를 전달하기만 하면, 이 함수가 내부적으로 `for` 루프를 돌면서 토큰을 하나씩 출력해줍니다.










# 프롬프트 캐싱

프롬프트 캐싱 기능을 활용하면 반복하여 동일하게 입력으로 들어가는 토큰에 대한 비용을 아낄 수 있습니다.

다만, 캐싱에 활용할 토큰은 고정된 PREFIX 를 주는 것이 권장됩니다.

아래의 예시에서는 `<PROMPT_CACHING>` 부분에 고정된 토큰을 주어 캐싱을 활용하는 방법을 설명합니다.

```python
from langchain_teddynote.messages import stream_response

  

very_long_prompt = """

당신은 매우 친절한 AI 어시스턴트 입니다.

당신의 임무는 주어진 질문에 대해 친절하게 답변하는 것입니다.

아래는 사용자의 질문에 답변할 때 참고할 수 있는 정보입니다.

주어진 정보를 참고하여 답변해 주세요.

  

<WANT_TO_CACHE_HERE>

#참고:

**Prompt Caching**

Model prompts often contain repetitive content, like system prompts and common instructions. OpenAI routes API requests to servers that recently processed the same prompt, making it cheaper and faster than processing a prompt from scratch. This can reduce latency by up to 80% and cost by 50% for long prompts. Prompt Caching works automatically on all your API requests (no code changes required) and has no additional fees associated with it.

  

Prompt Caching is enabled for the following models:

  

gpt-4.1 (excludes gpt-4.1-2024-05-13 and chatgpt-4.1-latest)

gpt-4.1-mini

o1-preview

o1-mini

This guide describes how prompt caching works in detail, so that you can optimize your prompts for lower latency and cost.

  

Structuring prompts

Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.

  

How it works

Caching is enabled automatically for prompts that are 1024 tokens or longer. When you make an API request, the following steps occur:

  

Cache Lookup: The system checks if the initial portion (prefix) of your prompt is stored in the cache.

Cache Hit: If a matching prefix is found, the system uses the cached result. This significantly decreases latency and reduces costs.

Cache Miss: If no matching prefix is found, the system processes your full prompt. After processing, the prefix of your prompt is cached for future requests.

Cached prefixes generally remain active for 5 to 10 minutes of inactivity. However, during off-peak periods, caches may persist for up to one hour.

  

Requirements

Caching is available for prompts containing 1024 tokens or more, with cache hits occurring in increments of 128 tokens. Therefore, the number of cached tokens in a request will always fall within the following sequence: 1024, 1152, 1280, 1408, and so on, depending on the prompt's length.

  

All requests, including those with fewer than 1024 tokens, will display a cached_tokens field of the usage.prompt_tokens_details chat completions object indicating how many of the prompt tokens were a cache hit. For requests under 1024 tokens, cached_tokens will be zero.

  

What can be cached

Messages: The complete messages array, encompassing system, user, and assistant interactions.

Images: Images included in user messages, either as links or as base64-encoded data, as well as multiple images can be sent. Ensure the detail parameter is set identically, as it impacts image tokenization.

Tool use: Both the messages array and the list of available tools can be cached, contributing to the minimum 1024 token requirement.

Structured outputs: The structured output schema serves as a prefix to the system message and can be cached.

Best practices

Structure prompts with static or repeated content at the beginning and dynamic content at the end.

Monitor metrics such as cache hit rates, latency, and the percentage of tokens cached to optimize your prompt and caching strategy.

To increase cache hits, use longer prompts and make API requests during off-peak hours, as cache evictions are more frequent during peak times.

Prompts that haven't been used recently are automatically removed from the cache. To minimize evictions, maintain a consistent stream of requests with the same prompt prefix.

Frequently asked questions

How is data privacy maintained for caches?

  

Prompt caches are not shared between organizations. Only members of the same organization can access caches of identical prompts.

  

Does Prompt Caching affect output token generation or the final response of the API?

  

Prompt Caching does not influence the generation of output tokens or the final response provided by the API. Regardless of whether caching is used, the output generated will be identical. This is because only the prompt itself is cached, while the actual response is computed anew each time based on the cached prompt.

  

Is there a way to manually clear the cache?

  

Manual cache clearing is not currently available. Prompts that have not been encountered recently are automatically cleared from the cache. Typical cache evictions occur after 5-10 minutes of inactivity, though sometimes lasting up to a maximum of one hour during off-peak periods.

  

Will I be expected to pay extra for writing to Prompt Caching?

  

No. Caching happens automatically, with no explicit action needed or extra cost paid to use the caching feature.

  

Do cached prompts contribute to TPM rate limits?

  

Yes, as caching does not affect rate limits.

  

Is discounting for Prompt Caching available on Scale Tier and the Batch API?

  

Discounting for Prompt Caching is not available on the Batch API but is available on Scale Tier. With Scale Tier, any tokens that are spilled over to the shared API will also be eligible for caching.

  

Does Prompt Caching work on Zero Data Retention requests?

  

Yes, Prompt Caching is compliant with existing Zero Data Retention policies.

</WANT_TO_CACHE_HERE>

  

#Question:

{}

  

"""
```

```python
from langchain.callbacks import get_openai_callback

  

with get_openai_callback() as cb:

    # 답변 요청

    answer = llm.invoke(

        very_long_prompt.format("프롬프트 캐싱 기능에 대해 2문장으로 설명하세요")

    )

    print(cb)

    # 캐싱된 토큰 출력

    cached_tokens = answer.response_metadata["token_usage"]["prompt_tokens_details"][

        "cached_tokens"

    ]

    print(f"캐싱된 토큰: {cached_tokens}")
```

```python
with get_openai_callback() as cb:

    # 답변 요청

    answer = llm.invoke(

        very_long_prompt.format("프롬프트 캐싱 기능에 대해 2문장으로 설명하세요")

    )

    print(cb)

    # 캐싱된 토큰 출력

    cached_tokens = answer.response_metadata["token_usage"]["prompt_tokens_details"][

        "cached_tokens"

    ]

    print(f"캐싱된 토큰: {cached_tokens}")
```


### 프롬프트 캐싱

프롬프트 캐싱 기능은 반복되는 프롬프트의 비용을 절감하는 데 유용합니다. 특히 프롬프트의 앞부분이 고정된 경우 효과적입니다. 아래 예시에서는 `<WANT_TO_CACHE_HERE>` 부분에 고정된 내용을 넣어 캐싱을 활용하는 방법을 설명합니다.

---

### 1. `very_long_prompt` 변수 정의

Python

```
very_long_prompt = """
당신은 매우 친절한 AI 어시스턴트 입니다.
...
<WANT_TO_CACHE_HERE>
...
#Question:
{}
"""
```

이 코드는 **`very_long_prompt`**라는 긴 문자열을 정의합니다. 이 프롬프트는 다음과 같이 구성되어 있습니다.

- **고정된 내용**: 모델의 역할, 임무, 그리고 프롬프트 캐싱에 대한 상세 정보가 포함되어 있습니다. 이 부분은 질문이 바뀌어도 항상 동일하게 유지됩니다.
    
- **동적인 내용**: `#Question: {}` 부분에 실제 사용자의 질문이 들어갑니다.
    

프롬프트의 **고정된 부분을 앞에, 동적인 부분을 뒤에** 배치함으로써, 캐싱의 효과를 극대화할 수 있습니다.

---

### 2. 첫 번째 API 호출 (캐시 생성)

Python

```
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    # 답변 요청
    answer = llm.invoke(
        very_long_prompt.format("프롬프트 캐싱 기능에 대해 2문장으로 설명하세요")
    )
    print(cb)
    # 캐싱된 토큰 출력
    cached_tokens = answer.response_metadata["token_usage"]["prompt_tokens_details"][
        "cached_tokens"
    ]
    print(f"캐싱된 토큰: {cached_tokens}")
```

이 코드는 **`get_openai_callback`**을 이용해 API 호출 정보를 추적합니다.

- `llm.invoke(...)`: `very_long_prompt`에 질문을 넣어 모델을 호출합니다.
    
- **첫 번째 호출 시**: OpenAI는 이 프롬프트의 앞부분을 캐시에 저장합니다. 이때 `cached_tokens`는 `0`으로 표시되거나, 캐싱 조건(1024 토큰 이상)에 따라 일부 토큰이 캐시될 수 있습니다. 비용은 전체 프롬프트에 대해 청구됩니다.
    

---

### 3. 두 번째 API 호출 (캐시 활용)

Python

```
with get_openai_callback() as cb:
    # 답변 요청
    answer = llm.invoke(
        very_long_prompt.format("프롬프트 캐싱 기능에 대해 2문장으로 설명하세요")
    )
    print(cb)
    # 캐싱된 토큰 출력
    cached_tokens = answer.response_metadata["token_usage"]["prompt_tokens_details"][
        "cached_tokens"
    ]
    print(f"캐싱된 토큰: {cached_tokens}")
```

이 코드는 동일한 질문으로 다시 한 번 호출하는 예시입니다.

- **두 번째 호출 시**: OpenAI API는 프롬프트의 앞부분이 이전에 캐시된 내용과 동일함을 감지합니다.
    
- **결과**: 캐시된 부분에 대해서는 비용이 청구되지 않습니다. `cached_tokens` 값은 `0`보다 큰 숫자로 출력되며, 이는 **고정된 내용에 해당하는 토큰이 캐시로 처리되었음**을 의미합니다. `cb`의 `prompt_tokens`는 전체 토큰 수보다 적게 계산됩니다.








# 멀티모달 모델(이미지 인식)

멀티모달은 여러 가지 형태의 정보(모달)를 통합하여 처리하는 기술이나 접근 방식을 의미합니다. 이는 다음과 같은 다양한 데이터 유형을 포함할 수 있습니다.

- 텍스트: 문서, 책, 웹 페이지 등의 글자로 된 정보

- 이미지: 사진, 그래픽, 그림 등 시각적 정보

- 오디오: 음성, 음악, 소리 효과 등의 청각적 정보

- 비디오: 동영상 클립, 실시간 스트리밍 등 시각적 및 청각적 정보의 결합

`gpt-4.1` 모델은 이미지 인식 기능(Vision) 이 추가되어 있는 모델입니다.

```python
from langchain_teddynote.models import MultiModal

from langchain_teddynote.messages import stream_response

  

# 객체 생성

llm = ChatOpenAI(

    temperature=0.1,  # 창의성 (0.0 ~ 2.0)

    model_name="gpt-4.1-nano",  # 모델명

)

  

# 멀티모달 객체 생성

multimodal_llm = MultiModal(llm)
```

```python
# 샘플 이미지 주소(웹사이트로 부터 바로 인식)

IMAGE_URL = "https://t3.ftcdn.net/jpg/03/77/33/96/360_F_377339633_Rtv9I77sSmSNcev8bEcnVxTHrXB4nRJ5.jpg"

  

# 이미지 파일로 부터 질의

answer = multimodal_llm.stream(IMAGE_URL)

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)

stream_response(answer)
```

```python
# 로컬 PC 에 저장되어 있는 이미지의 경로 입력

IMAGE_PATH_FROM_FILE = "./images/sample-image.png"

  

# 이미지 파일로 부터 질의(스트림 방식)

answer = multimodal_llm.stream(IMAGE_PATH_FROM_FILE)

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)

stream_response(answer)
```

### 첫 번째 코드 블록: 객체 생성


```python
from langchain_teddynote.models import MultiModal
from langchain_teddynote.messages import stream_response

# 객체 생성
llm = ChatOpenAI(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    model_name="gpt-4.1-nano",  # 모델명
)

# 멀티모달 객체 생성
multimodal_llm = MultiModal(llm)
```

1. `from langchain_teddynote.models import MultiModal`: `langchain-teddynote` 라이브러리의 `MultiModal` 클래스를 가져옵니다. 이 클래스는 LangChain의 `ChatOpenAI` 객체를 래핑(Wrapping)하여 이미지 처리 기능을 추가하는 역할을 합니다.
    
2. `llm = ChatOpenAI(...)`: 이전 코드와 마찬가지로 `ChatOpenAI` 객체를 생성합니다. `model_name`은 `gpt-4.1-nano`와 같이 이미지 인식을 지원하는 모델을 사용해야 합니다.
    
3. `multimodal_llm = MultiModal(llm)`: `MultiModal` 클래스의 인스턴스를 생성합니다. 이때 인자로 `ChatOpenAI` 객체(`llm`)를 전달합니다. `multimodal_llm` 객체는 이제 `.stream()` 메서드를 호출할 때 텍스트뿐만 아니라 이미지 URL 또는 파일 경로도 입력으로 받을 수 있게 됩니다.
    

---

### 두 번째 코드 블록: 웹 URL 이미지 인식


```python
# 샘플 이미지 주소(웹사이트로 부터 바로 인식)
IMAGE_URL = "https://t3.ftcdn.net/..."

# 이미지 파일로 부터 질의
answer = multimodal_llm.stream(IMAGE_URL)

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)
stream_response(answer)
```

이 코드는 **웹 URL에 있는 이미지를 인식**하는 예시입니다.

1. `IMAGE_URL`: 분석할 이미지의 URL을 변수에 저장합니다.
    
2. `answer = multimodal_llm.stream(IMAGE_URL)`: `multimodal_llm` 객체의 `.stream()` 메서드에 **이미지 URL**을 입력으로 전달합니다. 모델은 이 URL을 통해 이미지를 다운로드하거나 접근하여 분석을 시작합니다.
    
3. `stream_response(answer)`: `multimodal_llm` 객체는 이미지 분석 결과를 텍스트로 변환하여 토큰 스트림으로 반환하고, 이 헬퍼 함수를 통해 실시간으로 출력됩니다.
    

---

### 세 번째 코드 블록: 로컬 이미지 인식


```python
# 로컬 PC 에 저장되어 있는 이미지의 경로 입력
IMAGE_PATH_FROM_FILE = "./images/sample-image.png"

# 이미지 파일로 부터 질의(스트림 방식)
answer = multimodal_llm.stream(IMAGE_PATH_FROM_FILE)

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)
stream_response(answer)
```

이 코드는 **로컬 컴퓨터에 저장된 이미지 파일을 인식**하는 예시입니다.

1. `IMAGE_PATH_FROM_FILE`: 로컬 이미지 파일의 경로를 변수에 저장합니다.
    
2. `answer = multimodal_llm.stream(IMAGE_PATH_FROM_FILE)`: `multimodal_llm` 객체에 **로컬 파일 경로**를 전달합니다. `MultiModal` 클래스는 내부적으로 해당 파일을 읽어들여 Base64로 인코딩한 후, 이를 모델에 전달하여 분석을 요청합니다.
    
3. `stream_response(answer)`: 위와 마찬가지로 모델의 분석 결과를 스트리밍 방식으로 출력합니다.









# System, User 프롬프트 수정

```python
system_prompt = """당신은 표(재무제표) 를 해석하는 금융 AI 어시스턴트 입니다.

당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것입니다."""

  

user_prompt = """당신에게 주어진 표는 회사의 재무제표 입니다. 흥미로운 사실을 정리하여 답변하세요."""

  

# 멀티모달 객체 생성

multimodal_llm_with_prompt = MultiModal(

    llm, system_prompt=system_prompt, user_prompt=user_prompt

)
```

```python
# 로컬 PC 에 저장되어 있는 이미지의 경로 입력

IMAGE_PATH_FROM_FILE = "https://storage.googleapis.com/static.fastcampus.co.kr/prod/uploads/202212/080345-661/kwon-01.png"

  

# 이미지 파일로 부터 질의(스트림 방식)

answer = multimodal_llm_with_prompt.stream(IMAGE_PATH_FROM_FILE)

  

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)

stream_response(answer)
```

### 1. 프롬프트 정의

```python
system_prompt = """당신은 표(재무제표) 를 해석하는 금융 AI 어시스턴트 입니다.
당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것입니다."""

user_prompt = """당신에게 주어진 표는 회사의 재무제표 입니다. 흥미로운 사실을 정리하여 답변하세요."""
```

- **`system_prompt`**: 모델의 역할과 임무를 정의하는 프롬프트입니다.
    
    - **"금융 AI 어시스턴트"**: 모델의 정체성을 설정합니다.
        
    - **"주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것"**: 모델이 수행해야 할 구체적인 작업을 명시합니다. 이 프롬프트는 모델이 재무제표 이미지를 보고 분석적인 답변을 하도록 유도합니다.
        
- **`user_prompt`**: 사용자 질문의 맥락을 제공하는 프롬프트입니다.
    
    - "당신에게 주어진 표는 회사의 재무제표 입니다. 흥미로운 사실을 정리하여 답변하세요.": 이는 모델이 이미지를 **재무제표**로 인식하고 **흥미로운 사실**을 요약하라는 추가 지시를 내립니다.
        

---

### 2. 멀티모달 객체 생성 및 프롬프트 바인딩


```python
multimodal_llm_with_prompt = MultiModal(
    llm, system_prompt=system_prompt, user_prompt=user_prompt
)
```

이 코드는 앞서 정의한 프롬프트들을 `MultiModal` 객체에 연결합니다.

- `MultiModal(llm, ...)`: 이전에 생성한 `llm` 객체를 사용하여 `MultiModal` 객체를 만듭니다.
    
- `system_prompt=system_prompt`: `system_prompt` 변수에 저장된 내용을 모델의 시스템 프롬프트로 설정합니다.
    
- `user_prompt=user_prompt`: `user_prompt` 변수에 저장된 내용을 모델의 사용자 프롬프트로 설정합니다.
    

이제 `multimodal_llm_with_prompt` 객체는 모든 호출에서 이 두 프롬프트를 자동으로 포함하여 모델의 동작 방식을 제어하게 됩니다.

---

### 3. 멀티모달 질의


```python
# 로컬 PC 에 저장되어 있는 이미지의 경로 입력
IMAGE_PATH_FROM_FILE = "https://storage.googleapis.com/static.fastcampus.co.kr/prod/uploads/202212/080345-661/kwon-01.png"

# 이미지 파일로 부터 질의(스트림 방식)
answer = multimodal_llm_with_prompt.stream(IMAGE_PATH_FROM_FILE)

# 스트리밍 방식으로 각 토큰을 출력합니다. (실시간 출력)
stream_response(answer)
```

- **`IMAGE_PATH_FROM_FILE`**: 웹상에 있는 재무제표 이미지의 URL을 변수에 저장합니다.
    
- `multimodal_llm_with_prompt.stream(...)`: **이미지 URL**을 입력으로 전달하여 모델에 질의합니다. 이때 모델은 단순히 이미지를 분석하는 것이 아니라, `system_prompt`와 `user_prompt`의 지시에 따라 **"재무제표를 해석하는 금융 AI 어시스턴트"**의 역할을 수행하며 답변을 생성합니다.
    
- `stream_response(answer)`: 모델이 이미지 분석 후 생성한 답변을 실시간으로 출력합니다.