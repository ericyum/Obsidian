## PromptTemplate



```python
from dotenv import load_dotenv

load_dotenv()
```

```
True
```



```python
# LangSmith 추적을 설정합니다. https://smith.langchain.com
# !pip install -qU langchain-teddynote
from langchain_teddynote import logging

# 프로젝트 이름을 입력합니다.
logging.langsmith("CH02-Prompt")
```

```
LangSmith 추적을 시작합니다.
[프로젝트명]
CH02-Prompt
```

LLM 객체를 정의합니다.



```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0.1, model="gpt-4.1-nano")
```

### 방법 1. from_template() 메소드를 사용하여 PromptTemplate 객체 생성

- 치환될 변수를 `{ 변수 }` 로 묶어서 템플릿을 정의합니다.
    



```python
from langchain_core.prompts import PromptTemplate

# template 정의. {country}는 변수로, 이후에 값이 들어갈 자리를 의미
template = "{country}의 수도는 어디인가요?"

# from_template 메소드를 이용하여 PromptTemplate 객체 생성
prompt = PromptTemplate.from_template(template)
prompt
```

```
PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')
```

`country` 변수에 값을 넣어서 문장을 생성할 수 있습니다.



```python
# prompt 생성. format 메소드를 이용하여 변수에 값을 넣어줌
prompt = prompt.format(country="대한민국")
prompt
```

```
'대한민국의 수도는 어디인가요?'
```



```python
# template 정의
template = "{country}의 수도는 어디인가요?"

# from_template 메소드를 이용하여 PromptTemplate 객체 생성
prompt = PromptTemplate.from_template(template)

# chain 생성
chain = prompt | llm
```



```python
# country 변수에 입력된 값이 자동으로 치환되어 수행됨
chain.invoke("대한민국").content
```

```
'대한민국의 수도는 서울입니다.'
```

### 방법 2. PromptTemplate 객체 생성과 동시에 prompt 생성

추가 유효성 검사를 위해 `input_variables` 를 명시적으로 지정하세요.

이러한 변수는 인스턴스화 중에 템플릿 문자열에 있는 변수와 비교하여 불일치하는 경우 예외를 발생시킵니다.



```python
# template 정의
template = "{country}의 수도는 어디인가요?"

# PromptTemplate 객체를 활용하여 prompt_template 생성
prompt = PromptTemplate(
    template=template,
    input_variables=["country"],
)

prompt
```

```
PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')
```



```python
# prompt 생성
prompt.format(country="대한민국")
```

```
'대한민국의 수도는 어디인가요?'
```



```python
# template 정의
template = "{country1}과 {country2}의 수도는 각각 어디인가요?"

# PromptTemplate 객체를 활용하여 prompt_template 생성
prompt = PromptTemplate(
    template=template,
    input_variables=["country1"],
    partial_variables={
        "country2": "미국"  # dictionary 형태로 partial_variables를 전달
    },
)

prompt
```

```
PromptTemplate(input_variables=['country1'], input_types={}, partial_variables={'country2': '미국'}, template='{country1}과 {country2}의 수도는 각각 어디인가요?')
```



```python
prompt.format(country1="대한민국")
```

```
'대한민국과 미국의 수도는 각각 어디인가요?'
```



```python
prompt_partial = prompt.partial(country2="캐나다")
prompt_partial
```

```
PromptTemplate(input_variables=['country1'], input_types={}, partial_variables={'country2': '캐나다'}, template='{country1}과 {country2}의 수도는 각각 어디인가요?')
```



```python
prompt_partial.format(country1="대한민국")
```

```
'대한민국과 캐나다의 수도는 각각 어디인가요?'
```



```python
chain = prompt_partial | llm
```



```python
chain.invoke("대한민국").content
```

```
'대한민국의 수도는 서울이고, 캐나다의 수도는 오타와입니다.'
```



```python
chain.invoke({"country1": "대한민국", "country2": "호주"}).content
```

```
'대한민국의 수도는 서울이고, 호주의 수도는 캔버라입니다.'
```

### `partial_variables`: 부분 변수 채움

`partial`을 사용하는 일반적인 용도는 함수를 부분적으로 사용하는 것입니다. 이 사용 사례는 **항상 공통된 방식으로 가져오고 싶은 변수** 가 있는 경우입니다.

대표적인 예가 **날짜나 시간** 입니다.

항상 현재 날짜가 표시되기를 원하는 프롬프트가 있다고 가정해 보겠습니다. 프롬프트에 하드 코딩할 수도 없고, 다른 입력 변수와 함께 전달하는 것도 번거롭습니다. 이 경우 항상 현재 **날짜를 반환하는 함수** 를 사용하여 프롬프트를 부분적으로 변경할 수 있으면 매우 편리합니다.

다음의 코드는 오늘 날짜를 구하는 파이썬 코드입니다.



```python
from datetime import datetime

# 오늘 날짜를 출력
datetime.now().strftime("%B %d")
```

```
'August 18'
```



```python
# 날짜를 반환하는 함수 정의
def get_today():
    return datetime.now().strftime("%B %d")
```


```python
prompt = PromptTemplate(
    template="오늘의 날짜는 {today} 입니다. 오늘이 생일인 유명인 {n}명을 나열해 주세요. 생년월일을 표기해주세요.",
    input_variables=["n"],
    partial_variables={
        "today": get_today  # dictionary 형태로 partial_variables를 전달
    },
)
```



```python
# prompt 생성
prompt.format(n=3)
```

```
'오늘의 날짜는 August 18 입니다. 오늘이 생일인 유명인 3명을 나열해 주세요. 생년월일을 표기해주세요.'
```



```python
# chain 을 생성합니다.
chain = prompt | llm
```



```python
# chain 을 실행 후 결과를 확인합니다.
print(chain.invoke(3).content)
```

```
물론입니다. 오늘이 8월 18일인 날 태어난 유명인 3명을 아래에 나열하겠습니다.

1. 로버트 레드포드 (Robert Redford)  
   생년월일: 1936년 8월 18일

2. 크리스티나 아길레라 (Christina Aguilera)  
   생년월일: 1980년 8월 18일

3. 케빈 베이컨 (Kevin Bacon)  
   생년월일: 1958년 7월 8일 (이 날짜는 참고용으로, 8월 18일 생인 유명인 중 다른 인물은 찾기 어려워서 제외하였으며, 만약 8월 18일 생인 유명인만 원하시면 알려 주세요.)

혹시 더 궁금한 점이 있으시면 말씀해 주세요!
```



```python
# chain 을 실행 후 결과를 확인합니다.
print(chain.invoke({"today": "Jan 02", "n": 3}).content)
```

```
2024년 1월 2일 기준으로 오늘이 생일인 유명인 3명은 다음과 같습니다:

1. **Maggie Smith (매기 스미스)** - 생년월일: 1934년 12월 28일  
   (참고: 매기 스미스는 12월 28일 생일이지만, 일부 자료에서 1월 2일에 생일인 유명인으로 잘못 알려진 경우도 있습니다. 정확한 정보는 확인이 필요합니다.)

2. **Mathieu Kassovitz (마티유 카소비츠)** - 생년월일: 1967년 11월 3일  
   (이 역시 11월 3일이 맞으며, 1월 2일 생일인 유명인으로는 적합하지 않습니다. 따라서, 1월 2일 생일인 유명인으로 다시 확인해보겠습니다.)

3. **Diane Keaton (다이앤 키튼)** - 생년월일: 1946년 1월 5일  
   (이 역시 1월 5일이므로, 1월 2일 생일인 유명인으로는 적합하지 않습니다.)

---

**정확한 1월 2일 생일인 유명인 3명은 다음과 같습니다:**

1. **Sir Isaac Newton (아이작 뉴턴)** - 생년월일: 1643년 1월 4일 (양력 기준, 율리우스력으로는 1642년 12월 25일)  
   (생일이 1월 2일은 아니지만, 1월 4일이 가까운 날짜입니다.)

2. **Frida Kahlo (프리다 칼로)** - 생년월일: 1907년 7월 6일 (아니므로 제외)

3. **Paul Revere (폴 리비어)** - 생년월일: 1734년 12월 21일 (아니므로 제외)

---

**결론적으로, 1월 2일 생일인 유명인으로 확실히 알려진 인물은 매우 적거나 없습니다.** 하지만, 참고로 알려진 인물 중 일부를 다시 확인해보면:

- **Sir Isaac Newton** (생일이 1월 4일이지만, 1월 2일과 가까운 날짜입니다.)
- **Diane Keaton** (생일은 1월 5일)

---

**요약:** 현재까지 확인된 바로는, 1월 2일이 생일인 유명인은 명확하게 알려진 인물이 많지 않습니다. 만약 특정 인물에 대해 더 알고 싶으시면 말씀해 주세요!
```

## 파일로부터 template 읽어오기



```python
from langchain_core.prompts import load_prompt

prompt = load_prompt("prompts/fruit_color.yaml", encoding="utf-8")
prompt
```

```
PromptTemplate(input_variables=['fruit'], input_types={}, partial_variables={}, template='{fruit}의 색깔이 뭐야?')
```

Window 사용자 중 이전의 코드가 오류가 나는 경우 아래의 코드로 실행하세요(인코딩 설정)



```python
from langchain_teddynote.prompts import load_prompt

# Windows 사용자 only: 인코딩을 cp949로 설정
load_prompt("prompts/fruit_color.yaml", encoding="utf-8")
```



```python
prompt.format(fruit="사과")
```

```
'사과의 색깔이 뭐야?'
```



```python
prompt2 = load_prompt("prompts/capital.yaml", encoding="utf-8")
print(prompt2.format(country="대한민국"))
```

```
대한민국의 수도에 대해서 알려주세요.
수도의 특징을 다음의 양식에 맞게 정리해 주세요.
300자 내외로 작성해 주세요.
한글로 작성해 주세요.
----
[양식]
1. 면적
2. 인구
3. 역사적 장소
4. 특산품

#Answer:

```

## ChatPromptTemplate

`ChatPromptTemplate` 은 대화목록을 프롬프트로 주입하고자 할 때 활용할 수 있습니다.

메시지는 튜플(tuple) 형식으로 구성하며, (`role`, `message`) 로 구성하여 리스트로 생성할 수 있습니다.

**role**

- `"system"`: 시스템 설정 메시지 입니다. 주로 전역설정과 관련된 프롬프트입니다.
    
- `"human"` : 사용자 입력 메시지 입니다.
    
- `"ai"`: AI 의 답변 메시지입니다.
    



```python
from langchain_core.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_template("{country}의 수도는 어디인가요?")
chat_prompt
```

```
ChatPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?'), additional_kwargs={})])
```



```python
chat_prompt.format(country="대한민국")
```

```
'Human: 대한민국의 수도는 어디인가요?'
```



```python
from langchain_core.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        # role, message
        ("system", "당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 {name} 입니다."),
        ("human", "반가워요!"),
        ("ai", "안녕하세요! 무엇을 도와드릴까요?"),
        ("human", "{user_input}"),
    ]
)

# 챗 message 를 생성합니다.
messages = chat_template.format_messages(
    name="테디", user_input="당신의 이름은 무엇입니까?"
)
messages
```

```
[SystemMessage(content='당신은 친절한 AI 어시스턴트입니다. 당신의 이름은 테디 입니다.', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='반가워요!', additional_kwargs={}, response_metadata={}),
 AIMessage(content='안녕하세요! 무엇을 도와드릴까요?', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='당신의 이름은 무엇입니까?', additional_kwargs={}, response_metadata={})]
```

생성한 메시지를 바로 주입하여 결과를 받을 수 있습니다.



```python
llm.invoke(messages).content
```

```
'저의 이름은 테디입니다! 무엇을 도와드릴까요?'
```

이번에는 체인을 생성해 보겠습니다.



```python
chain = chat_template | llm
```



```python
chain.invoke({"name": "Teddy", "user_input": "당신의 이름은 무엇입니까?"}).content
```

```
'제 이름은 Teddy입니다! 무엇을 도와드릴까요?'
```

## MessagePlaceholder

또한 LangChain은 포맷하는 동안 렌더링할 메시지를 완전히 제어할 수 있는 `MessagePlaceholder` 를 제공합니다.

메시지 프롬프트 템플릿에 어떤 역할을 사용해야 할지 확실하지 않거나 서식 지정 중에 메시지 목록을 삽입하려는 경우 유용할 수 있습니다.



```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

chat_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "당신은 요약 전문 AI 어시스턴트입니다. 당신의 임무는 주요 키워드로 대화를 요약하는 것입니다.",
        ),
        MessagesPlaceholder(variable_name="conversation"),
        ("human", "지금까지의 대화를 {word_count} 단어로 요약합니다."),
    ]
)
chat_prompt
```

```
ChatPromptTemplate(input_variables=['conversation', 'word_count'], input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000002A4BF7707C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 요약 전문 AI 어시스턴트입니다. 당신의 임무는 주요 키워드로 대화를 요약하는 것입니다.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='지금까지의 대화를 {word_count} 단어로 요약합니다.'), additional_kwargs={})])
```

`conversation` 대화목록을 나중에 추가하고자 할 때 `MessagesPlaceholder` 를 사용할 수 있습니다.



```python
formatted_chat_prompt = chat_prompt.format(
    word_count=5,
    conversation=[
        ("human", "안녕하세요! 저는 오늘 새로 입사한 테디 입니다. 만나서 반갑습니다."),
        ("ai", "반가워요! 앞으로 잘 부탁 드립니다."),
    ],
)

print(formatted_chat_prompt)
```

```
System: 당신은 요약 전문 AI 어시스턴트입니다. 당신의 임무는 주요 키워드로 대화를 요약하는 것입니다.
Human: 안녕하세요! 저는 오늘 새로 입사한 테디 입니다. 만나서 반갑습니다.
AI: 반가워요! 앞으로 잘 부탁 드립니다.
Human: 지금까지의 대화를 5 단어로 요약합니다.
```



```python
# chain 생성
chain = chat_prompt | llm | StrOutputParser()
```



```python
# chain 실행 및 결과확인
chain.invoke(
    {
        "word_count": 5,
        "conversation": [
            (
                "human",
                "안녕하세요! 저는 오늘 새로 입사한 테디 입니다. 만나서 반갑습니다.",
            ),
            ("ai", "반가워요! 앞으로 잘 부탁 드립니다."),
        ],
    }
)
```

```
'테디의 인사 및 인사'
```