# Huggingface Endpoints

Hugging Face Hub은 12만 개 이상의 모델, 2만 개의 데이터셋, 5만 개의 데모 앱(Spaces)을 보유한 플랫폼으로, 모두 오픈 소스이며 공개적으로 사용 가능합니다. 이 온라인 플랫폼에서 사람들은 쉽게 협업하고 함께 머신러닝을 구축할 수 있습니다.

Hugging Face Hub은 또한 다양한 ML 애플리케이션을 구축하기 위한 다양한 엔드포인트를 제공합니다. 이 예제는 다양한 유형의 엔드포인트에 연결하는 방법을 보여줍니다.

특히, 텍스트 생성 추론은 Text Generation Inference에 의해 구동됩니다. 이는 매우 빠른 텍스트 생성 추론을 위해 맞춤 제작된 Rust, Python, gRPC 서버입니다.

## 허깅페이스 토큰 발급

허깅페이스([https://huggingface.co](https://huggingface.co)) 에 회원가입을 한 뒤, 아래의 주소에서 토큰 발급을 신청합니다.

- 토큰 발급주소: [https://huggingface.co/docs/hub/security-tokens](https://huggingface.co/docs/hub/security-tokens)
    

## 참고 모델 리스트

- 허깅페이스 LLM 리더보드: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    
- 모델 리스트: [https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads)
    
- LogicKor 리더보드: [https://lk.instruct.kr/](https://lk.instruct.kr/)
    

## Huggingface Endpoints 사용

사용하기 위해서는 Python의 `huggingface_hub` 패키지를 설치해야 합니다.



```python
# !pip install -qU huggingface_hub
```

`.env` 파일에 이미 발급받은 토큰을 `HUGGINGFACEHUB_API_TOKEN` 을 저장한 뒤 다음 단계롤 진행합니다.

`HUGGINGFACEHUB_API_TOKEN` 을 불러옵니다.



```python
from dotenv import load_dotenv

load_dotenv()
```

```
True
```



```python
# LangSmith 추적을 설정합니다. https://smith.langchain.com
# !pip install langchain-teddynote
from langchain_teddynote import logging

# 프로젝트 이름을 입력합니다.
logging.langsmith("CH04-Models")
```

```
LangSmith 추적을 시작합니다.
[프로젝트명]
CH04-Models
```

허깅페이스 토큰을 입력합니다.



```python
from huggingface_hub import login

login()
```

```
VBox(children=(HTML(value='<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…
```

간단한 프롬프트를 생성합니다.



```python
from langchain.prompts import PromptTemplate

template = """Question: {question}

Answer:"""

template = """<|system|>
You are a helpful assistant.<|end|>
<|user|>
{question}<|end|>
<|assistant|>"""

prompt = PromptTemplate.from_template(template)
```

## Serverless Endpoints

Inference API 는 무료로 사용할 수 있으며 요금은 제한되어 있습니다. 프로덕션을 위한 추론 솔루션이 필요한 경우, [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) 서비스를 확인하세요. Inference Endpoints 를 사용하면 모든 머신 러닝 모델을 전용 및 완전 관리형 인프라에 손쉽게 배포할 수 있습니다. 클라우드, 지역, 컴퓨팅 인스턴스, 자동 확장 범위 및 보안 수준을 선택하여 모델, 지연 시간, 처리량 및 규정 준수 요구 사항에 맞게 설정하세요.

다음은 Inference API 에 액세스하는 방법의 예시입니다.

**참고**

- [Serverless Endpoints](https://huggingface.co/docs/api-inference/index)
    
- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)
    

`repo_id` 변수에 HuggingFace 모델의 `repo ID`(저장소 ID) 를 할당합니다.

- `microsoft/Phi-3-mini-4k-instruct` 모델: [https://huggingface.co/microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
    

**주의** 지금은 `microsoft/Phi-3-mini-4k-instruct` 모델이 엔드포인트에서 내려갔기 때문에 에러가 난다. 다른 모델을 입력해서 진행할 것

아래의 코드를 실행하기 전에 다음 지시사항을 따라야 한다.

1. huggingface 사이트에서 해당 모델을 검색 ([https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct))
    
2. 스크롤을 내리다 보면 submit버튼이 보일 것이다. 클릭하고 개인 정보를 입력 한 뒤에 다시한번 submit버튼을 누른다.
    
3. 그럼 마이 페이지의 Gated Repositories에서 요청 상태가 'pending'이라고 뜰 것이다. 요청을 보내고 처리 중인 것이다. 그리고 이것이 'accepted'로 변경될 때까지 기다린다.
    
4. 그 후, Access Tokens 항목의 Access Tokens에 해당 모델을 선택할 수 있을 것이다. 그러면 그것을 선택한 뒤에 아래쪽에 있는 체크박스들을 체크한다.
    

**현재 어째서인지 HuggingFaceEndpoint로 하는 방법이 먹히고 있지 않음. 그래서 첫 번째인 InferenceClient로 클라이언트를 만들고 그것을 통해 model을 돌리는 식으로 가야함**



```python
import os
from huggingface_hub import InferenceClient

client = InferenceClient(
 provider="novita",
 api_key=os.environ["HUGGINGFACEHUB_API_TOKEN"],
)

completion = client.chat.completions.create(
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[
        {"role": "user", "content": "The capital of France is"},
    ],
    stream=False,
)

print(completion.choices[0].message)
```

```
ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_call_id=None, tool_calls=None)
```



```python
from langchain_community.llms import HuggingFaceHub

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_xxx"
```



```python
repo_id = "mistralai/Mistral-7B-Instruct-v0.2"

llm = HuggingFaceHub(
    repo_id=repo_id,
    task="text-generation",
    model_kwargs={
        "max_new_tokens": 512,
        "do_sample": False,
        "return_full_text": False,
    },
)

# 주어진 프롬프트에 대해 언어 모델을 실행합니다.
llm.invoke(input="대한민국의 수도는 어디인가요?")
```



```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "A chat between a curious user and an artificial intelligence assistant. "
            "The assistant gives helpful, detailed, and polite answers to the user's questions.",
        ),
        ("user", "Human: {question}\nAssistant: "),
    ]
)

chain = prompt | llm | StrOutputParser()
```



```python
chain.invoke("대한민국의 수도는?")
```