# TXT Loader

`.txt` 확장자를 가지는 파일을 로더로 로드하는 방법을 살펴보겠습니다.


```python
from langchain_community.document_loaders import TextLoader

# 텍스트 로더 생성
loader = TextLoader("./data/appendix-keywords.txt", encoding="utf-8")

# 문서 로드
docs = loader.load()
print(f"문서의 수: {len(docs)}\n")
print("[메타데이터]\n")
print(docs[0].metadata)
print("\n========= [앞부분] 미리보기 =========\n")
print(docs[0].page_content[:500])
```

```
문서의 수: 1

[메타데이터]

{'source': './data/appendix-keywords.txt'}

========= [앞부분] 미리보기 =========

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다.
연관키워드: 토큰화, 자연어
```

## TextLoader를 통한 파일 인코딩 자동 감지

이 예제에서는 TextLoader 클래스를 사용하여 디렉토리에서 임의의 파일 목록을 대량으로 로드할 때 유용한 몇 가지 전략을 살펴보겠습니다.

먼저 문제를 설명하기 위해 임의의 인코딩으로 여러 개의 텍스트를 로드해 보겠습니다.

- `silent_errors`: 디렉토리로더에 silent_errors 매개변수를 전달하여 로드할 수 없는 파일을 건너뛰고 로드 프로세스를 계속할 수 있습니다.
    
- `autodetect_encoding`: 또한 로더 클래스에 자동 감지_인코딩을 전달하여 실패하기 전에 파일 인코딩을 자동으로 감지하도록 요청할 수도 있습니다.
    


```python
# !pip install chardet
```

```
Collecting chardet
  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)
Downloading chardet-5.2.0-py3-none-any.whl (199 kB)
Installing collected packages: chardet
Successfully installed chardet-5.2.0

[notice] A new release of pip is available: 25.1.1 -> 25.2
[notice] To update, run: python.exe -m pip install --upgrade pip
```


```python
from langchain_community.document_loaders import DirectoryLoader

path = "data/"

text_loader_kwargs = {"autodetect_encoding": True}

loader = DirectoryLoader(
    path,
    glob="**/*.txt",
    loader_cls=TextLoader,
    silent_errors=True,
    loader_kwargs=text_loader_kwargs,
)
docs = loader.load()
```

`data/appendix-keywords.txt` 파일과 파일명이 유사한 파생 파일들은 모두 인코딩 방식이 다른 파일들입니다.


```python
doc_sources = [doc.metadata["source"] for doc in docs]
doc_sources
```

```
['data\\appendix-keywords-CP949.txt',
 'data\\appendix-keywords-EUCKR.txt',
 'data\\appendix-keywords-utf8.txt',
 'data\\appendix-keywords.txt',
 'data\\chain-of-density.txt',
 'data\\reference.txt']
```


```python
print("[메타데이터]\n")
print(docs[2].metadata)
print("\n========= [앞부분] 미리보기 =========\n")
print(docs[2].page_content[:500])
```

```
[메타데이터]

{'source': 'data\\appendix-keywords-utf8.txt'}

========= [앞부분] 미리보기 =========

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다.
연관키워드: 토큰화, 자연어
```


```python
print("[메타데이터]\n")
print(docs[3].metadata)
print("\n========= [앞부분] 미리보기 =========\n")
print(docs[3].page_content[:500])
```

```
[메타데이터]

{'source': 'data\\appendix-keywords.txt'}

========= [앞부분] 미리보기 =========

Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다.
연관키워드: 토큰화, 자연어
```


```python
print("[메타데이터]\n")
print(docs[4].metadata)
print("\n========= [앞부분] 미리보기 =========\n")
print(docs[4].page_content[:500])
```

```
[메타데이터]

{'source': 'data\\chain-of-density.txt'}

========= [앞부분] 미리보기 =========

Selecting the “right” amount of information to include in a summary is a difficult task. 
A good summary should be detailed and entity-centric without being overly dense and hard to follow. To better understand this tradeoff, we solicit increasingly dense GPT-4 summaries with what we refer to as a “Chain of Density” (CoD) prompt. Specifically, GPT-4 generates an initial entity-sparse summary before iteratively incorporating missing salient entities without increasing the length. Summaries genera
```