# CharacterTextSplitter

이 방법은 가장 간단한 방식입니다.

기본적으로 `"\n\n"` 을 기준으로 문자 단위로 텍스트를 분할하고, 청크의 크기를 문자 수로 측정합니다.

1. 텍스트 분할 방식: 단일 문자 기준
2. 청크 크기 측정 방식: 문자 수 기준

- `./data/appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.
- 읽어들인 내용을 `file` 변수에 저장합니다.

```python
# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.
with open("./data/appendix-keywords.txt", encoding="UTF-8") as f:
    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.
```

파일로부터 읽은 파일의 일부 내용을 출력합니다.

```python
# 파일로부터 읽은 내용을 일부 출력합니다.
print(file[:500])
```

**Output:**

```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다.
연관키워드: 토큰화, 자연어
```

# CharacterTextSplitter를 사용하여 분할

CharacterTextSplitter를 사용하여 텍스트를 청크(chunk)로 분할하는 코드에 대해 설명합니다.

- `separator`: 분할할 기준을 설정합니다. 기본값은 `"\n\n"`입니다.
- `chunk_size`: 각 청크의 최대 크기를 설정합니다. 예: 250자
- `chunk_overlap`: 인접한 청크 간 중복을 허용합니다. 예: 50자
- `length_function`: 텍스트의 길이를 계산하는 함수를 지정합니다. 예: `len`

```python
from langchain_text_splitters import CharacterTextSplitter

# CharacterTextSplitter를 사용하여 텍스트를 청크(chunk)로 분할하는 코드
text_splitter = CharacterTextSplitter(
    # 텍스트를 분할할 때 사용할 구분자를 지정합니다. 기본값은 "\n\n"입니다.
    separator="\n\n",
    # 분할된 텍스트 청크의 최대 크기를 지정합니다 (문자 수).
    chunk_size=250,
    # 분할된 텍스트 청크 간의 중복되는 문자 수를 지정합니다.
    chunk_overlap=0,
    # 텍스트의 길이를 계산하는 함수를 지정합니다.
    length_function=len,
)
```

- `text_splitter`를 사용하여 `file` 텍스트를 문서 단위로 분할합니다.
- 분할된 문서 리스트 중 첫 번째 문서(`texts[0]`)를 출력합니다.

```python
# 텍스트를 청크로 분할합니다.
texts = text_splitter.create_documents([file])
print(len(texts[0].page_content))  # 분할된 문서의 개수를 출력합니다.
print(texts[0])  # 분할된 문서 중 첫 번째 문서를 출력합니다.
```

**Output:**

```
197
page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding'
```

다음은 문서와 함께 메타데이터를 전달하는 예시입니다.

메타데이터가 문서와 함께 분할되는 점에 주목해 주세요.

- `create_documents` 메서드는 텍스트 데이터와 메타데이터 리스트를 인자로 받습니다.

```python
metadatas = [
    {"document": 1},
    {"document": 2},
]  # 문서에 대한 메타데이터 리스트를 정의합니다.
documents = text_splitter.create_documents(
    [
        file,
        file,
    ],  # 분할할 텍스트 데이터를 리스트로 전달합니다.
    metadatas=metadatas,  # 각 문서에 해당하는 메타데이터를 전달합니다.
)
print(documents[0])  # 분할된 문서 중 첫 번째 문서를 출력합니다.
```

**Output:**

```
page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding' metadata={'document': 1}
```

```python
print(documents[29])
```

**Output:**

```
page_content='정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고 정확한 정보를 추출하거나 예측하는 데 사용됩니다.\n예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\n연관키워드: 데이터 융합, 인공지능, 딥러닝' metadata={'document': 1}
```

```python
print(documents[30])
```

**Output:**

```
page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding' metadata={'document': 2}
```

```python
len(documents)
```

**Output:**

```
60
```

```python
documents[1].metadata
```

**Output:**

```
{'document': 1}
```

`split_text()` 메서드를 사용하여 텍스트를 분할합니다.

- `text_splitter.split_text(file)[0]`은 `file` 텍스트를 `text_splitter`를 사용하여 분할한 후, 분할된 텍스트 조각 중 첫 번째 요소를 반환합니다.

 - split_text(): 분할된 텍스트 청크를 **단순 문자열 리스트(List[str])**로 반환합니다.

 - create_documents(): 분할된 텍스트 청크를 **Document 객체의 리스트(List[Document])**로 반환합니다.

```python
# text_splitter를 사용하여 file 텍스트를 분할하고, 분할된 텍스트의 첫 번째 요소를 반환합니다.
text_splitter.split_text(file)[0]
```

**Output:**

```
'Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding'
```

split_documents 메서드는 문자열이 아니라 Document 객체의 리스트를 인수로 기대한다.

```python
# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.
with open("./data/appendix-keywords.txt", encoding="UTF-8") as f:
    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.
```

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    # 텍스트를 분할할 때 사용할 구분자를 지정합니다. 기본값은 "\n\n"입니다.
    separator="\n\n",
    # 분할된 텍스트 청크의 최대 크기를 지정합니다 (문자 수).
    chunk_size=250,
    # 분할된 텍스트 청크 간의 중복되는 문자 수를 지정합니다.
    chunk_overlap=0,
    # 텍스트의 길이를 계산하는 함수를 지정합니다.
    length_function=len,
)
```

```python
from langchain_core.documents import Document

documents2 = [Document(page_content=file, metadata={})]
```

```python
texts = text_splitter.split_documents(documents2) # 변환된 Document 객체를 인수로 전달합니다.

print(texts) # 분할된 청크를 확인합니다.
```

**Output:**

```
[Document(metadata={}, page_content='Semantic Search\n\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n\nEmbedding'), Document(metadata={}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n연관키워드: 자연어 처리, 벡터화, 딥러닝\n\nToken'), Document(metadata={}, page_content='정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n연관키워드: 토큰화, 자연어 처리, 구문 분석\n\nTokenizer'), Document(metadata={}, page_content='정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.\n연관키워드: 토큰화, 자연어 처리, 구문 분석\n\nVectorStore'), Document(metadata={}, page_content='정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\n연관키워드: 임베딩, 데이터베이스, 벡터화\n\nSQL'), Document(metadata={}, page_content='정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\n연관키워드: 데이터베이스, 쿼리, 데이터 관리\n\nCSV'), Document(metadata={}, page_content='정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n\nJSON'), Document(metadata={}, page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\n연관키워드: 데이터 교환, 웹 개발, API\n\nTransformer'), Document(metadata={}, page_content='정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Atten... [truncated]
```

정리  
`create_documents`: 텍스트 문자열 형의 리스트와 딕셔너리 형의 metadata를 받아서 Document형이 들어있는 리스트로 반환. 리스트에 2개 이상이 있을 경우에는 반드시 metadata를 넣어야 하며 1개만 있을 경우에는 안 넣어도 된다.  
`split_text`: 텍스트 문자열 형을 받아서 문자열 리스트로 반환  
`split_documents`: Document형의 리스트를 받아서 Document형이 들어있는 리스트로 반환  
