# TokenTextSplitter

언어 모델에는 토큰 제한이 있습니다. 따라서 토큰 제한을 초과하지 않아야 합니다.

`TokenTextSplitter` 는 텍스트를 토큰 수를 기반으로 청크를 생성할 때 유용합니다.

## tiktoken

`tiktoken` 은 OpenAI에서 만든 빠른 `BPE Tokenizer` 입니다.

- `./data/appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.
- 읽어들인 내용을 `file` 변수에 저장합니다.

```python
# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.
with open("./data/appendix-keywords.txt", encoding="UTF-8") as f:
    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.
```

파일로부터 읽은 파일의 일부 내용을 출력합니다.

```python
# 파일으로부터 읽은 내용을 일부 출력합니다.
print(file[:500])
```

**Output:**

```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처리, 벡터화, 딥러닝

Token

정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.
예시: 문장 "나는 학교에 간다"를 "나는", "학교에", "간다"로 분할합니다.
연관키워드: 토큰화, 자연어
```

`CharacterTextSplitter`를 사용하여 텍스트를 분할합니다.

- `from_tiktoken_encoder` 메서드를 사용하여 Tiktoken 인코더 기반의 텍스트 분할기를 초기화합니다.

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    # 청크 크기를 300으로 설정합니다.
    chunk_size=300,
    # 청크 간 중복되는 부분이 없도록 설정합니다.
    chunk_overlap=0,
)
# file 텍스트를 청크 단위로 분할합니다.
texts = text_splitter.split_text(file)
```

**Output:**

```
Created a chunk of size 358, which is longer than the specified 300
Created a chunk of size 315, which is longer than the specified 300
Created a chunk of size 305, which is longer than the specified 300
Created a chunk of size 366, which is longer than the specified 300
Created a chunk of size 330, which is longer than the specified 300
Created a chunk of size 351, which is longer than the specified 300
Created a chunk of size 378, which is longer than the specified 300
Created a chunk of size 361, which is longer than the specified 300
Created a chunk of size 350, which is longer than the specified 300
Created a chunk of size 362, which is longer than the specified 300
Created a chunk of size 335, which is longer than the specified 300
Created a chunk of size 353, which is longer than the specified 300
Created a chunk of size 358, which is longer than the specified 300
Created a chunk of size 336, which is longer than the specified 300
Created a chunk of size 324, which is longer than the specified 300
Created a chunk of size 337, which is longer than the specified 300
Created a chunk of size 307, which is longer than the specified 300
Created a chunk of size 361, which is longer than the specified 300
Created a chunk of size 354, which is longer than the specified 300
Created a chunk of size 378, which is longer than the specified 300
Created a chunk of size 381, which is longer than the specified 300
Created a chunk of size 365, which is longer than the specified 300
Created a chunk of size 377, which is longer than the specified 300
Created a chunk of size 329, which is longer than the specified 300
```

 - from_tiktoken_encoder(): 이 메서드는 tiktoken 인코더를 사용하여 토큰 수를 계산하는 length_function을 자동으로 생성하고, 이를 CharacterTextSplitter 객체에 적용해줍니다.  
  
 - chunk_size=300: 이는 최대 300개의 토큰을 한 덩어리(chunk)로 나누라는 의미입니다.  
  
 - chunk_overlap=0: 청크 간에 겹치는 토큰이 없도록 설정합니다.  

분할된 청크의 개수를 출력합니다.

```python
print(len(texts))  # 분할된 청크의 개수를 출력합니다.
```

**Output:**

```
51
```

texts 리스트의 첫 번째 요소를 출력합니다.

```python
# texts 리스트의 첫 번째 요소를 출력합니다.
print(texts[0])
```

**Output:**

```
Semantic Search
```

**참고**

- `CharacterTextSplitter.from_tiktoken_encoder`를 사용하는 경우, 텍스트는 `CharacterTextSplitter`에 의해서만 분할되고 `tiktoken` 토크나이저는 분할된 텍스트를 병합하는 데 사용됩니다. (이는 분할된 텍스트가 `tiktoken` 토크나이저로 측정한 청크 크기보다 클 수 있음을 의미합니다.)

- `RecursiveCharacterTextSplitter.from_tiktoken_encoder`를 사용하면 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할됩니다. 또한 tiktoken 분할기를 직접 로드할 수 있으며, 이는 각 분할이 청크 크기보다 작음을 보장합니다.

## TokenTextSplitter

- `TokenTextSplitter` 클래스를 사용하여 텍스트를 토큰 단위로 분할합니다.

```python
from langchain_text_splitters import TokenTextSplitter

text_splitter = TokenTextSplitter(
    chunk_size=300,  # 청크 크기를 300으로 설정합니다.
    chunk_overlap=0,  # 청크 간 중복을 0으로 설정합니다.
)

# state_of_the_union 텍스트를 청크로 분할합니다.
texts = text_splitter.split_text(file)
print(texts[0])  # 분할된 텍스트의 첫 번째 청크를 출력합니다.
```

**Output:**

```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연
```

1. CharacterTextSplitter안의 from_tiktoken_encoder() 메서드를 사용하는 방법  
2. TokenTextSplitter안의 split_text() 메서드를 사용하는 방법  
  
둘 다 문자열로 이루어진 파일을 받으면 설정한 기준으로 분할을 해서 문자열로 된 리스트를 반환한다. 사실상 동일한 기능을하는 것이다.

### 텍스트 분할기의 동작 방식 차이

`CharacterTextSplitter.from_tiktoken_encoder()`와 `TokenTextSplitter()`는 동일한 목표를 가졌지만, 내부적인 동작 방식의 차이로 인해 다른 결과를 만들어냅니다.

#### 1. `CharacterTextSplitter`의 작동 방식

`CharacterTextSplitter`는 기본적으로 **문자 기반**으로 텍스트를 분할하며, `tiktoken`은 **길이를 측정하는 함수로만** 사용됩니다.

* **1단계**: 텍스트를 `separator`(`

`, `
`, ` `, ` `(빈 문자열) 등)를 사용하여 나눕니다.
* **2단계**: 이렇게 나뉜 덩어리들을 하나씩 합쳐나가면서 `tiktoken`으로 **토큰 수를 계산**합니다.
* **3단계**: 합쳐진 덩어리의 토큰 수가 `chunk_size`를 넘어가면, 그 덩어리 직전까지를 하나의 청크로 확정합니다.

이 과정에서 구분자로 인해 분리된 덩어리 하나가 이미 `chunk_size`보다 클 경우, 해당 덩어리가 통째로 하나의 청크가 되어 **설정한 `chunk_size`를 초과할 수 있습니다.**

#### 2. `TokenTextSplitter`의 작동 방식

`TokenTextSplitter`는 `tiktoken`을 사용하여 **토큰 단위로 텍스트를 직접적으로 분할**합니다.

* `TokenTextSplitter`는 텍스트를 먼저 `tiktoken`으로 토큰화한 뒤, 이 토큰들을 `chunk_size`에 맞춰 자릅니다.
* 이후 잘려진 토큰 덩어리들을 다시 문자열로 디코딩하여 청크를 생성합니다.

이 방식은 문자열의 의미적 경계를 고려하기보다는 **각 청크의 토큰 수가 `chunk_size`를 넘지 않도록 보장**하는 데 중점을 둡니다.

#### 결론

* `CharacterTextSplitter.from_tiktoken_encoder()`는 **최대 토큰 크기를 보장하지 않으며**, 단락이나 문장 단위의 의미적 경계를 지키는 데 중점을 둡니다.
* `TokenTextSplitter`는 **`tiktoken`을 사용하여 각 청크의 토큰 크기가 `chunk_size`를 넘지 않도록 보장**하여, 언어 모델의 최대 입력 길이를 초과하는 문제를 방지하는 데 더 적합합니다.

## spaCy

spaCy는 Python과 Cython 프로그래밍 언어로 작성된 고급 자연어 처리를 위한 오픈 소스 소프트웨어 라이브러리입니다.

NLTK의 또 다른 대안은 spaCy tokenizer를 사용하는 것입니다.

1. 텍스트가 분할되는 방식: **spaCy tokenizer**에 의해 분할됩니다.

2. chunk size가 측정되는 방법: **문자 수**로 측정됩니다.

spaCy 라이브러리를 최신 버전으로 업그레이드하는 pip 명령어입니다.

```python
!pip install -qU spacy
```

`en_core_web_sm` 모델을 다운로드합니다.

```python
!python -m spacy download en_core_web_sm --quiet
```

**Output:**

```
✔ Download and installation successful
You can now load the package via spacy.load('en_core_web_sm')

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.3.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\spacy\__init__.py", line 6, in <module>
    from .errors import setup_default_warnings
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\spacy\errors.py", line 3, in <module>
    from .compat import Literal
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\spacy\compat.py", line 4, in <module>
    from thinc.util import copy_array
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\thinc\__init__.py", line 5, in <module>
    from .config import registry
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\thinc\config.py", line 5, in <module>
    from .types import Decorator
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\thinc\types.py", line 27, in <module>
    from .compat import cupy, has_cupy
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\thinc\compat.py", line 35, in <module>
    import torch
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\__init__.py", line 1471, in <module>
    from .functional import *  # noqa: F403
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\nn\__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\nn\modules\__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\nn\modules\transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\nn\modules\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\torch\csrc\utils\tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
```

- `appendix-keywords.txt` 파일을 열어 내용을 읽어들입니다.

```python
# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.
with open("./data/appendix-keywords.txt", encoding="UTF-8") as f:
    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.
```

일부 내용을 출력하여 확인합니다.

```python
# 파일으로부터 읽은 내용을 일부 출력합니다.
print(file[:350])
```

**Output:**

```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.
예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝

Embedding

정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.
예시: "사과"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.
연관키워드: 자연어 처
```

- `SpacyTextSplitter` 클래스를 사용하여 텍스트 분할기를 생성합니다.

```python
import warnings
from langchain_text_splitters import SpacyTextSplitter

# 경고 메시지를 무시합니다.
warnings.filterwarnings("ignore")

# SpacyTextSplitter를 생성합니다.
text_splitter = SpacyTextSplitter(
    chunk_size=200,  # 청크 크기를 200으로 설정합니다.
    chunk_overlap=50,  # 청크 간 중복을 50으로 설정합니다.
)
```

**Output:**

```

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.3.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel_launcher.py", line 18, in <module>
    app.launch_new_instance()
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\traitlets\config\application.py", line 1075, in launch_instance
    app.start()
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel\kernelapp.py", line 739, in start
    self.io_loop.start()
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\tornado\platform\asyncio.py", line 211, in start
    self.asyncio_loop.run_forever()
  File "C:\Users\SBA\.pyenv\pyenv-win\versions\3.11.9\Lib\asyncio\base_events.py", line 608, in run_forever
    self._run_once()
  File "C:\Users\SBA\.pyenv\pyenv-win\versions\3.11.9\Lib\asyncio\base_events.py", line 1936, in _run_once
    handle._run()
  File "C:\Users\SBA\.pyenv\pyenv-win\versions\3.11.9\Lib\asyncio\events.py", line 84, in _run
    self._context.run(self._callback, *self._args)
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel\kernelbase.py", line 519, in dispatch_queue
    await self.process_one()
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel\kernelbase.py", line 508, in process_one
    await dispatch(*args)
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel\kernelbase.py", line 400, in dispatch_shell
    await result
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel\ipkernel.py", line 368, in execute_request
    await super().execute_request(stream, ident, parent)
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel\kernelbase.py", line 767, in execute_request
    reply_content = await reply_content
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel\ipkernel.py", line 455, in do_execute
    res = shell.run_cell(
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\ipykernel\zmqshell.py", line 577, in run_cell
    return super().run_cell(*args, **kwargs)
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\IPython\core\interactiveshell.py", line 3116, in run_cell
    result = self._run_cell(
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\IPython\core\interactiveshell.py", line 3171, in _run_cell
    result = runner(coro)
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\IPython\core\async_helpers.py", line 128, in _pseudo_sync_runner
    coro.send(None)
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\IPython\core\interactiveshell.py", line 3394, in run_cell_async
    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\IPython\core\interactiveshell.py", line 3639, in run_ast_nodes
    if await self.run_code(code, result, async_=asy):
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\IPython\core\interactiveshell.py", line 3699, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File "C:\Users\SBA\AppData\Local\Temp\ipykernel_12720\3988215695.py", line 8, in <module>
    text_splitter = SpacyTextSplitter(
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\langchain_text_splitters\spacy.py", line 28, in __init__
    self._tokenizer = _make_spacy_pipeline_for_splitting(
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\langchain_text_splitters\spacy.py", line 47, in _make_spacy_pipeline_for_splitting
    import spacy
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\spacy\__init__.py", line 6, in <module>
    from .errors import setup_default_warnings
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\spacy\errors.py", line 3, in <module>
    from .compat import Literal
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\spacy\compat.py", line 4, in <module>
    from thinc.util import copy_array
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\thinc\__init__.py", line 5, in <module>
    from .config import registry
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\thinc\config.py", line 5, in <module>
    from .types import Decorator
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\thinc\types.py", line 27, in <module>
    from .compat import cupy, has_cupy
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\thinc\compat.py", line 35, in <module>
    import torch
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\__init__.py", line 1471, in <module>
    from .functional import *  # noqa: F403
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\nn\__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\nn\modules\__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "c:\Users\SBA\AppData\Local\pypoetry\Cache\virtualenvs\langchain-kr-Us6BDj1P-py3.11\Lib\site-packages\torch\nn\modules\transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
```

- `text_splitter` 객체의 `split_text` 메서드를 사용하여 `file` 텍스트를 분할합니다.

```python
# text_splitter를 사용하여 file 텍스트를 분할합니다.
texts = text_splitter.split_text(file)
print(texts[0])  # 분할된 텍스트의 첫 번째 요소를 출력합니다.
```

**Output:**

```
Semantic Search

정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된

결과를 반환하는 검색 방식입니다.


예시: 사용자가 "태양계 행성"이라고 검색하면, "목성", "화성" 등과 같이 관련된 행성에 대한 정보를 반환합니다.
```

# 정리  
  
### `SpacyTextSplitter`의 분할 메커니즘

`SpacyTextSplitter`는 다른 분할기들과 달리, **자연어 처리(NLP) 라이브러리인 SpaCy를 활용**하여 텍스트를 분할합니다. 이 방식의 가장 큰 특징은 **문법적이고 의미적인 경계를 존중**한다는 점입니다.

#### 1. SpaCy 모델 로드
`SpacyTextSplitter`는 초기화 시 기본 언어 모델(`en_core_web_sm` 등)을 로드합니다. 이 모델은 텍스트 파싱, 품사 태깅, 개체명 인식 등 다양한 NLP 작업을 수행합니다.

#### 2. 문장 분리 (Sentence Segmentation)
가장 중요한 단계로, SpaCy 모델의 문장 분리 기능을 사용합니다. 이 기능은 단순히 구두점(., ?, !)뿐만 아니라 **문법적 규칙과 문맥을 파악**하여 정확한 문장 경계를 찾아냅니다. 예를 들어, "Mr. Smith"와 같이 마침표가 문장 끝이 아닌 경우를 정확히 식별합니다.

#### 3. 청크 생성
SpaCy가 분리한 문장들을 기준으로 청크를 생성합니다.
* 문장들을 순서대로 합쳐나갑니다.
* 합쳐진 텍스트의 길이가 `chunk_size`를 초과하기 직전까지의 문장들을 하나의 청크로 확정합니다.

이러한 메커니즘을 통해 `SpacyTextSplitter`는 단순한 문자열 분할이 아닌, **의미적으로 온전한 문장 단위로 텍스트를 분할**하여 문맥 손실을 최소화합니다.  

### `SpacyTextSplitter`와 `CharacterTextSplitter`의 공통점

`SpacyTextSplitter`와 `CharacterTextSplitter`는 **의미적 또는 문법적 경계(Semantic/Grammatical Boundaries)**를 우선시하여 텍스트를 분할합니다. 이로 인해 다음과 같은 공통된 특징이 나타납니다.

* **분할 우선순위**: `SpacyTextSplitter`는 **문장 단위**, `CharacterTextSplitter`는 **구분자(`

`, ` ` 등) 단위**로 텍스트를 나눕니다.
* **청크 크기 문제**: 이 분할 단위(문장 또는 구분자로 나뉜 덩어리)의 길이가 설정한 `chunk_size`를 초과할 경우, 해당 단위를 더 이상 쪼개지 않고 통째로 하나의 청크로 반환합니다.
* **결론**: 두 분할기 모두 **청크의 최대 크기를 엄격하게 보장하지 못합니다.**

## SentenceTransformers

`SentenceTransformersTokenTextSplitter`는 `sentence-transformer` 모델에 특화된 텍스트 분할기입니다.

기본 동작은 사용하고자 하는 sentence transformer 모델의 토큰 윈도우에 맞게 텍스트를 청크로 분할하는 것입니다.

샘플 텍스트를 확인합니다.

```python
from langchain_text_splitters import SentenceTransformersTokenTextSplitter

# 문장 분할기를 생성하고 청크 간 중복을 0으로 설정합니다.
splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=0)
```

```python
# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.
with open("./data/appendix-keywords.txt", encoding="UTF-8") as f:
    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.

# 파일으로부터 읽은 내용을 일부 출력합니다.
print(file[:350])
```

다음은 `file` 변수에 담긴 텍스트의 토큰의 개수를 세는 코드입니다. 시작과 종료 토큰의 개수를 제외한 후 출력합니다.

```python
count_start_and_stop_tokens = 2  # 시작과 종료 토큰의 개수를 2로 설정합니다.

# 텍스트의 토큰 개수에서 시작과 종료 토큰의 개수를 뺍니다.
text_token_count = splitter.count_tokens(text=file) - count_start_and_stop_tokens
print(text_token_count)  # 계산된 텍스트 토큰 개수를 출력합니다.
```

`splitter.split_text()` 함수를 사용하여 `text_to_split` 변수에 저장된 텍스트를 청크(chunk) 단위로 분할합니다.

```python
text_chunks = splitter.split_text(text=file)  # 텍스트를 청크로 분할합니다.
```

첫 번째 청크를 출력하여 내용을 확인합니다.

```python
# 0번째 청크를 출력합니다.
print(text_chunks[1])  # 분할된 텍스트 청크 중 두 번째 청크를 출력합니다.
```

## NLTK

Natural Language Toolkit (NLTK)은 Python 프로그래밍 언어로 작성된 영어 자연어 처리(NLP)를 위한 라이브러리와 프로그램 모음입니다.

단순히 "

"으로 분할하는 대신, NLTK tokenizers를 기반으로 텍스트를 분할하는 데 NLTK를 사용할 수 있습니다.

1. 텍스트 분할 방법: NLTK tokenizer에 의해 분할됩니다.
2. chunk 크기 측정 방법: 문자 수에 의해 측정됩니다.

- `nltk` 라이브러리를 설치하는 pip 명령어입니다.
- NLTK(Natural Language Toolkit)는 자연어 처리를 위한 파이썬 라이브러리입니다.
- 텍스트 데이터의 전처리, 토큰화, 형태소 분석, 품사 태깅 등 다양한 NLP 작업을 수행할 수 있습니다.

```python
!pip install -qU nltk
```

NLTK는 기본 설치 시 모든 데이터를 포함하지 않습니다. 이는 초기 설치 크기를 줄이고, 사용자가 필요한 데이터만 선택적으로 다운로드할 수 있게 합니다. 
NLTK에서 사용할 데이터를 다운로드 받습니다. 다운로드는 "~/nltk_data"에 설치됩니다.

```python
import nltk
nltk.download('punkt')
```

샘플 텍스트를 확인합니다.

```python
# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.
with open("./data/appendix-keywords.txt") as f:
    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.

# 파일으로부터 읽은 내용을 일부 출력합니다.
print(file[:350])
```

`NLTKTextSplitter` 클래스를 사용하여 텍스트 분할기를 생성합니다.

```python
from langchain_text_splitters import NLTKTextSplitter

text_splitter = NLTKTextSplitter(
    chunk_size=200,  # 청크 크기를 200으로 설정합니다.
    chunk_overlap=0,  # 청크 간 중복을 0으로 설정합니다.
)
```

`text_splitter` 객체의 `split_text` 메서드를 사용하여 `file` 텍스트를 분할합니다.

```python
# text_splitter를 사용하여 file 텍스트를 분할합니다.
texts = text_splitter.split_text(file)
print(texts[0])  # 분할된 텍스트의 첫 번째 요소를 출력합니다.
```

## KoNLPy

KoNLPy(Korean NLP in Python)는 한국어 자연어 처리(NLP)를 위한 파이썬 패키지입니다.

토큰 분할은 텍스트를 토큰이라고 하는 더 작고 관리하기 쉬운 단위로 분할하는 과정을 포함합니다.

이러한 토큰은 종종 단어, 구, 기호 또는 추가 처리 및 분석에 중요한 다른 의미 있는 요소입니다.

영어와 같은 언어에서 토큰 분할은 일반적으로 공백과 구두점으로 단어를 분리하는 것을 포함합니다.

토큰 분할의 효과는 언어 구조에 대한 토크나이저의 이해에 크게 의존하며, 이는 의미 있는 토큰 생성을 보장합니다.

영어를 위해 설계된 토크나이저는 한국어와 같은 다른 언어의 고유한 의미 구조를 이해할 수 있는 능력이 없기 때문에 한국어 처리에 효과적으로 사용될 수 없습니다.

### KoNLPy의 Kkma 분석기를 사용한 한국어 토큰 분할

한국어 텍스트의 경우 KoNLPY에는 `Kkma`(Korean Knowledge Morpheme Analyzer)라는 형태소 분석기가 포함되어 있습니다.

`Kkma`는 한국어 텍스트에 대한 상세한 형태소 분석을 제공합니다.

문장을 단어로, 단어를 각각의 형태소로 분해하고 각 토큰에 대한 품사를 식별합니다.

텍스트 블록을 개별 문장으로 분할할 수 있어 긴 텍스트 처리에 특히 유용합니다.

### 사용시 고려사항

`Kkma`는 상세한 분석으로 유명하지만, 이러한 정밀성이 처리 속도에 영향을 미칠 수 있다는 점에 유의해야 합니다. 따라서 `Kkma`는 신속한 텍스트 처리보다 분석적 깊이가 우선시되는 애플리케이션에 가장 적합합니다.

- KoNLPy 라이브러리를 설치하는 pip 명령어입니다.
- KoNLPy는 한국어 자연어 처리를 위한 파이썬 패키지로, 형태소 분석, 품사 태깅, 구문 분석 등의 기능을 제공합니다.

```python
!pip install -qU konlpy
```

샘플 텍스트를 확인합니다.

```python
# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.
with open("./data/appendix-keywords.txt") as f:
    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.

# 파일으로부터 읽은 내용을 일부 출력합니다.
print(file[:350])
```

KonlpyTextSplitter를 사용하여 한국어 텍스트를 분할하는 예제입니다.

```python
import chunk
from langchain_text_splitters import KonlpyTextSplitter

# KonlpyTextSplitter를 사용하여 텍스트 분할기 객체를 생성합니다.
text_splitter = KonlpyTextSplitter(chunk_size=200, chunk_overlap=50)
```

`text_splitter`를 사용하여 `file`를 문장 단위로 분할합니다.

```python
texts = text_splitter.split_text(file)  # 한국어 문서를 문장 단위로 분할합니다.
print(texts[0])  # 분할된 문장 중 첫 번째 문장을 출력합니다.
```

## Hugging Face tokenizer

Hugging Face는 다양한 토크나이저를 제공합니다.

이 코드에서는 Hugging Face의 토크나이저 중 하나인 GPT2TokenizerFast를 사용하여 텍스트의 토큰 길이를 계산합니다.

텍스트 분할 방식은 다음과 같습니다:

- 전달된 문자 단위로 분할됩니다.

청크 크기 측정 방식은 다음과 같습니다:

- Hugging Face 토크나이저에 의해 계산된 토큰 수를 기준으로 합니다.

- `GPT2TokenizerFast` 클래스를 사용하여 `tokenizer` 객체를 생성합니다.
- `from_pretrained` 메서드를 호출하여 사전 학습된 "gpt2" 토크나이저 모델을 로드합니다.

```python
from transformers import GPT2TokenizerFast

# GPT-2 모델의 토크나이저를 불러옵니다.
hf_tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
```

샘플 텍스트를 확인합니다.

```python
# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.
with open("./data/appendix-keywords.txt") as f:
    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.

# 파일으로부터 읽은 내용을 일부 출력합니다.
print(file[:350])
```

`from_huggingface_tokenizer` 메서드를 통해 허깅페이스 토크나이저(`tokenizer`)를 사용하여 텍스트 분할기를 초기화합니다.

```python
text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(
    # 허깅페이스 토크나이저를 사용하여 CharacterTextSplitter 객체를 생성합니다.
    hf_tokenizer,
    chunk_size=300,
    chunk_overlap=50,
)
# state_of_the_union 텍스트를 분할하여 texts 변수에 저장합니다.
texts = text_splitter.split_text(file)
```

1 번째 요소의 분할 결과를 확인합니다.

```python
print(texts[1])  # texts 리스트의 1 번째 요소를 출력합니다.
```