## ë¬¸ì„œ ë¡œë”©

```python
from google import genai
from google.genai import types
import pathlib
import httpx

client = genai.Client()

# Retrieve and encode the PDF byte
file_path = pathlib.Path("data/RAG_25-42.pdf")

# Upload the PDF using the File API
sample_file = client.files.upload(
    file=file_path,
)

prompt = """pdfíŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œí•´ì¤˜. pdfì—ëŠ” ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€, ì°¨íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆì–´.
            í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ê³ , ì´ë¯¸ì§€ë‚˜ ì°¨íŠ¸ê°€ ìˆë‹¤ë©´ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ í•œê¸€ë¡œ ë³€í™˜í•´ì¤˜.
        """

response = client.models.generate_content(
    model="gemini-1.5-flash", contents=[sample_file, prompt]
)
```

```python
print(response.text)
```

## Stuff

![alt text](image.png)

```python
from langchain.schema import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•œ ì„¤ì •
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # ê° chunkì˜ ìµœëŒ€ í¬ê¸°
    chunk_overlap=100,  # chunk ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„
    separators=[
        "\n\n",
        "\n",
        ".",
        "!",
        "?",
        ";",
        ",",
        " ",
        "",
    ],  # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•œ êµ¬ë¶„ì
)

# response.textë¥¼ chunkë¡œ ë‚˜ëˆ„ê¸°
text_chunks = text_splitter.split_text(response.text)

# Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
docs = [Document(page_content=chunk) for chunk in text_chunks]

print(f"ì´ {len(docs)}ê°œì˜ ë¬¸ì„œë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤.")
for i, doc in enumerate(docs):  # ì²˜ìŒ 3ê°œë§Œ ë¯¸ë¦¬ë³´ê¸°
    print(f"\n--- Document {i+1} ---")
    print(doc.page_content)
```

```python
from langchain.prompts import ChatPromptTemplate


prompt = ChatPromptTemplate.from_template(
    """ë‹¤ìŒ ìš”ì²­ì‚¬í•­ì— ë§ê²Œ ë¬¸ì¥ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
        ìš”ì²­ (REQUEST):
        1. ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ í•œê¸€ë¡œ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
        2. ê° ìš”ì•½ ë¬¸ì¥ì€ ë‚´ìš©ì— ì–´ìš¸ë¦¬ëŠ” ì´ëª¨ì§€ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.
        3. ë‹¤ì–‘í•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ì„ ë” í¥ë¯¸ë¡­ê²Œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.\n\nCONTEXT: {context}\n\nSUMMARY:
    """
)

prompt
```

```python
from langchain_openai import ChatOpenAI
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_teddynote.callbacks import StreamingCallback


llm = ChatOpenAI(
    model_name="gpt-4o-mini",
    streaming=True,
    temperature=0,
    callbacks=[StreamingCallback()],
)

stuff_chain = create_stuff_documents_chain(llm, prompt)
answer = stuff_chain.invoke({"context": docs})
```

```python
from langchain_google_genai import ChatGoogleGenerativeAI  # 1. Import ë³€ê²½
from langchain.chains.combine_documents import create_stuff_documents_chain

# from langchain_teddynote.callbacks import StreamingCallback

llm = ChatGoogleGenerativeAI(  # 2. í´ë˜ìŠ¤ ì´ë¦„ ë³€ê²½
    model="gemini-1.5-flash",  # 3. 'model_name'ì„ 'model'ë¡œ ë³€ê²½í•˜ê³  ëª¨ë¸ ì§€ì •
    # streaming=True,
    temperature=0,
    # callbacks=[StreamingCallback()],
)

# ì•„ë˜ ì½”ë“œëŠ” ëª¨ë¸ ì¢…ë¥˜ì™€ ìƒê´€ì—†ì´ ë™ì¼í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.
stuff_chain = create_stuff_documents_chain(llm, prompt)
answer = stuff_chain.invoke({"context": docs})
```

```python
print(answer)
```

## Markdownìœ¼ë¡œ ì¶œë ¥

SUMMARY:
*   ğŸ’¡ **RAG ì •ì˜ ë° ëª©í‘œ:** RAG(Retrieval-Augmented Generation)ëŠ” GPT ëª¨ë¸ì˜ í• ë£¨ì‹œë„¤ì´ì…˜ì„ ì¤„ì´ê³  ìµœì‹  ì •ë³´ì™€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì™¸ë¶€ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì‘ë‹µ ì •í™•ë„ë¥¼ ëŒ€í­ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë‚˜ íŒŒì¸ íŠœë‹ë³´ë‹¤ êµ¬í˜„ì´ ìš©ì´í•˜ë©°, íŠ¹ì • ë„ë©”ì¸ì— íŠ¹í™”ëœ ì±—ë´‡ ì œì‘ì— ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤.\n*   ğŸš« **ChatGPTì˜ í•œê³„:** ChatGPTëŠ” ìµœì‹  ì •ë³´ ë° ê°œì¸/íšŒì‚¬ ë‚´ë¶€ ë°ì´í„° í•™ìŠµ ë¶€ì¡±, ë³´ì•ˆ ë¬¸ì œ, ë¬¸ì„œ ì–‘ ì¦ê°€ ì‹œ í• ë£¨ì‹œë„¤ì´ì…˜ ë°œìƒ ë“±ì˜ ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n*   âœ… **RAGë¥¼ í†µí•œ ë¬¸ì œ í•´ê²°:** RAGëŠ” ì™¸ë¶€ ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¸ì¡°í•˜ì—¬ ìµœì‹  ì •ë³´ ê¸°ë°˜ ë‹µë³€, ë‚´ë¶€ ë°ì´í„° í™œìš©, ë°ì´í„° ì¶•ì  ë° ì¶œì²˜ ê²€ì¦ì„ í†µí•´ í• ë£¨ì‹œë„¤ì´ì…˜ì„ ì¤„ì´ê³  ë‹µë³€ í’ˆì§ˆì„ 50%ì—ì„œ 80~90% ìˆ˜ì¤€ìœ¼ë¡œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n*   âš™ï¸ **ChatGPT ë‚´ì¥ RAGì˜ í•œê³„:** ChatGPT ìì²´ RAGëŠ” ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆê°€ ì–´ë µê³ , íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì„œ ë‚´ ì •ë³´ ê²€ìƒ‰ ì‹¤íŒ¨ ë° í• ë£¨ì‹œë„¤ì´ì…˜ì´ ë°œìƒí•  ìˆ˜ ìˆì–´ ì‚¬ìš©ìê°€ ì§ì ‘ ì œì–´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n*   ğŸš€ **RAGì˜ ì£¼ìš” ëŠ¥ë ¥:** RAGëŠ” í”ŒëŸ¬ê·¸ì¸ì²˜ëŸ¼ ì‰¬ìš´ êµ¬í˜„, ìµœì‹  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹µë³€, ë‹µë³€ ê³¼ì •ì˜ íˆ¬ëª…í•œ í™•ì¸ ë° í•´ì„ ê°€ëŠ¥(LangSmithë¥¼ í†µí•œ ì¶”ì ), í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì†Œ ë“± ë›°ì–´ë‚œ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤.\n*   ğŸ”— **LangChainì„ ì´ìš©í•œ RAG êµ¬ì¶•:** LangChainì€ LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ë¡œ, ë³µì¡í•´ ë³´ì´ëŠ” RAG ì‹œìŠ¤í…œì˜ ë¬¸ì„œ ë¡œë“œ, í…ìŠ¤íŠ¸ ë¶„í• , ì„ë² ë”©, ê²€ìƒ‰, ë‹µë³€ ìƒì„± ë“± ëª¨ë“  ì„¸ë¶€ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‰½ê²Œ êµ¬í˜„í•˜ê³  íŠœë‹í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.\n*   ğŸ“š **RAG ì‚¬ì „ ì²˜ë¦¬ ë‹¨ê³„:** ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì™€(ë¬¸ì„œ ë¡œë“œ) ì²˜ë¦¬ ê°€ëŠ¥í•œ ì‘ì€ ë‹¨ìœ„(ì²­í¬)ë¡œ ë¶„í• í•˜ê³ (í…ìŠ¤íŠ¸ ë¶„í• ), ì´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ëŠ” ìˆ˜ì¹˜(ë²¡í„°)ë¡œ ë³€í™˜(ì„ë² ë”©)í•œ í›„, ë¹ ë¥´ê²Œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥(ë²¡í„° ìŠ¤í† ì–´ ì €ì¥)í•©ë‹ˆë‹¤.\n*   ğŸƒâ€â™‚ï¸ **RAG ì‹¤í–‰ ë‹¨ê³„:** ì‚¬ìš©ì ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰(ë¦¬íŠ¸ë¦¬ë²„)í•˜ê³ , ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì„ ìœ„í•œ ì§ˆë¬¸ì„ êµ¬ì„±(í”„ë¡¬í”„íŠ¸)í•˜ì—¬, ìµœì¢…ì ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±(LLM)í•˜ë©°, ì´ ëª¨ë“  ê³¼ì •ì„ í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸(ì²´ì¸ ìƒì„±)ìœ¼ë¡œ ë¬¶ìŠµë‹ˆë‹¤.\n*   ğŸ“ˆ **RAG ì„±ëŠ¥ í–¥ìƒ:** RAGëŠ” ê¸°ë³¸ ê²€ìƒ‰ë§Œìœ¼ë¡œ 45%ì˜ ì •í™•ë„ë¥¼ ë³´ì´ì§€ë§Œ, HyDE ê²€ìƒ‰, íŒŒì¸ íŠœë‹ ì„ë² ë”©, ìˆœìœ„ ì¬ì¡°ì •, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë“± ë‹¤ì–‘í•œ ê³ ê¸‰ ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ 98%ê¹Œì§€ ë‹µë³€ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## Map-Reduce
1. **Map ë‹¨ê³„** ì—ì„œëŠ” **ê° chunkë¥¼ ë³‘ë ¬ë¡œ ìš”ì•½** í•˜ê³ 
2. **reduce ë‹¨ê³„** ì—ì„œëŠ” ì´ ìš”ì•½ë“¤ì„ **í•˜ë‚˜ì˜ ìµœì¢… ìš”ì•½ìœ¼ë¡œ í†µí•©** í•©ë‹ˆë‹¤. 

```python
from langchain import hub
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import chain


@chain
def map_reduce_chain(docs):
    map_llm = ChatOpenAI(
        temperature=0,
        model_name="gpt-4o-mini",
    )

    # map prompt ë‹¤ìš´ë¡œë“œ
    map_prompt = hub.pull("teddynote/map-prompt")

    # map chain ìƒì„±
    map_chain = map_prompt | map_llm | StrOutputParser()

    # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸, ChatOpenAI, ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œë¥¼ ì—°ê²°í•˜ì—¬ ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    doc_summaries = map_chain.batch(docs)

    # reduce prompt ë‹¤ìš´ë¡œë“œ
    reduce_prompt = hub.pull("teddynote/reduce-prompt")
    reduce_llm = ChatOpenAI(
        model_name="gpt-4o",
        temperature=0,
        callbacks=[StreamingCallback()],
        streaming=True,
    )

    reduce_chain = reduce_prompt | reduce_llm | StrOutputParser()

    return reduce_chain.invoke(
        {"doc_summaries": "\n".join(doc_summaries), "language": "Korean"}
    )
```

```python
# ê²°ê³¼ ì¶œë ¥
answer = map_reduce_chain.invoke(docs)
```

## Map-Refine

1. Map ë‹¨ê³„: ë¬¸ì„œë¥¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ chunkë¡œ ë‚˜ëˆ„ê³ , ê° chunkì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.

2. Refine ë‹¨ê³„: ìƒì„±ëœ ìš”ì•½ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©° ìµœì¢… ìš”ì•½ì„ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•©ë‹ˆë‹¤. ê° ë‹¨ê³„ì—ì„œ ì´ì „ ìš”ì•½ê³¼ ìƒˆë¡œìš´ chunkì˜ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ìš”ì•½ì„ ê°±ì‹ í•©ë‹ˆë‹¤.
   
3. ë°˜ë³µ ê³¼ì •: ëª¨ë“  chunkê°€ ì²˜ë¦¬ë  ë•Œê¹Œì§€ refine ë‹¨ê³„ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.

4. ìµœì¢… ìš”ì•½: ë§ˆì§€ë§‰ chunkê¹Œì§€ ì²˜ë¦¬í•œ í›„ ì–»ì€ ìš”ì•½ì´ ìµœì¢… ê²°ê³¼ê°€ ë©ë‹ˆë‹¤.

```python
input_for_map = [{"documents": doc.page_content, "language": "Korean"} for doc in docs]
len(input_for_map)
```

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
# pip install langchain langchain-openai langchain_google_genai langchain-teddynote python-dotenv

from langchain_core.runnables import chain
from langchain import hub
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.output_parsers import StrOutputParser


# LangChain ì»¤ë®¤ë‹ˆí‹° í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë°ì½”ë ˆì´í„°
@chain
def map_refine_chain(docs):
    """
    ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ìš”ì•½í•˜ê³ (Map), ì ì§„ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ìµœì¢… ìš”ì•½ìœ¼ë¡œ ê°œì„ (Refine)í•˜ëŠ” ì²´ì¸ì…ë‹ˆë‹¤.
    """

    # 1. Map ë‹¨ê³„: ê° ë¬¸ì„œë¥¼ ê°œë³„ì ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    # ---------------------------------------------------
    map_summary_prompt = hub.pull("teddynote/map-summary-prompt")

    map_llm = ChatGoogleGenerativeAI(  # 2. í´ë˜ìŠ¤ ì´ë¦„ ë³€ê²½
        model="gemini-1.5-flash",  # 3. 'model_name'ì„ 'model'ë¡œ ë³€ê²½í•˜ê³  ëª¨ë¸ ì§€ì •
        temperature=0,
    )

    map_chain = map_summary_prompt | map_llm | StrOutputParser()

    # ê° ë¬¸ì„œ ë‚´ìš©(doc.page_content)ì„ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
    input_for_map = [
        {"documents": doc.page_content, "language": "Korean"} for doc in docs
    ]

    # .batch()ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë¬¸ì„œë¥¼ ë³‘ë ¬ë¡œ ìš”ì•½
    doc_summaries = map_chain.batch(input_for_map)
    print("--- ê° ë¬¸ì„œ ê°œë³„ ìš”ì•½ (Map) ê²°ê³¼ ---")
    for i, summary in enumerate(doc_summaries):
        print(f"[{i+1}ë²ˆ ë¬¸ì„œ ìš”ì•½]: {summary}")
    print("------------------------------------\n")

    # 2. Refine ë‹¨ê³„: ìš”ì•½ë³¸ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ë©° ê°œì„ í•©ë‹ˆë‹¤.
    # ---------------------------------------------------
    refine_prompt = hub.pull("teddynote/refine-prompt")

    # Refine ë‹¨ê³„ì—ì„œëŠ” Google Gemini ëª¨ë¸ì„ ì‚¬ìš©
    refine_llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0,
        # 'streaming' ì´ë‚˜ 'callbacks' íŒŒë¼ë¯¸í„°ëŠ” ì—¬ê¸°ì„œ ì œê±°í•©ë‹ˆë‹¤.
    )

    refine_chain = refine_prompt | refine_llm | StrOutputParser()

    # ì²« ë²ˆì§¸ ìš”ì•½ë³¸ì„ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘
    previous_summary = doc_summaries[0]

    # ë‚˜ë¨¸ì§€ ìš”ì•½ë³¸ë“¤ì„ ìˆœíšŒí•˜ë©° ê¸°ì¡´ ìš”ì•½ ë‚´ìš©ì— ìƒˆë¡œìš´ ìš”ì•½ì„ í†µí•©
    for i, current_summary in enumerate(doc_summaries[1:]):
        print(f"--- Refine {i+1}ë²ˆì§¸ ë‹¨ê³„ ì§„í–‰ ì¤‘ ---")
        previous_summary = refine_chain.invoke(
            {
                "previous_summary": previous_summary,
                "current_summary": current_summary,
                "language": "Korean",
            }
        )
        print(f"[ê²°ê³¼]: {previous_summary}")
        print("------------------------------------\n")

    return previous_summary


# ì²´ì¸ ì‹¤í–‰
refined_summary = map_refine_chain.invoke(docs)

print("\nğŸ‰ ìµœì¢… ìš”ì•½ ê²°ê³¼:")
print(refined_summary)
```

## Chain of Density

```python
import textwrap
from langchain import hub
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import SimpleJsonOutputParser

# {content}ë¥¼ ì œì™¸í•œ ëª¨ë“  ì…ë ¥ì— ëŒ€í•œ ê¸°ë³¸ê°’ ì§€ì •
cod_chain_inputs = {
    "content": lambda d: d.get("content"),
    "content_category": lambda d: d.get("content_category", "Article"),
    "entity_range": lambda d: d.get("entity_range", "1-3"),
    "max_words": lambda d: int(d.get("max_words", 80)),
    "iterations": lambda d: int(d.get("iterations", 5)),
}

# Chain of Density í”„ë¡¬í”„íŠ¸ ë‹¤ìš´ë¡œë“œ
cod_prompt = hub.pull("teddynote/chain-of-density-prompt")

# Chain of Density ì²´ì¸ ìƒì„±
cod_chain = (
    cod_chain_inputs
    | cod_prompt
    | ChatOpenAI(temperature=0, model="gpt-4o-mini")
    | SimpleJsonOutputParser()
)

# ë‘ ë²ˆì§¸ ì²´ì¸ ìƒì„±, ìµœì¢… ìš”ì•½ë§Œ ì¶”ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ë¶ˆê°€ëŠ¥, ìµœì¢… ê²°ê³¼ê°€ í•„ìš”í•¨)
cod_final_summary_chain = cod_chain | (
    lambda output: output[-1].get(
        "denser_summary", 'ì˜¤ë¥˜: ë§ˆì§€ë§‰ ë”•ì…”ë„ˆë¦¬ì— "denser_summary" í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤'
    )
)
```

```python
content = docs[1].page_content
print(content)
```

```python
# ê²°ê³¼ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
results: list[dict[str, str]] = []

# cod_chainì„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ê³  ë¶€ë¶„ì ì¸ JSON ê²°ê³¼ë¥¼ ì²˜ë¦¬
for partial_json in cod_chain.stream(
    {"content": content, "content_category": "Article"}
):
    # ê° ë°˜ë³µë§ˆë‹¤ resultsë¥¼ ì—…ë°ì´íŠ¸
    results = partial_json

    # í˜„ì¬ ê²°ê³¼ë¥¼ ê°™ì€ ì¤„ì— ì¶œë ¥ (ìºë¦¬ì§€ ë¦¬í„´ì„ ì‚¬ìš©í•˜ì—¬ ì´ì „ ì¶œë ¥ì„ ë®ì–´ì”€)
    print(results, end="\r", flush=True)

# ì´ ìš”ì•½ ìˆ˜ ê³„ì‚°
total_summaries = len(results)
print("\n")

# ê° ìš”ì•½ì„ ìˆœíšŒí•˜ë©° ì²˜ë¦¬
i = 1
for cod in results:
    # ëˆ„ë½ëœ ì—”í‹°í‹°ë“¤ì„ ì¶”ì¶œí•˜ê³  í¬ë§·íŒ…
    added_entities = ", ".join(
        [
            ent.strip()
            for ent in cod.get(
                "missing_entities", 'ERR: "missing_entiies" key not found'
            ).split(";")
        ]
    )
    # ë” ë°€ë„ ìˆëŠ” ìš”ì•½ ì¶”ì¶œ
    summary = cod.get("denser_summary", 'ERR: missing key "denser_summary"')

    # ìš”ì•½ ì •ë³´ ì¶œë ¥ (ë²ˆí˜¸, ì´ ê°œìˆ˜, ì¶”ê°€ëœ ì—”í‹°í‹°)
    print(
        f"### CoD Summary {i}/{total_summaries}, ì¶”ê°€ëœ ì—”í‹°í‹°(entity): {added_entities}"\n"
        + "\n"
    )
    # ìš”ì•½ ë‚´ìš©ì„ 80ì ë„ˆë¹„ë¡œ ì¤„ë°”ê¿ˆí•˜ì—¬ ì¶œë ¥
    print(textwrap.fill(summary, width=80) + "\n")
    i += 1

print("\n============== [ìµœì¢… ìš”ì•½] =================\n")
print(summary)
```

```python
print("\n============== [CoDì´ì „ê¸€] =================\n")
print(content)
print("\n============== [ìµœì¢… ìš”ì•½] =================\n")
print(summary)
```

```python
content
```

```python
content = "\n\n".join([doc.page_content for doc in docs])


# ê²°ê³¼ë¥¼ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
results: list[dict[str, str]] = []

# cod_chainì„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‹¤í–‰í•˜ê³  ë¶€ë¶„ì ì¸ JSON ê²°ê³¼ë¥¼ ì²˜ë¦¬
for partial_json in cod_chain.stream(
    {"content": content, "content_category": "Article"}
):
    # ê° ë°˜ë³µë§ˆë‹¤ resultsë¥¼ ì—…ë°ì´íŠ¸
    results = partial_json

    # í˜„ì¬ ê²°ê³¼ë¥¼ ê°™ì€ ì¤„ì— ì¶œë ¥ (ìºë¦¬ì§€ ë¦¬í„´ì„ ì‚¬ìš©í•˜ì—¬ ì´ì „ ì¶œë ¥ì„ ë®ì–´ì”€)
    print(results, end="\r", flush=True)

# ì´ ìš”ì•½ ìˆ˜ ê³„ì‚°
total_summaries = len(results)
print("\n")

# ê° ìš”ì•½ì„ ìˆœíšŒí•˜ë©° ì²˜ë¦¬
i = 1
for cod in results:
    # ëˆ„ë½ëœ ì—”í‹°í‹°ë“¤ì„ ì¶”ì¶œí•˜ê³  í¬ë§·íŒ…
    added_entities = ", ".join(
        [
            ent.strip()
            for ent in cod.get(
                "missing_entities", 'ERR: "missing_entiies" key not found'
            ).split(";")
        ]
    )
    # ë” ë°€ë„ ìˆëŠ” ìš”ì•½ ì¶”ì¶œ
    summary = cod.get("denser_summary", 'ERR: missing key "denser_summary"')

    # ìš”ì•½ ì •ë³´ ì¶œë ¥ (ë²ˆí˜¸, ì´ ê°œìˆ˜, ì¶”ê°€ëœ ì—”í‹°í‹°)
    print(
        f"### CoD Summary {i}/{total_summaries}, ì¶”ê°€ëœ ì—”í‹°í‹°(entity): {added_entities}"\n"
        + "\n"
    )
    # ìš”ì•½ ë‚´ìš©ì„ 80ì ë„ˆë¹„ë¡œ ì¤„ë°”ê¿ˆí•˜ì—¬ ì¶œë ¥
    print(textwrap.fill(summary, width=80) + "\n")
    i += 1

print("\n============== [ìµœì¢… ìš”ì•½] =================\n")
print(summary)
```

## Clustering-Map-Refine

1. map-reduce ë‚˜ map-refine ë°©ì‹ì€ ëª¨ë‘ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³ , ë¹„ìš©ì´ ë§ì´ ë“¬.
2. ë”°ë¼ì„œ, ë¬¸ì„œë¥¼ ëª‡ ê°œ(N ê°œ)ì˜ í´ëŸ¬ìŠ¤í„°ë¡œ ë‚˜ëˆˆ ë’¤, ê°€ì¥ ì¤‘ì‹¬ì¶•ì—ì„œ ê°€ê¹Œìš´ ë¬¸ì„œë¥¼ í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ë¬¸ì„œë¡œ ì¸ì§€í•˜ê³ , ì´ë¥¼ map-reduce(í˜¹ì€ map-refine) ë°©ì‹ìœ¼ë¡œ ìš”ì•½í•˜ëŠ” ë°©ì‹ì„ ì œì•ˆ.

```python
# í•˜ë‚˜ì˜ Text ë¡œ ëª¨ë“  ë¬¸ì„œë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
texts = "\n\n".join([doc.page_content for doc in docs])
len(texts)
```

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
split_docs = text_splitter.split_text(texts)
```

```python
# ì´ ë¬¸ì„œì˜ ìˆ˜ í™•ì¸
len(split_docs)
```

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

vectors = embeddings.embed_documents(split_docs)
```

```python
from sklearn.cluster import KMeans

# í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì„ íƒí•˜ë©´ ë¬¸ì„œì˜ ì½˜í…ì¸ ì— ë”°ë¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
num_clusters = 10

# Perform K-means clustering
kmeans = KMeans(n_clusters=num_clusters, random_state=123).fit(vectors)
```

```python
# ê²°ê³¼ í™•ì¸
kmeans.labels_
```

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ê²½ê³  ì œê±°
import warnings

warnings.filterwarnings("ignore")

# t-SNE ìˆ˜í–‰ ë° 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
tsne = TSNE(n_components=2, random_state=42)
reduced_data_tsne = tsne.fit_transform(np.array(vectors))

# seaborn ìŠ¤íƒ€ì¼ ì„¤ì •
sns.set_style("white")

# ì¶•ì†Œëœ ë°ì´í„° í”Œë¡¯
plt.figure(figsize=(10, 8))
sns.scatterplot(
    x=reduced_data_tsne[:, 0],
    y=reduced_data_tsne[:, 1],
    hue=kmeans.labels_,
    palette="deep",
    s=100,
)
plt.xlabel("Dimension 1", fontsize=12)
plt.ylabel("Dimension 2", fontsize=12)
plt.title("Clustered Embeddings", fontsize=16)
plt.legend(title="Cluster", title_fontsize=12)

# ë°°ê²½ìƒ‰ ì„¤ì •
plt.gcf().patch.set_facecolor("white")

plt.tight_layout()
plt.show()
```

```python
import numpy as np

# ê°€ì¥ ê°€ê¹Œìš´ ì ë“¤ì„ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±
closest_indices = []

# í´ëŸ¬ìŠ¤í„° ìˆ˜ë§Œí¼ ë°˜ë³µ
for i in range(num_clusters):

    # í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬ ëª©ë¡ êµ¬í•˜ê¸°
    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)

    # ê°€ì¥ ê°€ê¹Œìš´ ì ì˜ ì¸ë±ìŠ¤ ì°¾ê¸° (argminì„ ì‚¬ìš©í•˜ì—¬ ìµœì†Œ ê±°ë¦¬ ì°¾ê¸°)
    closest_index = np.argmin(distances)

    # í•´ë‹¹ ì¸ë±ìŠ¤ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    closest_indices.append(closest_index)
```

```python
closest_indices
```

```python
# ë¬¸ì„œì˜ ìš”ì•½ì„ ìˆœì„œëŒ€ë¡œ ì§„í–‰í•˜ê¸° ìœ„í•˜ì—¬ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
selected_indices = sorted(closest_indices)
selected_indices
```

```python
from langchain_core.documents import Document

selected_docs = [Document(page_content=split_docs[doc]) for doc in selected_indices]
selected_docs
```

```python
# ì´ì „ì— ìƒì„±í•œ map_refine_chainì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ ìƒì„±
cluster_refined_summary = map_refine_chain.invoke(selected_docs)
print(cluster_refined_summary)
```

# map-refined summary
LLMì˜ ì„±ëŠ¥ í–¥ìƒì€ RAG(Retrieval-Augmented Generation)ë¥¼ í†µí•œ 'ì»¨í…ìŠ¤íŠ¸ ìµœì í™”'ì™€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, íŒŒì¸ íŠœë‹ê³¼ ê°™ì€ 'LLM ìµœì í™”' ê¸°ë²•ì´ ê²°í•©ë  ë•Œ 'ì „ì²´ í™œìš©'ì„ ì´ë£¬ë‹¤. íŠ¹íˆ RAGëŠ” ì¼ë°˜ì ì¸ LLMì´ í”„ë¡¬í”„íŠ¸ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒê³¼ ë‹¬ë¦¬, LLMì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ì™¸ë¶€ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì§€ì‹ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” 'ë¦¬íŠ¸ë¦¬ë²„' ë‹¨ê³„ë¥¼ ì¶”ê°€í•œë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ í•¨ê»˜ 'ì»¨í…ìŠ¤íŠ¸'ë¡œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ì–´ LLMì´ ìµœì í™”ëœ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ë•ëŠ”ë‹¤. ì´ë¥¼ í†µí•´ LLMì€ ì‚¬ì „ í•™ìŠµëœ ë‚´ìš© ì™¸ì— ìµœì‹  ì •ë³´ ë° ë‚´ë¶€ ë°ì´í„°ì— íŠ¹í™”ëœ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆìœ¼ë©°, ìµœì‹  ì •ë³´ ê¸°ë°˜ ì‘ë‹µ ì„±ëŠ¥ì—ì„œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, PEFT, Full Fine-Tuningë³´ë‹¤ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. RAGëŠ” LLM ë‹µë³€ì˜ ì •í™•ë„ë¥¼ íšê¸°ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ”ë°, ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì„ ì´ìš©í•œ ê¸°ë³¸ ê²€ìƒ‰ì´ 45%ì˜ ì •í™•ë„ë¥¼ ë³´ì¸ ë°˜ë©´, HyDE ê²€ìƒ‰, íŒŒì¸ íŠœë‹ ì„ë² ë”©, ì²­í¬ ë¶„í• /ì„ë² ë”©ì„ ì ìš©í•˜ë©´ 65%ë¡œ, ìˆœìœ„ ì¬ì¡°ì • ë° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ë¥¼ í†µí•´ 85%ê¹Œì§€ í–¥ìƒëœë‹¤. íŠ¹íˆ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë„êµ¬ ì‚¬ìš© ë° ì¿¼ë¦¬ í™•ì¥ì„ ì ìš©í–ˆì„ ë•Œ 98%ë¡œ ê°€ì¥ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•œë‹¤. ì´ëŠ” ChatGPT ë‹¨ë… ì‚¬ìš© ì‹œ 50ì  ìˆ˜ì¤€ì´ë˜ ë‹µë³€ í’ˆì§ˆì„ 80~90ì ëŒ€ë¡œ í¬ê²Œ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒê³¼ ë”ë¶ˆì–´ ìµœì‹  ì •ë³´ ë° ë‚´ë¶€ ë°ì´í„° í•™ìŠµ í•œê³„ë¡œ ì¸í•œ íŠ¹ì • ë„ë©”ì¸ ì§ˆë¬¸ ë‹µë³€ì˜ ì œì•½ì„ í•´ì†Œí•œë‹¤.\n
RAG ì‹œìŠ¤í…œì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ì „, 'ì „ì²˜ë¦¬ ê³¼ì •'ì„ ê±°ì¹œë‹¤. ì´ ê³¼ì •ì—ì„œ ë‹¤ì–‘í•œ ì›ë³¸ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë§¥ë½ ìœ ì§€ë¥¼ ìœ„í•´ ì¼ë¶€ ê²¹ì¹˜ëŠ” 'ì²­í¬ ì˜¤ë²„ë©' ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë” ì‘ì€ 'ì²­í¬'ë¡œ ë¶„í• í•œë‹¤. ë³µì¡í•œ í…ìŠ¤íŠ¸ ì •ë³´ëŠ” ë²¡í„°(ì„ë² ë”©)ë¡œ ë³€í™˜ë˜ë©°, ì´ ì„ë² ë”© ê°’ì€ ë¹„ìš© íš¨ìœ¨ì„±ì„ ìœ„í•´ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ëœë‹¤. ì´í›„ ë¦¬íŠ¸ë¦¬ë²„ ë‹¨ê³„ì—ì„œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í•œ í›„, ë²¡í„° ìŠ¤í† ì–´ ë‚´ ë¬¸ì„œ ë²¡í„°ë“¤ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì´ë‚˜ MMR ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë¥¼ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì„ íƒí•œë‹¤. ë¦¬íŠ¸ë¦¬ë²„ì˜ ì„±ëŠ¥ì€ ì‹œìŠ¤í…œ ì‘ë‹µ í’ˆì§ˆê³¼ ì§ê²°ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, OpenAIì˜ ì„ë² ë”© ë²¡í„°ëŠ” 1536ì°¨ì›ìœ¼ë¡œ, ì°¨ì›ì´ ë†’ì„ìˆ˜ë¡ ì •êµí•œ ìœ ì‚¬ë„ ë¹„êµê°€ ê°€ëŠ¥í•˜ì§€ë§Œ ë” ë§ì€ ë¦¬ì†ŒìŠ¤ë¥¼ ì†Œëª¨í•œë‹¤. ì„ë² ë”©ì€ ë³µì¡í•œ ìì—°ì–´ì˜ ì˜ë¯¸ë¥¼ ì •ëŸ‰í™”ëœ ìˆ«ì ê°’ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì»´í“¨í„°ê°€ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ë” ì˜ ì´í•´í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”ë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë§Œì„ ì„ ë³„í•˜ì—¬ ê²€ìƒ‰ê¸°ì— ì „ë‹¬í•˜ëŠ” ê³¼ì •ì´ RAG í”„ë¡œì„¸ìŠ¤ì˜ ì‚¬ì „ ë‹¨ê³„ë¡œ ì¤‘ìš”í•˜ê²Œ ì‘ìš©í•˜ë©°, ìµœì¢…ì ìœ¼ë¡œ ê²€ìƒ‰ëœ ì •ë³´ì™€ í”„ë¡¬í”„íŠ¸ê°€ LLMìœ¼ë¡œ ì „ë‹¬ë˜ì–´ ì‹¤ì œ ì‘ë‹µì„ ìƒì„±í•œë‹¤. ì´ ê³¼ì •ì€ LLMì˜ ëŠ¥ë ¥ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ë§Œë“¤ë©°, ì•ì„  ëª¨ë“  ê³¼ì •ì˜ ê²°ê³¼ë¬¼ì„ ì‚¬ìš©ìê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ìµœì¢… ê´€ë¬¸ ì—­í• ì„ í•œë‹¤.\n
LLMì´ ì™¸ë¶€ ì°¸ì¡° ì •ë³´ ì—†ì´ ë‹µë³€í•  ê²½ìš° í• ë£¨ì‹œë„¤ì´ì…˜ì„ ì¼ìœ¼í‚¬ ê°€ëŠ¥ì„±ì´ ë†’ì§€ë§Œ, RAGëŠ” ê´€ë ¨ ë¬¸ì„œë¥¼ ì œê³µí•˜ì—¬ ë‹µë³€ì˜ ì‹ ë¢°ì„±ê³¼ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤. RAGëŠ” ìœ íš¨í•œ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ë„ì¶œí•˜ê±°ë‚˜ ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œë§Œ ë‹µë³€ì˜ ì¶œì²˜ë¥¼ ì°¾ë„ë¡ ê°•ì œí•˜ì—¬ í• ë£¨ì‹œë„¤ì´ì…˜ í˜„ìƒì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ë©°, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§, PEFT, Full Fine-Tuning ë“± ë‹¤ë¥¸ ê¸°ìˆ ë³´ë‹¤ í• ë£¨ì‹œë„¤ì´ì…˜ íšŒí”¼ ëŠ¥ë ¥ì´ ê°€ì¥ ë›°ì–´ë‚˜ë‹¤. íŠ¹íˆ, ì €ì¥ëœ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë‹µë³€ì˜ ì¶œì²˜ë¥¼ ì—­ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  ê²€ì¦í•¨ìœ¼ë¡œì¨ ì‘ë‹µ ì •í™•ë„ë¥¼ ëŒ€í­ í–¥ìƒì‹œí‚¨ë‹¤. ë˜í•œ, RAGëŠ” ë‹¤ë¥¸ AI ê¸°ìˆ ë“¤ì— ë¹„í•´ ë‹µë³€ ìƒì„± ê³¼ì •ì˜ ì „ ë‹¨ê³„ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ì¶”ì í•  ìˆ˜ ìˆì–´ ê°€ì¥ ë†’ì€ íˆ¬ëª…ì„±ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì„ ì œê³µí•œë‹¤. LangSmithì™€ ê°™ì€ ë„êµ¬ë¥¼ í™œìš©í•˜ë©´ 'RunnableSequence'ì˜ ì‹¤í–‰ ê³¼ì •ì„ ì‹œê°ì ìœ¼ë¡œ ì¶”ì í•˜ê³ , ê° ë‹¨ê³„ì˜ ì†Œìš” ì‹œê°„, ì…ë ¥ ì§ˆë¬¸, ì¶œë ¥ ë‹µë³€, ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡ ë° ì¶œì²˜, êµ¬ì²´ì ì¸ í…ìŠ¤íŠ¸ ë‚´ìš©ê¹Œì§€ ìƒì„¸íˆ ë¶„ì„í•  ìˆ˜ ìˆì–´ LLMì˜ ë‹µë³€ ë„ì¶œ ê³¼ì •ì„ ì‹¬ì¸µì ìœ¼ë¡œ ì´í•´í•˜ê³  ì„±ëŠ¥ ê°œì„ ì— í™œìš©í•  ìˆ˜ ìˆë‹¤.\n
RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ê³¼ ë‹µë³€ì˜ ì •êµí•¨ì€ ë‹¤ì–‘í•œ ì •ë³´ ê²€ìƒ‰ ê¸°ìˆ ì„ í™œìš©í•˜ê³ , GPTê°€ ì°¸ê³ í•  ë°ì´í„°ë¥¼ í•µì‹¬ ì •ë³´ ìœ„ì£¼ë¡œ ì •ì œí•¨ìœ¼ë¡œì¨ ë”ìš± ê³ ë„í™”ë  ìˆ˜ ìˆë‹¤. íŠ¹íˆ, RAGì— íŒŒì¸ íŠœë‹(ë¯¸ì„¸ ì¡°ì •)ì„ ë”í•˜ë©´ ëª¨ë¸ì˜ ì ì¬ë ¥ì„ ìµœê³  ìˆ˜ì¤€ìœ¼ë¡œ ëŒì–´ì˜¬ë ¤ ìƒì„¸í•œ ë‹µë³€ì„ ì–»ê³  í• ë£¨ì‹œë„¤ì´ì…˜ í˜„ìƒë„ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ë˜í•œ, ë¯¼ê°í•œ ê°œì¸/íšŒì‚¬ ì •ë³´ ì—…ë¡œë“œ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë³´ì•ˆ ë¬¸ì œë¥¼ ë°©ì§€í•œë‹¤. RAGëŠ” êµ¬í˜„ ë³µì¡ë„ ì¸¡ë©´ì—ì„œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ë³´ë‹¤ëŠ” ë†’ì§€ë§Œ, ì™„ì „ íŒŒì¸ íŠœë‹ì´ë‚˜ PEFTì™€ ê°™ì€ ë‹¤ë¥¸ ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ê¸°ìˆ ë³´ë‹¤ëŠ” ë‚®ì€ ì¤‘ê°„ ìˆ˜ì¤€ìœ¼ë¡œ ì‹¤ìš©ì ì´ë©°, ì‚¬ìš©ìê°€ ë°ì´í„°ë² ì´ìŠ¤ì™€ ëª¨ë¸ì„ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•˜ì—¬ íŠ¹ì • ë„ë©”ì¸ì— ìµœì í™”ëœ ì±—ë´‡ì„ ì œì‘í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ, ChatGPTì˜ ë‚´ì¥ RAG ì‹œìŠ¤í…œì€ ë‹µë³€ ì¶œì²˜ í™•ì¸ì˜ ì–´ë ¤ì›€ê³¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì œí•œìœ¼ë¡œ íˆ¬ëª…ì„±ê³¼ ì‹ ë¢°ë„ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë©°, ë¬¸ì„œ ë‚´ìš©ê³¼ ë‹¤ë¥¸ ë‹µë³€(í• ë£¨ì‹œë„¤ì´ì…˜)ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” í•œê³„ê°€ ìˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ë¬¸ì„œ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ì„œëŠ” LangChain, LangSmith, LangGraph, LangServe ë“± LLM ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì— í•„ìˆ˜ì ì¸ LangChain ìƒíƒœê³„ ë„êµ¬ë“¤ì„ í™œìš©í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì§ì ‘ êµ¬í˜„í•˜ê³  ì„¸ë¶€ ì•Œê³ ë¦¬ì¦˜ì„ 'íŠœë‹'í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. íŠ¹íˆ, LCEL(LangChain Expression Language) ë¬¸ë²•ì„ í™œìš©í•œ ì²´ì¸ ìƒì„± ë‹¨ê³„ëŠ” RAG íŒŒì´í”„ë¼ì¸ì„ ì™„ì„±í•˜ëŠ” ë§ˆì§€ë§‰ ê³¼ì •ìœ¼ë¡œ, ì´ë¥¼ í†µí•´ ê¸°ì¡´ RAG ì‹œìŠ¤í…œì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ì›í•˜ëŠ” í˜•íƒœì˜ ë‹µë³€ì„ ì–»ê¸° ìœ„í•´ ê° ê³¼ì •ì„ íˆ¬ëª…í•˜ê²Œ ê°œì„ í•  ìˆ˜ ìˆê²Œ í•œë‹¤. LangChainì€ GPT ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œ ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì‰½ê²Œ ì²˜ë¦¬í•˜ê³ , ë³µì¡í•œ íŒŒì´ì¬ ì½”ë“œ ëŒ€ì‹  í•œë‘ ì¤„ë¡œ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ ìˆê²Œ í•˜ë©°, ë°ì´í„°ë² ì´ìŠ¤, ì„ë² ë”, ë¬¸ì„œ ì¢…ë¥˜ ë“±ì„ ìœ ì—°í•˜ê²Œ ì„ íƒí•  ìˆ˜ ìˆì–´ ê·¸ ì ìš© ë¶„ì•¼ì™€ ì‚°ì—… ê·œëª¨ê°€ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ê³  ìˆë‹¤. ê¶ê·¹ì ìœ¼ë¡œ ì´ëŸ¬í•œ ì²´ê³„ì ì¸ ë°©ë²•ë¡  ì •ë¦½ì„ í†µí•´ ë‹¤ì–‘í•œ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì—ì„œ í™œìš© ê°€ëŠ¥í•œ ê³ ì„±ëŠ¥ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.

```python
print(refined_summary)
```

```

```