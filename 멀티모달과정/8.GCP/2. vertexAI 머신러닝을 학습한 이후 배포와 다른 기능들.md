# 학습

 - vertexAI의 '모델 학습'안의 '학습' 배너를 클릭하면 방금 머신러닝으로 학습한 것에 대한 결과가 나온다.

 - 클릭을 하면 '배포 및 사용' 탭의 '모델 레지스트리'로 이동한다. 레지스트리는 '등록'이라는 뜻으로 각 모델들의 학습 내용에 대한 평가가 '등록'되어있다. 거기에 있는 '점별 평가' 요소를 클릭하면

![[Pasted image 20250715092831.png]]
 - 위의 화면 처럼 평가 세부정보를 볼 수 있다.


# 배포

 - 그리고 초기 모델 레지스트리 화면에서 머신러닝 결과의 우측에 있는 점 3개를 누르면 'endpoint에 배포'가 있다.
 - 이것을 누르면 endpoint에 배포를 할 수 있다. (Docker Container에서 어느정도 표준화된 박스에 담아서 배표를 하는 원리이다.) 또는 화면 상단의 '배포 및 테스트' 배너를 클릭해서도 할 수 있다.

![[Pasted image 20250715093422.png]]


 - 엔드포인트는 모델 레지스트리 바로 아래에 있다.

![[Pasted image 20250715093050.png]]



### **복습: 머신러닝에 쓰이는 데이터의 종류**

**학습용(train)** 인공지능 모델이 찾아야 할 변수(파라미터)를 튜닝하는 데 사용

**검증용(validation)** 실제 모델의 변수 값을 업데이트하는 데 사용하지 X 현재 학습된 상태의 파라미터 값이 학습이 잘 되어 있는지 확인(방향성)

**테스트(test)** 실제 학습을 끝내고, 모델의 성능 평가




 - 엔드 포인드에 배포를 클릭할 시 
 
![[Pasted image 20250715093613.png]]
컴퓨팅 리소스가 모델에 예측 트래픽을 제공할 방식을 선택할 수 있다.

## GCP Vertex AI 컴퓨팅 리소스 설정 이해

* **Autoscaling (자동 스케일링):**
    * **설명:** "If you set a minimum and maximum, compute nodes will scale to meet traffic demand within those boundaries"
    * **조건:** **'최소 컴퓨팅 노드 수'와 '최대 컴퓨팅 노드 수'를 모두 설정했을 때** 적용됩니다.
    * **특징:** 트래픽 수요에 따라 컴퓨팅 노드 수가 최소와 최대 범위 내에서 자동으로 조절됩니다. 트래픽이 많으면 노드 수가 늘어나고, 트래픽이 적으면 노드 수가 줄어들어 자원을 효율적으로 사용하고 비용을 절감할 수 있습니다.

* **No scaling (스케일링 없음, 고정 노드):**
    * **설명:** "If you only set a minimum, then that number of compute nodes will always run regardless of traffic demand (the maximum will be set to minimum)"
    * **조건:** **'최소 컴퓨팅 노드 수'만 설정하고 '최대 컴퓨팅 노드 수'를 설정하지 않았을 때** (또는 자동으로 최소값으로 설정될 때) 적용됩니다.
    * **특징:** 설정된 최소 컴퓨팅 노드 수만큼의 노드가 트래픽 수요와 상관없이 항상 실행됩니다. 이는 일정한 자원 할당을 보장하지만, 트래픽 변동에 따른 자원 비효율성(트래픽이 적어도 노드가 계속 실행되어 비용 발생)이 발생할 수 있습니다.

**요약:**

* **'최소 컴퓨팅 노드 수'만 설정** → **No scaling (고정 노드)**
* **'최소 컴퓨팅 노드 수'와 '최대 컴퓨팅 노드 수'를 모두 설정** → **Autoscaling (자동 스케일링)**


 - 그 다음에는 다음과 같은 창이 뜬다.

![[Pasted image 20250715094113.png]]

 - 학습에 사용되었던 데이터를 다시 한 번 제공해야 학습-서빙 편향을 감지 할 수 있기 때문에 다시 하면 사용했던 데이터를 제공해야 한다. 학습을 할 때 데이터 세트에 어떤 데이터 인지를 지정했었기 때문에 **Vertex AI 데이터 세트**를 클릭한다. Cloud Storage의 버킷에 있다면 **Cloud Storage의 버킷**을 클릭하여 경로를 지정하는 방식도 가능하다.




### API 샘플요청 선택

![[Pasted image 20250715094332.png]]
 - 그 후에 endpoint가 배포 완료 상태인 것을 클릭한 후 'API 샘플요청'을 클릭하면 위와 같은 화면이 나온다.
 - colab enterprise 같은 곳에서 위 코드를 복붙하여 실행 한 후에 데이터와 함께 엔드 포인트로 요청을 보내면 예측된 결과를 반환하는 것이다.





 - 그 후에 엔드포인트에서 화면 상단의 Google Cloud Assist를 열고

```
내가 엔드포인드 아이디 	3517941329139073024의 추론용 파이썬 코드를 작성하려고 해. 데이터는 gs://cloud-samples-data/ai-platform-unified/datasets/tabular/petfinder-tabular-classification.csv이야.
```
 - 위와 같이 프롬프트를 입력 하면 다음과 같이 나온다.

```python
from google.cloud import aiplatform
import pandas as pd # CSV 파일 처리를 위해 pandas 라이브러리 추가

# Google Cloud 프로젝트 ID를 설정합니다.
project_id = "sesac-ericyum9196"

# Vertex AI 엔드포인트 ID를 설정합니다.
endpoint_id = "3517941329139073024"

# 엔드포인트가 배포된 리전을 설정합니다.
# 이 정보는 제공되지 않았으므로 적절한 리전으로 변경해야 합니다.
# (예: 'us-central1', 'asia-east1' 등)
location = "us-central1" # 예시 리전, 실제 리전으로 변경 필요

# Vertex AI 클라이언트를 초기화합니다.
aiplatform.init(project=project_id, location=location)

# 엔드포인트 객체를 가져옵니다.
endpoint = aiplatform.Endpoint(endpoint_id)

# gs://cloud-samples-data/ai-platform-unified/datasets/tabular/petfinder-tabular-classification.csv
# 파일에서 데이터를 로드하고 추론 입력으로 준비합니다.
try:
    # GCS 경로에서 CSV 데이터 읽기
    # pandas와 google-cloud-storage 라이브러리가 설치되어 있어야 합니다.
    df = pd.read_csv("gs://cloud-samples-data/ai-platform-unified/datasets/tabular/petfinder-tabular-classification.csv")

    # 추론에 사용할 데이터 선택.
    # 'AdoptionSpeed' 컬럼은 타겟(label)이므로, 모델 입력에서 제외합니다.
    # 모델이 훈련될 때 사용된 특징 컬럼만 포함해야 합니다.
    # 여기서는 예시로 'AdoptionSpeed' 컬럼만 제외하고 모든 다른 컬럼을 사용합니다.
    if 'AdoptionSpeed' in df.columns:
        features_df = df.drop(columns=['AdoptionSpeed'])
    else:
        features_df = df # 'AdoptionSpeed' 컬럼이 없으면 모든 컬럼 사용

    # DataFrame을 인스턴스 리스트로 변환 (각 행이 딕셔너리가 되도록)
    # 이 형식은 일반적으로 Vertex AI의 테이블형 모델이 기대하는 입력 형식입니다.
    instances = features_df.to_dict(orient='records')

    # 인스턴스가 너무 많으면 추론 요청이 실패할 수 있으므로, 일부만 사용하거나 배치 처리를 고려하세요.
    # 예시를 위해 첫 5개 인스턴스만 사용합니다.
    instances_for_prediction = instances[:5]

    # 엔드포인트에 추론 요청을 보냅니다.
    predictions = endpoint.predict(instances=instances_for_prediction)

    print(f"Prediction results: {predictions.predictions}")
    # print(f"Prediction metadata: {predictions.metadata}") # 메타데이터는 필요에 따라 출력

except Exception as e:
    print(f"An error occurred during prediction: {e}")


```

 - 그럼 위와 같이 테스트 코드를 만들 수 있다. 이를 colab enterprise에 복붙하고 실행 하면 실제로 데이터에 대해 분석을 하고 그 예측 결과값을 출력한다.





# 삭제 및 정리

그 후 관련 내용을 삭제해야 비용을 줄일 수 있다.

 - 배포한것 완전히 정리하기

1. 데이터 세트: 비어있어야함
2. 학습: 비어있어야함
3. 모델 레지스트리: 비어있어야함
4. 엔드 포인트: 비어있어야함

![[Pasted image 20250715164549.png]]
**주의) 엔드 포인트를 삭제할 때는 이렇게 배포 취소 처리를 해야 삭제를 할 수 있다.**









# 버킷

다음과 같은 경로에 있다.
**GCP -> Cloud Storage -> 버킷**

 - 버킷은 파일 등을 저장하는 구글 드라이브와 비슷하지만 이 후에 다른 사람들을 초대 해서 해당 파일을 '공유'할 수 있다는 차이점이 있다.

 - 그리고 버킷의 이름은 '전역적으로' 독립적이여야 한다. 즉, 전세계의 다른 사람들의 버킷 이름과도 완전히 달라야 한다.








# colab enterprise 추가 활용 및 모델가든, media studio 소개

**1**. ipynb파일을 colab enterprise에 사용하고 싶다면 '가져오기'를 활용할 수 있다. 

- 구글 클라우드에서 대시보드
- Colab 엔터프라이즈: 일시적으로 사용경우
- workbench
- Vertex AI (AutoML)
- tabular(테이블:csv)
- 이미지(분류, 객체탐지, 분할)
- 동영상(....)
- 구글 클라우드 github의 vertex-ai-samples에서 automl에서 노트북


**2**. vertexAI 안에 모델가든이라는 것이 있어서 이것 안의 기능들 중 하나를 그대로 배포를 해서 사용할 수 있다.

**3**. 또 media studio에서 이미지, 음성, 노래, 동영상을 제작할 수 있다. 또한 API 코드를 가져와서 colab enterprise에서도 할 수 있게 해주고 있다.

**4**. 아래는 media studio의 동영상 툴인 veo를 선택하고 '코드 가져오기'를 선택햇을 때 나오는 사진이다. 즉, 저것을 복사해서 colab에 붙여넣고 실행하면 동일한 기능을 한다는 것이다.

![[Pasted image 20250715113001.png]]


**5**. 추가로 '프롬프트 만들기'에서 '코드 가져오기' 기능을 통해서 gemini의 대화 기능을 colab enterprise에서 하도록 할 수 있다.



 - vertex ai -> 프롬프트 만들기 에서

 - 도구 -> 그라운딩 (배경지식 같은 거임)

![[Pasted image 20250715131106.png]]

 - 그 후 코드로 빌드 -> 코드 가져오기 -> python 선택 -> 노트북으로 열기

이렇게 하면 gemini와 대화하는 것을 colab enterprise에서 코드를 실행해서 할 수 있다.




## 그라운딩(Grounding) 기능에 대한 이해

'그라운딩(Grounding)' 기능을 '배경 지식'으로 이해하는 것은 큰 틀에서는 맞지만, 더 구체적으로는 다음과 같이 설명할 수 있습니다.

그라운딩 기능은 AI 모델(예: Gemini)이 답변을 생성할 때, 단순히 모델 자체의 내부 학습 데이터에만 의존하는 것이 아니라, **외부의 신뢰할 수 있고 최신 정보 소스에 기반하여 답변의 사실적 정확성과 최신성을 확보하도록 돕는 기능**입니다.

제공된 이미지에서 그라운딩의 주요 소스로는 다음과 같은 것들이 제시되어 있습니다.

* **Google 검색으로 그라운딩:**
    * **설명:** "Google 검색으로 그라운딩 모델을 세상의 지식, 광범위한 주제, 인터넷상의 최신 정보와 연결합니다."
    * **역할:** Gemini 모델이 질문에 답변할 때, **실시간 Google 검색 결과**를 활용하여 답변의 정확성을 높이고, 최신 정보를 반영하며, 모델이 없는 사실을 지어내는 **'환각(hallucination)' 현상을 줄이는 데 기여**합니다. 이는 마치 사람이 어떤 주제에 대해 답변하기 전에 인터넷 검색을 통해 최신 정보를 확인하는 과정과 유사합니다.

* **Google 지도로 그라운딩 (실험용):**
    * **설명:** "Google 지도의 위치 검색 결과"
    * **역할:** 특히 위치나 장소와 관련된 질의에 대해 **Google 지도의 실시간 데이터**를 활용하여 정확한 지리적 정보를 제공하도록 돕는 실험적인 기능입니다.

**결론적으로:**

'그라운딩'은 단순히 모델의 '배경 지식'을 늘리는 것을 넘어, **'실시간으로 업데이트되고 외부에서 검증된 정보(Google 검색 결과, Google 지도 등)를 활용하여 AI 모델의 답변을 사실에 기반하여 정확하고 최신성 있게 만드는 과정 또는 기술'** 이라고 이해하는 것이 가장 적합합니다. 이 기능은 AI 모델의 '신뢰성'을 획기적으로 향상시키는 데 매우 중요한 역할을 합니다.








# Media Studio

vertex AI -> Media Studio

1. **Imagen (이미지)** 
* **기능:** 텍스트를 이미지로 변환하는 생성형 AI 모델입니다. (Text-to-Image) 
* **특징:** 
	 -  **고품질 실사 이미지 생성:** 텍스트 프롬프트의 미묘한 차이까지 이해하여 놀랍도록 사실적이거나 예술적인 이미지를 생성하는 데 탁월합니다. 
* **높은 프롬프트 이해도:** 복잡하고 길거나 추상적인 프롬프트도 정확하게 해석하여 원하는 이미지를 만들어냅니다. 
* **다양한 스타일 및 콘텐츠:** 특정 스타일(예: 유화, 만화, 사진 등)이나 구체적인 장면, 인물, 사물 등을 묘사하는 이미지를 생성할 수 있습니다. 
* **Media Studio에서의 역할:** 마케팅 자료용 이미지, 웹사이트 삽화, 게임 캐릭터 디자인, 광고 시안 등 텍스트 설명만으로 필요한 이미지를 빠르게 생성하는 데 활용됩니다. 
2. **Chirp (음성/오디오)** 
* **기능:** 텍스트를 음성 또는 오디오(음악, 음향 효과 포함)로 변환하는 생성형 AI 모델입니다. (Text-to-Audio / Text-to-Speech) 
* **특징:** 
	* **자연스럽고 표현력 있는 음성:** 단순한 기계음이 아닌, 억양, 감정(기쁨, 슬픔, 분노 등), 특정 목소리 톤을 반영하여 매우 자연스럽고 생동감 있는 음성을 생성합니다. 
	* **다양한 오디오 콘텐츠 생성:** 음성뿐만 아니라 배경 음악, 환경 음향(예: 숲 속 소리, 도시 소음), 특정 악기 소리 등 다양한 오디오 콘텐츠를 텍스트로 지시하여 생성할 수 있습니다. 
	* **다국어 지원:** 여러 언어와 방언으로 음성을 생성할 수 있습니다. 
	* **Media Studio에서의 역할:** 내레이션, 오디오북 더빙, 팟캐스트 콘텐츠, 게임 캐릭터 음성, 광고 음악 및 효과음 등 다양한 오디오 콘텐츠를 손쉽게 제작하는 데 사용됩니다. 
3. **Lyra (리라 - 오디오 코덱)** 
* **기능:** 다른 세 가지 생성형 AI 모델과는 성격이 조금 다릅니다. Lyra는 음성 압축 코덱입니다. 
* **특징:** 
	 -  **저대역폭 고품질 오디오:** 매우 낮은 비트레이트(bandwidth)에서도 깨끗하고 선명한 음성 품질을 유지하는 데 특화되어 있습니다. 
	 * **신경망 기반:** 신경망 기술을 활용하여 오디오를 효율적으로 압축하고 복원합니다. 이는 VoIP 통화, 오디오 스트리밍 등 대역폭이 제한적인 환경에서 고품질 음성 통신을 가능하게 해줍니다. 
	 * **Media Studio에서의 역할:** Lyra는 직접적으로 콘텐츠를 "생성"하는 모델이라기보다는, Media Studio 내에서 생성된 오디오 콘텐츠(Chirp 등으로 생성된 음성 등)를 효율적으로 저장하거나 스트리밍하거나, 또는 사용자가 업로드하는 오디오의 품질을 최적화하여 처리하는 **오디오 인프라 기술**로 활용될 수 있습니다. 사용자에게는 눈에 띄지 않지만, Media Studio가 제공하는 오디오 기능의 품질과 효율성을 뒷받침하는 기술입니다. 
4. **Veo (베오 - 비디오)** 
* **기능:** 텍스트를 비디오로 변환하는 생성형 AI 모델입니다. (Text-to-Video) 
* **특징:** 
	* **고품질 비디오 생성:** 텍스트 프롬프트로부터 고화질의 비디오 클립을 생성합니다. 
	* **일관된 장면 유지:** 생성된 비디오 전반에 걸쳐 캐릭터, 객체, 스타일의 일관성을 유지하는 데 강점을 가집니다. 
	* **다양한 카메라 움직임 및 스타일:** 줌인/아웃, 패닝 등 다양한 카메라 워크나 특정 비디오 스타일(예: 애니메이션, 다큐멘터리)을 적용할 수 있습니다. 
	* **긴 비디오 길이 지원:** 비교적 긴 길이의 비디오 클립을 생성할 수 있습니다. 
	* **Media Studio에서의 역할:** 광고 영상, 짧은 영화 클립, 소셜 미디어용 콘텐츠, 제품 설명 비디오, 뉴스 보도 영상 등 텍스트 설명만으로 동적인 비디오 콘텐츠를 빠르게 제작하는 데 사용됩니다. 이는 특히 비디오 제작에 따르는 시간과 비용을 획기적으로 줄여줍니다.
 
 요약하자면, Vertex AI Studio의 Media Studio는 **Imagen(이미지), Chirp(음성/오디오), Veo(비디오)**와 같은 강력한 생성형 AI 모델들을 통합하여 사용자가 텍스트 기반으로 다양한 형태의 미디어 콘텐츠를 만들 수 있는 원스톱 플랫폼을 제공합니다. 여기에 **Lyra**와 같은 효율적인 오디오 처리 기술이 결합되어 전반적인 미디어 생성 및 처리 경험을 향상시킵니다.



 Media Studio 뿐만 아니라 Vertex AI -> 실시간 스트리밍 경로에 **실시간 스트리밍**이라는 것이 있어서 마치 인터넷 방송을 하듯이 음성 화면 등 다양한 데이터를 입력하면 이에 맞춰서 출력을 하는 기능도 있다.