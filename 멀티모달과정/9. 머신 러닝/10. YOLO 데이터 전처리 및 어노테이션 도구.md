
# 데이터 전처리 및 어노테이션 도구 사용하기

https://github.com/venture21/autoLabeler

YOLO v8 객체 탐지 모델 학습을 위한 전문적인 데이터 어노테이션 및 증식 프로그램입니다. PyQt5와 OpenCV를 사용하여 개발되었으며, 직관적인 GUI를 통해 효율적인 바운딩 박스 편집을 지원합니다.

## 🎯 주요 기능

### 바운딩 박스 편집


- **그리기**: 마우스 좌클릭 드래그로 새 바운딩 박스 생성
- **이동**: `Ctrl + 드래그`로 바운딩 박스 이동
- **크기 조절**: `Alt + 드래그`로 바운딩 박스 크기 조절 (8개 핸들 지원)
- **삭제**: 마우스 우클릭으로 삭제 (한국어 확인 팝업)
- **클래스별 색상**: 각 클래스마다 다른 색상으로 바운딩 박스 표시
- **최소 크기 제한**: 5픽셀 미만 바운딩 박스 생성 방지

### 스마트 저장 시스템


- **자동 저장**: 바운딩 박스 편집 시에만 자동 저장
- **변경 추적**: 실제 변경이 있을 때만 파일 저장
- **데이터 보호**: 프로그램 시작 시 기존 레이블 파일 보호
- **안전한 탐색**: 파일 선택 시 자동 저장하지 않음

### Undo/Redo 시스템


- **명령 패턴**: 모든 편집 작업에 대한 취소/재실행 지원
- **무제한 히스토리**: 메모리 허용 범위 내에서 무제한 Undo/Redo
- **이미지별 관리**: 새 이미지 로드 시 히스토리 초기화

### 데이터 증식


- **좌우 대칭**: 수평 flip 변환
- **수직 대칭**: 수직 flip 변환
- **회전**: 사용자 정의 각도로 회전 (어노테이션 자동 변환)
- **스케일 변경**: 비율 유지하면서 크기 조절
- **구조화된 저장**: `augmented/images/`와 `augmented/labels/` 폴더로 분리 저장

### 사용자 인터페이스


- **한국어 지원**: 모든 UI 요소와 메시지 한국어 제공
- **반응형 레이아웃**: 스플리터로 영역 크기 조절 가능
- **키보드 탐색**: 파일 리스트에서 ↑↓ 키로 빠른 이미지 전환
- **상태 표시**: 현재 설정된 디렉토리 경로 표시 (16px 크기)




anaconda prompt에서 다음과 같이 진행
```
(base) C:\Users\SBA\github>conda activate yolov11

(yolov11) C:\Users\SBA\github>git clone https://github.com/venture21/autoLabeler.git

(yolov11) C:\Users\SBA\github>cd autolabeler

(yolov11) C:\Users\SBA\github\autoLabeler>pip install pyqt5

(yolov11) C:\Users\SBA\github\autoLabeler>python yolo_data_preprocessing_tool.py
```


그러면 이런 창이 뜸

![[Pasted image 20250730151837.png]]


여기서 
1. 이미지 파일이 있는 폴더의 경로를 지정하고 
2. 박스를 드래그 했을 때 그 좌표 값을 저장하기 위한 라벨(label) 폴더를 지정해야 함.
3. 그 후 Data Aug 에서 상하 반전, 좌우 반전, 화면 회전, 화면 크기 조절 등의 다양한 증강을 시킬 수 있고 
4. 그 후 증강 버튼을 누르면 이미지 파일이 있는 폴더 안에 augmented라는 이름의 새로운 폴더가 생기고 그 안에 증강된 이미지가 있음.

5. 박스를 드래그 하기 전에 Box Labels에서 박스 라벨의 종류를 지정할 수 있는데 autoLabeler 폴더에 classes.txt파일을 생성하고 그 안에 어떤 라벨들을 넣고 싶은지를 작성하면 위의 이미지 파일과 같이 해당 내용으로 Box Labels가 변경된다.
6. 그리고 박스를 드래그 하면 해당 박스의 좌표 값이 라벨 폴더 안에 .txt파일의 형태로 저장된다.

참고) label 텍스트 폴더에는 이런 식으로 좌표가 존재한다.

car, bus, truck 중 하나를 선택한 후에 박스를 생성한다.
그 후에 증식을 하면 labels에 이렇게 좌표가 있다.

```
<class_index>   <x_center> <y_center>       <width> <height>
# class 번호    바운딩 박스(의 중심 좌표)      x축 중심좌표
  0             0.687037 0.575781           0.198148 0.109896
```



# GCP Vertex AI에서 전처리된 이미지 파일로 학습하기

1. autoLabeler를 이용해서 이미지들을 증강 시킨 후 약 6:2의 비율로 train 폴더와 valid 폴더에 각각 이미지와 라벨을 넣는다.
2. sample_dataset.zip라는 이름으로 압축을 한다. 그러면 다음과 같은 구조로 압축이 된다.
![[Pasted image 20250730162605.png]]

3. 이제 GCP Vertex AI의 Colab Enterprise에서 **train-yolo11-object-detection-on-custom-dataset.ipynb**를 킨 후 일부 코드를 실행해서 이 데이터 셋을 가지고 학습을 할 것이다. 그 이전에 zip파일을 옮기고 data.yaml파일을 작성해서 올려야 한다.

ultralytics 설치
```python
!pip install --upgrade ultralytics
```

내 컴퓨터의 nvidia 그래픽 드라이버 정보 확인
```python
!nvidia-smi
```


현재 작업 디렉토리 HOME으로 지정
```python
import os

HOME = os.getcwd()

print(HOME)
```


현재 ultralytics 버전 체크
```python
# Jupyter 환경에서 ultralytics 라이브러리가 사용자의 활동 데이터를 원격 서버와 동기화하지 않도록 설정

!yolo settings sync=False

import ultralytics

ultralytics.checks()
```

출력:
```python
Ultralytics 8.3.170 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 14913MiB) Setup complete ✅ (4 CPUs, 14.6 GB RAM, 50.8/94.3 GB disk)
```







datasets폴더 만들고 위치 이동
```python
!mkdir {HOME}/datasets

%cd {HOME}/datasets
```

출력:
```python
/content/datasets
```


압축 해제
```python
!unzip -qq sample_dataset.Zip
```




4. data.yaml파일 생성. 다음과 같이 작성한다.

custom dataset 구성: 클래스의 갯수 3개 이상(클래스당 10 ~ 20장)

data.yaml
```
train: /content/datasets/train/images

val: /content/datasets/valid/images

  

# Classes

nc: 3 # number of classes

names: ['scissors', 'mouse', 'knife']
```


data.yaml -> pothole의 data.yaml파일을 참조

train - images(jpg, png...)
	labels(txt)

valid - images(jpg, png...)
	labels(txt)

test - images(jpg, png...)
	labels(txt)


5. data.yaml파일을 colab enterprise에 올리기

![[Pasted image 20250730173813.png]]


6. 학습하기
```python
%cd {HOME}

  

!yolo task=detect mode=train model=yolo11s.pt data=/content/datasets/data.yaml epochs=10 imgsz=640 plots=True
```

그러면 아래와 같이 runs/detext/train 폴더가 생기고 
![[Pasted image 20250730174539.png]]
그 안에 들어가 보면 best.pt가 있다. 가장 최적의 가중치 라는 것이다.
![[Pasted image 20250730174633.png]]





# Vertex AI에서 만든 가중치 파일을 가져와서 터미널을 통해 Yolo11을 실행하기


![[Pasted image 20250730174944.png]]

best.pt를 가져와서 이전에 생성했던 로컬의 yolo11폴더에 복사하고 다음을 실행한다.

yolo에서 webcam으로 테스트시 cli명령입니다
```
yolo predict model=best.pt data=data.yaml source=1 conf=0.3 show=True
```

그러면 컴퓨터의 카메라를 통해 물체를 인지하고 학습한 물체가 있으면 해당 물체에 박스를 자동으로 치는 것을 볼 수 있다.

아래는 ultralytics의 사전 학습된 yolo11n.pt 가중치로 하는 명령어이다. 
```
(yolov11) C:\Users\SBA\github\autoLabeler>yolo predict model=yolo11n.pt data=data.yaml source=1 conf=0.3 show=True
```

colab enterprise에서 지금까지 잘 실행을 하면 이런 게 있을 것이다.

![[Pasted image 20250730175525.png]]

이 yolo11n.pt를 가져오면 된다.