# YOLO를 통한 분류 

train-yolo11-instance-segmentation-on-custom-dataset.ipynb 를 이용
## 사전 준비

다음의 코드를 실행
```python
!nvidia-smi




import os

HOME = os.getcwd()
print(HOME)




%pip install "ultralytics<=8.3.40" supervision roboflow

# prevent ultralytics from tracking your activity
!yolo settings sync=False
import ultralytics
ultralytics.checks()

```



## 데이터 셋 가져오기

코드를 실행하기 전에 Roboflow에서 데이터 셋을 가져와야한다.

1. Roboflow 홈페이지에서 API key 생성
Roboflow Settings -> API key 생성 클릭
그 후 API key를 Colab의 secret key에 등록

2. dataset 가져오기
Pelvis AP X-ray dataset를 검색한 후 그 데이터 셋을 fork하기. 그러면 my project에 해당 이미지들이 있는 것을 볼 수 있다.

3. dataset을 download하기
Versions 탭에 가서 download datasets를 클릭

그 후 다음과 같은 화면에서
![[Pasted image 20250731131659.png]]
Show download code를 선택을 하면 아래와 같은 창이 뜬다.

![[Pasted image 20250731131723.png]]
이 코드를 colab에서 사용한다.



4. 이제 다음 코드를 실행하면 이미지 파일들을 가져온다.
```python
!mkdir {HOME}/datasets

%cd {HOME}/datasets

  

from google.colab import userdata

from roboflow import Roboflow

  

!curl -L "https://app.roboflow.com/ds/R7d79n7G3g?key=88htbKkGwC" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip
```








## 드라이브에 진행상황 저장하기

colab의 좌측 하단에 있는 '터미널'을 클릭한 후 다음을 입력한다.
```
/content# ls
datasets dog.jpeg drive runs yololl-seg.pt yololln.pt yololls-seg.pt 

/content# cp -r runs /content/drive/MyDrive/runs_backup
```

`cp -r runs /content/drive/MyDrive/runs_backup` 명령어를 해석하면 다음과 같습니다:

- `cp`: 파일을 복사하는 명령어
    
- `-r`: (recursive) 디렉토리와 그 안의 모든 내용(하위 디렉토리 및 파일)을 재귀적으로 복사하라는 옵션
    
- `runs`: 복사할 원본 디렉토리 (현재 `/content` 디렉토리에 있는 `runs` 디렉토리)
    
- `/content/drive/MyDrive/runs_backup`: 복사될 대상 경로.
    
    - `/content/drive`: Google Colab에서 Google Drive가 마운트되는 기본 경로입니다.
        
    - `MyDrive`: 사용자의 Google Drive 최상위 디렉토리입니다.
        
    - `runs_backup`: `runs` 디렉토리가 백업될 새로운 디렉토리 이름입니다. 만약 `runs_backup`이라는 디렉토리가 없으면 새로 생성되고, 이미 있다면 그 안에 `runs` 디렉토리의 내용이 복사됩니다.




## 학습하기

```python
%cd {HOME}

!yolo task=detect mode=train model=yolo11s-seg.pt data=/content/datasets/data.yaml epochs=10 imgsz=640 plots=True
```


## 학습 결과 확인

confusion matrix로 얼마나 정확한 지를 볼 수 있다.

```python
from IPython.display import Image as IPyImage

IPyImage(filename=f'{HOME}/runs/segment/train/confusion_matrix.png', width=600)
```


![[Pasted image 20250731135715.png]]

또한 아래와 같이 손실과 정확도를 확인할 수도 있다.
```python
from IPython.display import Image as IPyImage

IPyImage(filename=f'{HOME}/runs/segment/train/results.png', width=600)
```

![[Pasted image 20250731140523.png]]


아래는 학습을 통해 나온 이미지들 중 하나이다.

```python
from IPython.display import Image as IPyImage

IPyImage(filename=f'{HOME}/runs/segment/train/val_batch0_pred.jpg', width=600)
```


![[Pasted image 20250731140632.png]]




# 학습된 가중치의 정확도를 검증하기

가장 정확도가 높은 best.pt를 사용해서 검증(validate)을 한다.

```python
!yolo task=detect mode=val model=/content/runs/segment/train/weights/best.pt data=/content/datasets/data.yaml
```

그럼 다음과 같이 검증의 결과를 볼 수 있다.

![[Pasted image 20250731140901.png]]







## 실제 데이터를 넣어서 예측 하기(CLI 명령어로)

```python
!yolo task=detect mode=predict model=/content/runs/Dsegment/train/weights/best.pt conf=0.25 source=/content/datasets/test/images save=True
```

실행 후에 runs/segment/predict 경로가 저장이 되었다고 나올 것이다.

그러면 다음 코드를 통해 예측한 결과를 볼 수 있다.

```python
import glob

import os

from IPython.display import Image as IPyImage, display

  

latest_folder = max(glob.glob(f'{HOME}/runs/segment/predict*/'), key=os.path.getmtime)

for img in glob.glob(f'{latest_folder}/*.jpg')[:3]:

    display(IPyImage(filename=img, width=600))

    print("\n")
```


![[Pasted image 20250731142051.png]]

![[Pasted image 20250731142100.png]]

![[Pasted image 20250731142114.png]]







## 실제 데이터를 넣어서 예측 하기(SDK 명령어로)

SDK 명령어로 예측하는 코드.
```python
from ultralytics import YOLO

from PIL import Image

import requests

  

model = YOLO(f'{HOME}/runs/segment/train/weights/best.pt')

image = Image.open("/content/datasets/test/images/45_jpg.rf.2cb51355c31372f436a4c6248cf39a01.jpg")

result = model.predict(image, conf=0.25)[0]
```


그 후 다음과 같이 supervision을 통해서 화면에 예측 결과를 출력할 수 있다.
```python
import supervision as sv

detections = sv.Detections.from_ultralytics(result)




# Rename classes

name_dictionary = {

    "SUORCIL": "SOURCIL",

    "SAKRO-ILIAK-EKLEM": "SACROILIAC-JOINT",

    "ILIAK": "ILIUM"

}

detections.data["class_name"] = [name_dictionary.get(class_name, class_name) for class_name in detections.data["class_name"]]




mask_annotator = sv.MaskAnnotator()

label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK, text_position=sv.Position.CENTER)

  

annotated_image = image.copy()

mask_annotator.annotate(annotated_image, detections=detections)

label_annotator.annotate(annotated_image, detections=detections)

  

sv.plot_image(annotated_image, size=(10, 10))
```

![[Pasted image 20250731142316.png]]





# Gemini를 통한 인지 및 분류


## API key 등록하기

1. Google Ai Studio에서 API key발급

2. GOOGLE API KEY라는 이름으로 Colab에 비밀 키 등록




## Gemini 사용해서 프롬프트로 인지 및 분류하기

API key를 환경 변수로 등록

```python
import os

from google.colab import userdata

  

os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
```


supervision 설치
```python
!pip install google-genai supervision
```


이미지 다운로드
```python
!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg

!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg
```


Gemini client 초기화 및 설정

```python
from google import genai

from google.genai import types

  

client = genai.Client(api_key=os.environ["GOOGLE_API_KEY"])

  

safety_settings = [

    types.SafetySetting(

        category="HARM_CATEGORY_DANGEROUS_CONTENT",

        threshold="BLOCK_ONLY_HIGH",

    ),

]
```


이미지와 프롬프트 작성

```python
MODEL_NAME = "gemini-2.5-flash-preview-05-20"

TEMPERATURE = 0.5

  

IMAGE_PATH = "/content/dog-3.jpeg"

PROMPT = "Detect dogs tail. And cap" + \

"Output a JSON list of bounding boxes where each entry contains the 2D bounding box in the key \"box_2d\", " + \

"and the text label in the key \"label\". Use descriptive labels."
```


Gemini를 통해 답변 받아내기
```python
from PIL import Image

  

image = Image.open(IMAGE_PATH)

width, height = image.size

target_height = int(1024 * height / width)

resized_image = image.resize((1024, target_height), Image.Resampling.LANCZOS)

  

response = client.models.generate_content(

    model=MODEL_NAME,

    contents=[resized_image, PROMPT],

    config = types.GenerateContentConfig(

        temperature=TEMPERATURE,

        safety_settings=safety_settings,

        thinking_config=types.ThinkingConfig(

          thinking_budget=0

        )

    )

)

  

response.text
```





결과를 BoxAnnotator와 LabelAnnotator를 이용해서 기존 이미지 위에 나타내기
```python
import supervision as sv

  

resolution_wh = image.size

  

detections = sv.Detections.from_vlm(

    vlm=sv.VLM.GOOGLE_GEMINI_2_5,

    result=response.text,

    resolution_wh=resolution_wh

)

  

thickness = sv.calculate_optimal_line_thickness(resolution_wh=resolution_wh)

text_scale = sv.calculate_optimal_text_scale(resolution_wh=resolution_wh)

  

box_annotator = sv.BoxAnnotator(thickness=thickness)

label_annotator = sv.LabelAnnotator(

    smart_position=True,

    text_color=sv.Color.BLACK,

    text_scale=text_scale,

    text_position=sv.Position.CENTER

)

  

annotated = image

for annotator in (box_annotator, label_annotator):

    annotated = annotator.annotate(scene=annotated, detections=detections)

  

sv.plot_image(annotated)
```
