https://github.com/venture21/mnist-flask

# MNIST 손글씨 숫자 인식


1. 주피터 노트북으로 모델을 생성(직접 모델을 만들고 저장)
mnist_cnn_model.keras
2. 저장한 모델 파일을 다운로드(PC로) git clond했던 폴더 (mnist-flask)
3. 실행하기 전에 app.py 코드 확인(모델명과 파일명이 동일한지)
model = tf.keras.models.load_model('mnist_cnn_model.keras')
4. 실행 (python app.py)

5. 내손으로 직접 모델을 만들어본다.  
   몇가지 바꿔보고 싶은 하이퍼파라메터를 변경  
   (히든 레이어를 1개에서 2~3개 변경, 히든 레이어의 노드를  
    784-> 256->10    784->512->256->10)  
   변경 포인트 : 히든 레이어 갯수, 히든 레이어의 노드의 갯수  
  
   학습후 저장할 파일명 :  mnist_cnn_model.keras  
  
6. 저장한 모델 파일을 다운로도(PC로) git clone했던 폴더(mnist-flask)  
7. 실행하기 전에 app.py 코드 확인(모델명이 파일명과 동일한지)  
   app.py의 52번째 라인  
   model = tf.keras.models.load_model('mnist_cnn_model.keras')  
8. 실행(python app.py)

![[Pasted image 20250723105259.png]]
원래는 98% ~ 97%

수정 후
![[Pasted image 20250723142658.png]]
최종 테스트 정확도: 0.9915
최종 테스트 정밀도: 0.9923
최종 테스트 재현율: 0.9897



![[Pasted image 20250723112354.png]]


---
### 최초 코드로부터 변경된 사항 최종 정리:

1. **데이터 증강 (Data Augmentation) 추가:**
    
    - **추가된 임포트:**
        
        ```Python
        from tensorflow.keras.preprocessing.image import ImageDataGenerator
        ```
        
    - **`train_model` 함수 내 `ImageDataGenerator` 설정 및 사용:**
        
        ```Python
        datagen = ImageDataGenerator(
            rotation_range=10,
            zoom_range=0.1,
            width_shift_range=0.1,
            height_shift_range=0.1
        )
        datagen.fit(x_train)
        
        history = model.fit(
            datagen.flow(x_train, y_train, batch_size=128), # 이 부분 변경
            epochs=20,
            validation_data=(x_test, y_test),
            callbacks=callbacks,
            verbose=1
        )
        ```
        
        훈련 데이터에 대해 회전, 확대/축소, 이동 등의 변형을 적용하여 모델이 더 다양한 이미지 패턴을 학습하도록 했습니다.
        
2. **BatchNormalization 레이어 추가:**
    
    - **`create_cnn_model` 함수 내 Convolutional 레이어 뒤에 추가:**
        
        
        ```Python
        keras.layers.Conv2D(128, (3,3), activation='relu', input_shape=(28,28,1)),
        keras.layers.BatchNormalization(), # 추가됨
        keras.layers.MaxPooling2D((2,2)),
        keras.layers.Conv2D(256, (3,3), activation='relu'),
        keras.layers.BatchNormalization(), # 추가됨
        keras.layers.MaxPooling2D((2,2)),
        keras.layers.Conv2D(256, (3,3), activation='relu'),
        keras.layers.BatchNormalization(), # 추가됨
        ```
        
        각 Conv2D 레이어 다음에 BatchNormalization이 추가되어 학습 안정성을 높이고 수렴 속도를 향상시킵니다.
        
3. **Convolutional 레이어의 필터(노드) 수 증가:**
    
    - **`create_cnn_model` 함수 내 Conv2D 필터 수 변경:**
        
        ```Python
        # 기존: Conv2D(32, ...), Conv2D(64, ...), Conv2D(64, ...)
        keras.layers.Conv2D(128, (3,3), activation='relu', input_shape=(28,28,1)), # 32 -> 128
        keras.layers.MaxPooling2D((2,2)),
        keras.layers.Conv2D(256, (3,3), activation='relu'), # 64 -> 256
        keras.layers.MaxPooling2D((2,2)),
        keras.layers.Conv2D(256, (3,3), activation='relu'), # 64 -> 256
        ```
        
        모델의 복잡도를 높여 더 많은 특징을 학습할 수 있도록 Conv2D 필터 수를 32/64에서 128/256으로 늘렸습니다.
        
4. **평가 지표 (Metrics) 추가:**
    
    - **`train_model` 함수 내 `model.compile`의 `metrics` 인자 변경:**
        
        ```Python
        metrics=[
            'accuracy',         # 기존 정확도
            keras.metrics.Precision(), # 정밀도 추가
            keras.metrics.Recall(),    # 재현율 추가
        ]
        ```
        
        학습 과정 및 결과에서 정확도 외에 정밀도(Precision)와 재현율(Recall) 지표를 함께 확인하도록 추가했습니다.








# CNN(Convolution Neural Network)

네, CNN(Convolutional Neural Network), 즉 컨볼루션 신경망의 개념에 대해 자세히 설명해 드리겠습니다.

### CNN(Convolutional Neural Network)이란?

CNN은 딥러닝의 한 종류로, 주로 이미지 인식, 객체 탐지, 영상 처리 등 시각적 데이터 분석에 특화된 신경망입니다. 기존의 인공신경망(ANN)이 이미지와 같은 2차원 데이터를 처리할 때 한계가 있었던 반면, CNN은 이미지의 공간적 구조 정보를 효과적으로 학습하고 유지할 수 있도록 설계되었습니다.

### CNN의 주요 특징 및 구성 요소

CNN은 크게 세 가지 주요 계층(Layer)으로 구성됩니다:

1. **컨볼루션 계층 (Convolutional Layer):**
    
    - **핵심 역할:** 이미지에서 특징(feature)을 추출하는 역할을 합니다. 예를 들어, 선, 모서리, 질감 등과 같은 저수준 특징부터 눈, 코, 입과 같은 고수준 특징까지 계층적으로 학습합니다.
        
    - **필터(Filter) 또는 커널(Kernel):** 작은 크기의 행렬(예: 3x3, 5x5)로, 이미지 위를 슬라이딩(sliding)하면서 이미지의 각 부분과 곱셈 및 덧셈 연산(컨볼루션 연산)을 수행합니다. 이 필터가 이미지의 특정 특징을 감지하는 역할을 합니다.
        
    - **특징 맵(Feature Map):** 컨볼루션 연산의 결과로 생성되는 출력입니다. 각 필터는 이미지의 다른 특징을 감지하여 별도의 특징 맵을 생성합니다.
        
    - **패딩(Padding):** 이미지의 가장자리를 0으로 채워 컨볼루션 연산 후에도 이미지의 크기가 줄어들지 않도록 하거나, 특정 출력 크기를 맞추기 위해 사용됩니다.
        
    - **스트라이드(Stride):** 필터가 이미지 위를 이동하는 간격을 의미합니다. 스트라이드를 크게 하면 특징 맵의 크기가 줄어듭니다.
        
2. **활성화 계층 (Activation Layer):**
    
    - 컨볼루션 계층에서 나온 특징 맵에 비선형성을 추가하는 계층입니다. 선형적인 변환만으로는 복잡한 패턴을 학습하기 어렵기 때문에, ReLU(Rectified Linear Unit)와 같은 비선형 활성화 함수를 적용하여 모델의 표현력을 높입니다.
        
    - **ReLU:** 가장 일반적으로 사용되는 활성화 함수로, 입력이 양수이면 그대로 출력하고 음수이면 0을 출력합니다. 계산이 간단하고 경사 소실(vanishing gradient) 문제를 완화하는 데 도움을 줍니다.
        
3. **풀링 계층 (Pooling Layer):**
    
    - **핵심 역할:** 특징 맵의 크기를 줄여(다운샘플링) 모델의 계산량을 줄이고, 과적합(overfitting)을 방지하며, 이미지의 위치 변화에 대한 불변성(invariance)을 확보합니다.
        
    - **맥스 풀링(Max Pooling):** 가장 흔히 사용되는 풀링 방식입니다. 특정 영역(예: 2x2) 내에서 가장 큰 값만 추출하여 특징 맵의 크기를 줄입니다. 이로 인해 이미지 내에서 특징의 정확한 위치보다는 특징의 존재 여부가 더 중요하게 학습됩니다.
        
    - **평균 풀링(Average Pooling):** 특정 영역 내의 모든 값의 평균을 취하여 크기를 줄입니다.
        

### CNN의 동작 원리 (흐름)

일반적인 CNN 모델의 데이터 흐름은 다음과 같습니다:

1. **입력 이미지:** 원본 이미지가 모델에 입력됩니다.
    
2. **컨볼루션 계층 & 활성화 계층:** 여러 개의 필터와 활성화 함수를 사용하여 이미지에서 다양한 특징 맵을 추출합니다. 이 과정이 여러 번 반복될 수록 저수준 특징에서 고수준 특징으로 학습됩니다.
    
3. **풀링 계층:** 특징 맵의 크기를 줄여 중요한 특징만 남기고 계산 효율성을 높입니다.
    
4. **반복:** 2, 3번의 과정을 여러 번 반복하여 더 추상적이고 고차원적인 특징을 추출합니다.
    
5. **완전 연결 계층 (Fully Connected Layer):**
    
    - 반복된 컨볼루션 및 풀링 계층을 거쳐 추출된 최종 특징 맵들은 1차원 벡터로 평탄화(Flatten)됩니다.
        
    - 평탄화된 벡터는 일반적인 신경망처럼 하나 이상의 완전 연결 계층에 연결됩니다. 이 계층은 추출된 특징들을 기반으로 최종 분류(예: 이 이미지가 고양이인지 개인지)를 수행합니다.
        
6. **출력 계층 (Output Layer):**
    
    - 완전 연결 계층의 마지막 단계로, 최종 예측을 수행합니다. 이미지 분류 문제에서는 주로 소프트맥스(softmax) 활성화 함수를 사용하여 각 클래스에 속할 확률을 출력합니다.
        

### 왜 CNN이 이미지에 효과적인가?

- **공간적 특징 유지:** 필터를 이용한 컨볼루션 연산은 이미지의 인접 픽셀 간의 공간적 관계를 효과적으로 학습하고 유지합니다.
    
- **패턴 반복 및 재사용:** 학습된 필터는 이미지의 어느 위치에서든 동일한 패턴을 감지할 수 있습니다 (가중치 공유, Weight Sharing). 이는 매번 새로운 가중치를 학습할 필요가 없어 모델의 파라미터 수를 줄이고 효율성을 높입니다.
    
- **부분적인 변화에 강인함 (Translation Invariance):** 풀링 계층 덕분에 이미지 내에서 객체의 위치가 약간 이동하더라도 동일한 특징으로 인식할 수 있는 능력이 생깁니다.
    
- **계층적 특징 학습:** 낮은 계층에서는 단순한 특징(선, 모서리)을 학습하고, 높은 계층으로 갈수록 이러한 특징들을 조합하여 더 복잡하고 추상적인 특징(눈, 얼굴)을 학습합니다.


**다음의 자료를 통해서 이미지 학습을 해볼 수 있을 것이다.**
코로나-19에 대한 데이터 셋
https://www.kaggle.com/datasets/imdevskp/corona-virus-report

코로나-19가 걸렸을 때의 흉부 x-ray사진
https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia






## 합성곱 신경망을 사용한 이미지 분류

https://github.com/rickiepark/hg-mldl2/blob/main/08-2.ipynb


08-2.ipynb처럼 프롬프트로 이미지 분류를 학습해보자
## 패키지 불러오기

```python
import keras

import tensorflow as tf
```

## 데이터셋 불러오기

```python
# keras 3.x로 만들고 싶으면 tf.keras X

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
```

## 데이터 전처리

```python
train_images = train_images / 255.0

test_images = test_images / 255.0

  

# train_labels and test_labels will be one-hot encoded after the split

# train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)

# test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)
```

```python
train_images.shape
# (60000, 28, 28)
```

```python
train_images = train_images.reshape((60000, 28, 28, 1))

test_images = test_images.reshape((10000, 28, 28, 1))
```

```python
train_images.shape
# (60000, 28, 28, 1)
```

## CNN으로 신경망 구성하기

```python
model = keras.Sequential([

    # 특징 추출기

    # 커널의 갯수 32개, 커널의 사이즈는 3x3, 활성화함수는 relu, 입력 이미지 shape (28,28,1)

    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),

    keras.layers.MaxPooling2D((2, 2)),

    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),

    keras.layers.MaxPooling2D((2, 2)),

    keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),

  

    # 분류기

    keras.layers.Flatten(),

    keras.layers.Dense(64, activation='relu'),

    keras.layers.Dense(10, activation='softmax')

])

  

model.compile(optimizer='adam',

              loss='categorical_crossentropy',

              metrics=['accuracy'])

  

model.summary()
```

## 데이터셋을 train과 valid로 나누기

```python
from sklearn.model_selection import train_test_split

  

train_images, val_images, train_labels, val_labels = train_test_split(

    train_images, train_labels, test_size=0.2, random_state=42

)

  

# Now apply one-hot encoding after the split

train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)

val_labels = tf.keras.utils.to_categorical(val_labels, num_classes=10)
```

## 콜백 정의

```python
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

  

# ModelCheckpoint callback to save the best model during training

checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy',save_best_only=True, verbose=1)

  

# EarlyStopping callback to stop training when validation accuracy stops improving

early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, mode='max', verbose=1)

  

# ReduceLROnPlateau callback to reduce learning rate when validation accuracy plateaus

reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, min_lr=0.0001, verbose=1)

  

callbacks_list = [checkpoint, early_stopping, reduce_lr]
```

## 학습

```python
history = model.fit(

    train_images,

    train_labels,

    epochs=50,

    validation_data=(val_images, val_labels),

    callbacks=callbacks_list

)
```

