
YOLO11은 물체 감지기인 **ultralytics**의 최신 버전

홈페이지
https://docs.ultralytics.com/ko/

github
https://github.com/ultralytics/ultralytics

추가)
YOLO를 사용하는 예제
https://github.com/roboflow/notebooks

파인 도로 데이터 셋
https://public.roboflow.com/object-detection/pothole


### ultralytics 설치

1. 가상환경 설정  
conda create -n yolov11 python=3.11  
  
2. 가상환경 진입  
conda activate yolov11  
  
3. YOLO 설치  
pip install ultralytics



향후 다음과 같은 코드를 통해서 물체를 예측할 수 있다.

두 이미지를 넣어서 예측 하기

```
from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")  # pretrained YOLO11n model

# Run batched inference on a list of images
results = model(["image1.jpg", "image2.jpg"])  # return a list of Results objects

# Process results list
for result in results:
    boxes = result.boxes  # Boxes object for bounding box outputs
    masks = result.masks  # Masks object for segmentation masks outputs
    keypoints = result.keypoints  # Keypoints object for pose outputs
    probs = result.probs  # Probs object for classification outputs
    obb = result.obb  # Oriented boxes object for OBB outputs
    result.show()  # display to screen
    result.save(filename="result.jpg")  # save to disk
```







### 유용하게 사용할 수도 있는 COCO 데이터 셋

https://cocodataset.org/#home

`cocodataset.org`는 **COCO (Common Objects in Context)** 데이터셋 관련 정보를 제공하는 웹사이트입니다. 이 웹사이트는 컴퓨터 비전 작업에 사용되는 COCO 데이터셋을 중심으로 다음과 같은 기능을 제공합니다.

- **COCO 데이터셋 정보 제공:** 데이터셋에 대한 상세 정보와 사용 조건을 확인할 수 있습니다.
    
- **데이터셋 탐색 및 다운로드:** 사용자들이 COCO 데이터셋을 탐색하고 다운로드할 수 있습니다.
    
- **다양한 컴퓨터 비전 작업 지원:** 탐지(Detection), 키포인트(Keypoints), 파놉틱(Panoptic), 덴스포즈(Densepose), 스터프(Stuff) 등 여러 컴퓨터 비전 작업에 대한 데이터와 정보를 제공합니다.
    
- **챌린지 및 평가 도구:** 각 작업에 대한 챌린지에 참여하여 결과를 업로드하고, 리더보드를 통해 다른 참가자들의 순위를 확인할 수 있습니다. 결과 평가를 위한 도구와 가이드라인도 제공됩니다.
    

요약하자면, `cocodataset.org`는 컴퓨터 비전 연구 및 개발을 위한 핵심적인 자료인 COCO 데이터셋을 공유하고, 관련 챌린지 및 평가를 지원하는 플랫폼입니다.







## Before you start

```
# 내 컴퓨터의 nvidia 그래픽 드라이버 정보 확인

!nvidia-smi
```


```
import os

# 현재 작업 디렉토리 HOME으로 지정

HOME = os.getcwd()

print(HOME)
```




## Install YOLO11 via Ultralytics

```
%pip install "ultralytics<=8.3.40" supervision roboflow

# prevent ultralytics from tracking your activity

  

# Jupyter 환경에서 ultralytics 라이브러리가 사용자의 활동 데이터를 원격 서버와 동기화하지 않도록 설정

!yolo settings sync=False

  

import ultralytics

ultralytics.checks()
```



## Inference with model pre-trained on COCO dataset

### CLI

```
# task=detect: 수행할 작업의 종류를 지정(객체를 탐지)

# conf=0.25: 신뢰도 점수가 25% (0.25) 이상인 탐지 결과만 유효한 것으로 간주하겠다는 뜻

!yolo task=detect mode=predict model=yolo11n.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg' save=True





from IPython.display import Image as IPyImage

IPyImage(filename=f'/content/runs/detect/predict/dog.jpg', width=600)
```


### SDK

```
from ultralytics import YOLO

from PIL import Image

import requests

  

model = YOLO('yolo11n.pt')

image = Image.open(requests.get('https://media.roboflow.com/notebooks/examples/dog.jpeg', stream=True).raw)

result = model.predict(image, conf=0.25)[0]





# 탐지된 각 객체의 바운딩 박스(bounding box) 좌표

result.boxes.xyxy

# 탐지된 각 객체에 대한 신뢰도(Confidence) 점수

result.boxes.conf

# 탐지된 각 객체의 클래스(Class) ID

result.boxes.cls






# supervision 라이브러리를 sv라는 짧은 별칭으로 임포트

import supervision as sv

  

# ultralytics YOLO 모델의 예측 결과(result)를 supervision 라이브러리가 이해하고 쉽게 다룰 수 있는 Detections 객체로 변환

detections = sv.Detections.from_ultralytics(result)

# 박스 생성

box_annotator = sv.BoxAnnotator()

# 레이블 생성

label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK)

  

annotated_image = image.copy()

# 실제 이미지에다가 박스를 그리고

annotated_image = box_annotator.annotate(annotated_image, detections=detections)

# 실제 이미지에다가 레이블을 그리고

annotated_image = label_annotator.annotate(annotated_image, detections=detections)

  

# 이미지 출력

sv.plot_image(annotated_image, size=(10, 10))
```






## Fine-tune YOLO11 on custom dataset

```
!mkdir {HOME}/datasets

%cd {HOME}/datasets

  

from google.colab import userdata

from roboflow import Roboflow

  

!curl -L "https://public.roboflow.com/ds/VoJLgFvCEB?key=tgCALUZmNn" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip

  

# ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')

# rf = Roboflow(api_key=ROBOFLOW_API_KEY)

  

# workspace = rf.workspace("liangdianzhong")

# project = workspace.project("-qvdww")

# version = project.version(3)

# dataset = version.download("yolov11")
```






## Custom Training

```
%cd {HOME}

!yolo task=detect mode=train model=yolo11s.pt data=/content/datasets/data.yaml epochs=10 imgsz=640 plots=True

!ls {HOME}/runs/detect/train/
```




**Confusion Matrix(혼동 행렬) 시각화 결과**

경로:
/content/runs/detect/train/confusion_matrix.png
/content/runs/detect/train/confusion_matrix_normalized.png


![[Pasted image 20250729114427.png]]



![[Pasted image 20250729114438.png]]


이런 느낌
![[Pasted image 20250729135029.png]]



pothole인 것 들 중에서 제대로 예측한 것이 67% 그리고 잘 못 예측한 것이 33%이고 background인 것들 중에서 잘 못 예측한 것이 100%이다. 왜냐하면 모델이 background라고 판단이 되면 아예 탐지를 하지 않기 때문이다.


![[Pasted image 20250729140222.png]]


### **가중치가 있는 위치**

/content/runs/detect/train/weights/best.pt
가장 최고의 성능을 내는 가중치

/content/runs/detect/train/weights/last.pt
가장 최근 학습의 가중치



### **ILO** -> 예측값과 정답 값이 겹치는 정도. 1에 가까울 수록 좋다.



### 데이터 셋 구성

train: 7 + a(2) = 9 (이렇게 더 추가할 수 있다.)

val: 2

test: 1



### train 결과
/content/runs/detect/train/results.png

![[Pasted image 20250729134524.png]]



