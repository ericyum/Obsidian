

# OpenCV

OpenCV (Open Source Computer Vision Library)는 컴퓨터 비전 및 머신러닝 작업을 위한 오픈소스 라이브러리입니다. C++로 작성되었지만 Python, Java, MATLAB 등 다양한 프로그래밍 언어를 지원하며, Windows, Linux, macOS, Android, iOS 등 여러 운영체제에서 사용할 수 있습니다.

**주요 특징 및 역할:**

- **컴퓨터 비전 애플리케이션 개발:** OpenCV는 이미지와 비디오 데이터를 처리하고 분석하기 위한 방대한 알고리즘 및 함수를 제공합니다. 이를 통해 컴퓨터가 사람의 시각처럼 이미지를 "이해"할 수 있도록 돕습니다.
    
- **다양한 기능:**
    
    - **이미지 및 비디오 처리:** 이미지 필터링 (블러링, 경계선 검출 등), 색상 공간 변환, 이미지 크기 조정, 회전 등 기본적인 이미지 조작부터 복잡한 영상 처리까지 가능합니다.
        
    - **객체 검출 및 인식:** 얼굴, 사람, 특정 객체 등을 이미지나 비디오에서 검출하고 인식하는 기능을 제공합니다. 자율 주행, 감시 시스템 등에서 활용됩니다.
        
    - **객체 추적:** 움직이는 객체를 실시간으로 추적하는 데 사용됩니다.
        
    - **특징점 추출 및 매칭:** 이미지에서 특징적인 부분을 찾아내고 이를 다른 이미지와 비교하여 매칭하는 데 사용됩니다.
        
    - **3D 모델링:** 여러 장의 2D 이미지를 조합하여 3D 모델을 생성하는 데 활용될 수 있습니다.
        
    - **머신러닝 및 딥러닝 연동:** 딥러닝 프레임워크(TensorFlow, PyTorch 등)와도 쉽게 연동하여 더욱 정교한 컴퓨터 비전 모델을 구축할 수 있습니다.
        
- **실시간 처리:** OpenCV는 실시간 애플리케이션에 중점을 두고 설계되어, 빠른 속도와 효율성을 자랑합니다.
    
- **오픈소스 및 상업적 사용 가능:** Apache 2 라이선스로 배포되어 있어 상업적인 목적으로도 자유롭게 사용할 수 있으며, 소스 코드 공개 의무도 없습니다.
    
- **폭넓은 활용 분야:**
    
    - 얼굴 인식 시스템 및 보안
        
    - 자율 주행 자동차 및 로봇 비전
        
    - 의료 영상 분석
        
    - 공장 자동화 및 품질 검사
        
    - 증강 현실 (AR)
        
    - 사진 및 비디오 편집 애플리케이션 등


```
(yolov11) C:\Users\SBA\github\yolo11>pip install opencv-python==4.11.0.86
```

openCV

```python
import cv2

image = cv2.imread("Image/lunar.jpg", cv2.IMREAD_ANYCOLOR)
cv2.imshow("Moon", image)
cv2.waitKey()
cv2.destroyAllWindows()
```

달 이미지 출력



# SAM2

**SAM 2**는 훨씬 더 구체적인 최신 인공지능 모델입니다.

**SAM 2 (Segment Anything Model 2)**는 Meta AI Research에서 개발한 **이미지 및 비디오 분할(Segmentation)**을 위한 **기초 모델(Foundation Model)**입니다.

**주요 특징 및 역할:**

- **"Segment Anything"의 확장판:** 2023년 Meta가 발표했던 "Segment Anything Model (SAM)"의 후속 버전입니다. 기존 SAM은 주로 이미지에서 객체를 "잘라내는" 것에 초점을 맞췄다면, SAM 2는 이를 **비디오** 영역으로 확장했습니다.
    
- **이미지와 비디오 통합 분할:** SAM 2는 이미지를 단일 프레임 비디오로 간주하여, 단일 모델로 이미지와 비디오 모두에서 객체 분할을 수행할 수 있도록 설계되었습니다. 이는 배포를 간소화하고 다양한 미디어 유형에서 일관된 성능을 제공합니다.
    
- **프롬프트 기반 분할:** SAM 2는 사용자가 특정 "프롬프트"를 제공하면 해당 프롬프트에 해당하는 객체를 분할하는 방식으로 작동합니다. 프롬프트는 다음과 같은 형태가 될 수 있습니다:
    
    - **점(Point):** 객체 내의 특정 위치를 클릭하여 해당 객체를 분할하도록 지시합니다.
        
    - **바운딩 박스(Bounding Box):** 객체를 감싸는 직사각형을 그려서 해당 영역 내의 객체를 분할하도록 지시합니다.
        
    - **마스크(Mask):** (이전 프레임의 마스크 등을 활용하여) 직접 마스크를 제공하여 분할을 유도합니다.
        
- **제로샷 일반화(Zero-Shot Generalization):** 모델 학습 시 보지 못했던 새로운 객체나 이미지, 비디오에서도 뛰어난 분할 성능을 보여줍니다. 이는 별도의 재학습 없이도 다양한 실제 시나리오에 적용할 수 있게 해줍니다.
    
- **실시간 성능:** SAM 2는 실시간 비디오 처리를 위해 스트리밍 메모리를 갖춘 간단한 트랜스포머 아키텍처를 사용하며, 빠른 추론 속도를 자랑합니다. 이는 비디오 편집, 증강 현실 등 즉각적인 피드백이 필요한 애플리케이션에 매우 유용합니다.
    
- **상호작용적 개선:** 사용자는 추가적인 프롬프트를 제공함으로써 분할 결과를 반복적으로 세밀하게 조정할 수 있습니다. 이는 의료 영상 분석이나 비디오 주석 작업과 같이 정밀한 제어가 필요한 경우에 필수적입니다.
    
- **비디오 트래킹 및 Occlusion 처리:** 비디오에서 객체가 다른 물체에 의해 가려지거나 잠시 사라졌다가 다시 나타나는 경우에도, SAM 2는 객체를 지속적으로 추적하고 분할을 유지하는 고급 메모리 메커니즘을 갖추고 있습니다.






## Setup

```
!pip install ultralytics

import ultralytics

ultralytics.checks()
```


## Segment Everything

```python
from ultralytics import SAM

  

# Load a model

# For SAM=sam_b.pt, SAM2=sam2_b.pt, SAM2.1=sam2.1_b.pt

model = SAM("sam2.1_b.pt")

  

model.info()  # Display model information (optional)

  

# Run inference (image or video)

results = model("https://ultralytics.com/images/bus.jpg")  # image

# results = model("https://youtu.be/LNwODJXcvt4")  # video file

  

results[0].show()  # Display results
```


## Segment Anything

### Bounding box prompt

```python
from ultralytics import SAM

  

# Load a model

model = SAM("sam2.1_b.pt")

  

# Run inference with bboxes prompt (Provide the bounding box coordinates

# for the bus area, ensuring that only bus is segmented in the entire image)

results = model("https://ultralytics.com/images/bus.jpg",

                bboxes=[3.8328723907470703, 229.35601806640625,

                        796.2098999023438, 728.4313354492188])

  

results[0].show()  # Display results
```


### Point prompt

```python
from ultralytics import SAM

  

# Load a model

model = SAM("sam2.1_b.pt")

  

# Run inference with point prompt (Provide the point coordinates for the

# person area, ensuring that only the person is segmented in the entire image)

results = model("https://ultralytics.com/images/bus.jpg",

                points=[34, 714])

  

results[0].show()  # Display results
```



### Multiple points prompt

```python
from ultralytics import SAM

  

# Load a model

model = SAM("sam2.1_b.pt")

  

# Run inference with multiple point prompts (Provide the points coordinates for

# person area, ensuring that only the person is segmented in the entire image)

results = model("https://ultralytics.com/images/bus.jpg",

                points=[[34, 714], [283, 634]])

  

results[0].show()  # Display results
```



## Auto Annotation using Segment Anything Model

```python
from ultralytics.data.annotator import auto_annotate

  

# Use the Ultralytics sample dataset (You can use your own images)

from ultralytics.utils.downloads import safe_download

images = ["bus.jpg", "zidane.jpg"]

for img in images:

  path = safe_download(f"https://ultralytics.com/assets/{img}", dir="assets")

  

# return the annotation in the Ultralytics YOLO segmentation format.

# output directory i.e assets_auto_annotate_labels

auto_annotate(data="assets",

              det_model="yolo11x.pt",

              sam_model="sam_b.pt")
```
